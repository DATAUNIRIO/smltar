# Word Embeddings {#embeddings}

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE, 
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
``` 

> You shall know a word by the company it keeps.
> `r tufte::quote_footer('--- [John Rupert Firth](https://en.wikiquote.org/wiki/John_Rupert_Firth)')`

So far in our discussion of natural language features, we have discussed preprocessing steps such as tokenization, removing stop words, and stemming in detail. We implement these types of preprocessing steps to be able to represent our text data in some data structure that is a good fit for modeling. An example of such a data structure is a sparse matrix. Perhaps we would start with straightforward word counts.

```{r}
library(tidyverse)
library(tidytext)
library(SnowballC)
library(scotus)

scotus_sample %>% 
  unnest_tokens(word, text) %>%  
  anti_join(get_stopwords()) %>%  
  mutate(stem = wordStem(word)) %>% 
  count(case_name, stem) %>%  
  cast_dfm(case_name, stem, n)
```

Another way to represent our text data is to use [tf-idf](https://www.tidytextmining.com/tfidf.html) instead of word counts. This weighting for text features can often work better in predictive modeling.

```{r}
scotus_sample %>% 
  unnest_tokens(word, text) %>% 
  anti_join(get_stopwords()) %>%  
  mutate(stem = wordStem(word)) %>% 
  count(case_name, stem) %>%  
  bind_tf_idf(stem, case_name, n) %>% 
  cast_dfm(case_name, stem, tf_idf)
```

Notice that in either case, our final data structure is incredibly sparse and of high dimensionality. Linguists have long worked on vector models for language that can reduce this number of dimensions because of the nature of how people use language; the quote that opened this chapter dates to 1957. These kinds of word vectors are often called **word embeddings**.

## Understand word embeddings by finding them yourself

## Word embeddings from deep learning

## Fairness and word embeddings

## Summary


