# Word Embeddings {#embeddings}

> You shall know a word by the company it keeps.
> `r tufte::quote_footer('--- [John Rupert Firth](https://en.wikiquote.org/wiki/John_Rupert_Firth)')`

So far in our discussion of natural language features, we have discussed preprocessing steps such as tokenization, removing stop words, and stemming in detail. We implement these types of preprocessing steps to be able to represent our text data in some data structure that is a good fit for modeling. An example of such a data structure is a sparse matrix. Perhaps we would start with straightforward word counts.

```{r}
library(tidyverse)
library(tidytext)
library(scotus)

scotus_sample %>% 
  unnest_tokens(word, text) %>%          ## tokenize
  anti_join(get_stopwords()) %>%         ## remove stop words
  mutate(stem = wordStem(word)) %>%      ## stem
  count(case_name, stem) %>%             ## count words
  cast_dfm(case_name, stem, n)
```

Another way to represent our text data is to use [tf-idf](https://www.tidytextmining.com/tfidf.html) instead of word counts. This weighting for text features can often work better in predictive modeling.

```{r}
scotus_sample %>% 
  unnest_tokens(word, text) %>%          ## tokenize
  anti_join(get_stopwords()) %>%         ## remove stop words
  mutate(stem = wordStem(word)) %>%      ## stem
  count(case_name, stem) %>%             ## count words
  bind_tf_idf(stem, case_name, n) %>%    ## tf-idf
  cast_dfm(case_name, stem, tf_idf)
```

Notice that in either case, our final data structure is incredibly sparse and of high dimensionality. Linguists have long worked on vector models for language that reduce this number of dimensions because of the nature of how people use language; the quote that opened this chapter dates to 1957.
