[
["index.html", "Predictive modeling with text Welcome to Predictive Modeling with Text in R", " Predictive modeling with text Emil Hvitfeldt and Julia Silge 2020-01-18 Welcome to Predictive Modeling with Text in R This is the website for Predictive Modeling with Text in R! Visit the GitHub repository for this site. This online work by Emil Hvitfeldt and Julia Silge is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["preface.html", "Preface Outline Topics this book will not cover Who is this book for? Code Data Acknowledgements", " Preface Modeling as a statistical practice can encompass a wide variety of activities. This book focuses on supervised or predictive modeling for text, using text data to make predictions about the world around us. The two types of models we train in this book are regression and classification. Think of regression models as predicting numeric, continuous quantities and classification models as predicting discrete quantities or class membership or labels. We use the tidymodels framework for modeling, with … Silge and Robinson (2017) provides a practical introduction to text mining with R using tidy data principles. If you have already started on the path of gaining insight from your text data, a next step is using that text directly in predictive modeling. Text data contains within it latent information that can be used for insight, understanding, and better decision-making, and predictive modeling with text can bring that information and insight to light. If you have already explored how to analyze text as demonstrated in Silge and Robinson (2017), this book will move one step further to show you how to learn and make predictions from that text data with supervised models. If you are unfamiliar with this previous work, this book will still provide a robust introduction to how text can be represented in useful ways for modeling and a diverse set of supervised modeling approaches for text. ONE MORE PARAGRAPH Outline The book is divided into three sections. We make a (perhaps arbitrary) distinction between machine learning methods and deep learning methods by defining deep learning as any kind of multi-layer neural network (LSTM, bi-LSTM, CNN) and machine learning as anything else (regularized regression, Bayes, SVM, random forest). We make this distinction because these different methods use separate software packages and modeling infrastructure; from a pragmatic point of view, it is helpful to split up the chapters this way. Natural language features: How do we transform text data into a representation useful for modeling? In these chapters, we explore the most common preprocessing steps for text, when they are helpful, and when they are not. Machine learning methods: We investigate the power of some of the simpler and more light-weight models in our arsenal. Deep learning methods: Given more time and resources, we see what is possible once we turn to neural networks. Some of the topics in the second and third sections will overlap as these provide different approaches to the same tasks. Topics this book will not cover This book serves as a thorough introduction to prediction and modeling with text, along with detailed practical examples, but there are many areas of natural language processing we do not cover. The CRAN Task View on Natural Language Processing provides details on other ways to use R for computational linguistics. Specific topics we do not cover include: Text generation Speech processing Machine translation Text-to-speech Who is this book for? This book is designed to provide practical guidance and directly applicable knowledge for data scientists and analysts who want to integrate text into their modeling pipelines. We assume that the reader is somewhat familiar with R, predictive modeling concepts for non-text data, and the tidyverse family of packages. For users who don’t have this background, we recommend books such as R for Data Science (Wickham and Grolemund 2017). Helpful resources for getting started with modeling and machine learning include a free interactive course developed by one of the authors (JS) and Hands-On Machine Learning with R (Boehmke and Greenwell 2019). We don’t assume an extensive background in text analysis, but Text Mining with R (Silge and Robinson 2017), by one of the authors (JS) and David Robinson, provides helpful skills in exploratory data analysis for text that will promote successful text modeling. This book is more advanced than Text Mining with R and will help practitioners use their text data in ways not covered in that book. Code All the code used to generate this book, including the figures and examples, is available in our public GitHub repository. The end of this book contains a software bibliography with descriptions of the software and packages being used; refer to this bibliography for details on all software. Each chapter or section will start by loading the packages used in it. Data Throughout the book, we will demonstrate with examples and build models using a selection of text datasets. A description of these datasets can be found in the data appendix. Acknowledgements We are so thankful for the contributions, help, and perspectives of people who have supported us in this project. There are several we would like to thank in particular. We would like to thank David Robinson for his collaboration on the tidytext package, … This book was written in the open, and several people contributed via pull requests or issues. Special thanks goes to those who contributed via GitHub: … References "],
["language.html", "1 Language and modeling 1.1 Linguistics for text analysis 1.2 A glimpse into one area: morphology 1.3 Different languages 1.4 Other ways text can vary 1.5 Summary", " 1 Language and modeling Machine learning and deep learning models for text are put into action by computers, but they are designed and trained by human beings. As natural language processing (NLP) practitioners, we bring our understanding of what language is and how language works to the task of building language models. This is true even when we don’t think about how language works very deeply or when our understanding is unsophisticated or inaccurate; speaking a language is not the same as understanding how it works. We can improve our machine learning models for text by heightening that understanding. Throughout the course of this book, we will discuss these kinds of improvements and how they are related to language. Data scientists involved in the everyday work of text analysis and text modeling typically don’t have formal training in how language works, but there is an entire field focused on exactly that, linguistics. 1.1 Linguistics for text analysis Briscoe (2013) provide helpful introductions to what linguistics is and how it intersects with the practical computational field of natural language processing. The broad field of linguistics includes subfields focusing on different aspects of language, which are somewhat hierarchical, as shown in Table 1.1. Table 1.1: Some subfields of linguistics, moving from smaller structures to broader structures Linguistics subfield What does it focus on? Phonetics Sounds that people use in language Phonology Systems of sounds in particular languages Morphology How words are formed Syntax How sentences are formed from words Semantics What sentences mean Pragmatics How language is used in context These fields each study a different level at which language exhibits organization. At the same time, this organization and the rules of language can be ambiguous. Beatrice Santorini, a linguist at the University of Pennsylvania, compiles examples of just such ambiguity from news headlines: Include Your Children When Baking Cookies March Planned For Next August Enraged Cow Injures Farmer with Ax Wives Kill Most Spouses In Chicago If you don’t have knowledge about what linguists study and what they know about language, these news headlines are just hilarious. To linguists, these are hilarious because they exhibit certain kinds of semantic ambiguity. Notice also that the first two subfields on this list are about sounds, i.e., speech. Most linguists view speech as primary, and writing down languge as text as a technological step. Remember that some language is signed, not spoken, so the description laid out here is limited. Written text is typically less creative and further from the primary language than we would wish. This points out how fundamentally limited modeling from written text is. Imagine the abstract language data we want exists in some high-dimensional latent space; we would like to extract that information using the text somehow, but it just isn’t completely possible. Any model we build is inherently limited. 1.2 A glimpse into one area: morphology How can a deeper knowledge of how language works inform text modeling? Let’s focus on morphology, the study of words’ internal structures and how they are formed, to illustrate this. Words are medium to small in length in English; English has a moderately low ratio of morphemes (the smallest unit of language with meaning) to words while other languages like Turkish and Russian have a higher ratio of morphemes to words (Bender 2013). A related idea is the categorization of languages as either more analytic (like Mandarin or modern English, breaking up concepts into separate words) or synthetic (like Hungarian or Swahili, combining concepts into one word). Morphology focuses on how morphemes such as prefixes, suffixes, and root words come together to form words. However, even the very question of what a word is turns out to be difficult, and not only for languages other than English. Compound words in English like “real estate” and “dining room” represent one concept but contain whitespace. The morphological characteristics of a text dataset are deeply connected to preprocessing steps like tokenization (Chapter 2), removing stop words (Chapter 3), and even stemming (Chapter 4). These preprocessing steps, in turn, have dramatic effects on model results. 1.2.0.1 TODO for Emil: add some additional discussion in this section for Danish. 1.3 Different languages We believe that most of the readers of this book are probably native English speakers, and most of the text used in training machine learning models is also English. However, English is by no means a dominant language globally, especially as a native or first language. As an example close to home for us, of the two authors of this book, one is a native English speaker and one is not. According to the comprehensive and detailed Ethnologue project, less than 20% of the world’s population speaks English at all. Bender (2011) provides guidance to computational linguists building models for text, for any language. One specific point she makes is to name the language being studied. Do state the name of the language that is being studied, even if it’s English. Acknowledging that we are working on a particular language foregrounds the possibility that the techniques may in fact be language specific. Conversely, neglecting to state that the particular data used were in, say, English, gives [a] false veneer of language-independence to the work. This idea is simple (acknowledge that the models we build are typically language specific) but the #BenderRule has led to increased awareness of the limitations of the current state of this field. Our book is not geared toward academic NLP researchers developing new methods, but toward data scientists and analysts working with everyday datasets; this issue is relevant even for us. Name the languages used in training models, and think through what that means for their generalizability. We will practice what we preach and tell you that most of the text used for modeling in this book is English, with some text in Danish. 1.4 Other ways text can vary The concept of differences in language is relevant for modeling beyond only the broadest language level (for example, English vs. Danish vs. German vs. Farsi). Language from a specific dialect often cannot be handled well with a model trained on data from the same language but not inclusive of that dialect. One dialect used in the United States is African American Vernacular English (AAVE). Models trained to detect toxic or hate speech are more likely to falsely identify AAVE as hate speech (Sap et al. 2019); this is deeply troubling not only because the model is less accurate than it should be, but because it amplifies harm against an already marginalized group. Language is also changing over time. This is a known characteristic of language; if you notice the evolution of your own language, don’t be depressed or angry, because it means that people are using it! Teenage girls are especially effective at language innovation, and have been for centures (McCulloch 2015); innovations spread from groups such as young women to other parts of society. This is another difference that impacts modeling. Differences in language relevant for models also include the use of slang, and even the context or medium of that text. Consider two bodies of text, both mostly standard written English, but one made up of tweets and one made up of legal documents. If an NLP practitioner trains a model on the dataset of tweets to predict some characteristic of the text, it is very possible (in fact, likely, in our experience) that the model will perform poorly if applied to the dataset of legal documents. Like machine learning in general, text modeling is exquisitely sensitive to the data used for training. This is why we are somewhat skeptical of AI products such as sentiment analysis APIs, not because they never work well, but because they work well only when the text you need to predict from is a good match to the text such a product was trained on. 1.5 Summary TODO References "],
["tokenization.html", "2 Tokenization 2.1 What is a token? 2.2 Types of tokens 2.3 Where does tokenization break down? 2.4 Building your own tokenizer 2.5 Tokenization benchmark 2.6 Summary", " 2 Tokenization In this chapter we will focus on the concepts of tokens, n-grams, tokenization, and how to perform tokenization in R. 2.1 What is a token? In R, text is typically represented with the character data type, similar to strings in other languages. Let’s explore some text from fairy tales by Hans Christian Andersen, available in the hcandersenr package (Hvitfeldt 2019a). If we look at the first paragraph of one story titled “The Fir Tree”, we find the text of the story is in a character vector: a series of letters, spaces, and punctuation stored as a vector. library(tokenizers) library(tidyverse) library(tidytext) library(hcandersenr) the_fir_tree &lt;- hcandersen_en %&gt;% filter(book == &quot;The fir tree&quot;) %&gt;% pull(text) head(the_fir_tree, 9) ## [1] &quot;Far down in the forest, where the warm sun and the fresh air made a sweet&quot; ## [2] &quot;resting-place, grew a pretty little fir-tree; and yet it was not happy, it&quot; ## [3] &quot;wished so much to be tall like its companions– the pines and firs which grew&quot; ## [4] &quot;around it. The sun shone, and the soft air fluttered its leaves, and the&quot; ## [5] &quot;little peasant children passed by, prattling merrily, but the fir-tree heeded&quot; ## [6] &quot;them not. Sometimes the children would bring a large basket of raspberries or&quot; ## [7] &quot;strawberries, wreathed on a straw, and seat themselves near the fir-tree, and&quot; ## [8] &quot;say, \\&quot;Is it not a pretty little tree?\\&quot; which made it feel more unhappy than&quot; ## [9] &quot;before.&quot; This character vector has nine atomic elements, each of which consists of a series of character symbols. These elements don’t contain any metadata or information to tell us which characters are words and which aren’t. Identifying these kinds of boundaries is where the process of tokenization comes in. In tokenization, we take an input (a string) and a token type (a meaningful unit of text, such as a word) and split the input in to pieces (tokens) that correspond to the type. (Manning, Raghavan, and Schütze 2008) TODO Most commonly, the meaningful unit or type of token that we want to split text into units of is a word. However, it is difficult to clearly define what a word is, for many or even most languages. Many languages, such as Chinese, do not use white space between words at all. Even languages that do use white space, including English, often have particular examples that are ambiguous (Bender 2013). Romance languages like Italian and French use pronouns and negation words that may better be considered prefixes with a space, and English contractions like &quot;didn't&quot; may more accurately be considered two words with no space. To understand the process of tokenization, let’s start by defining a word as any selection of alphanumeric (letters and numbers) symbols. Let’s use some regular expressions (or regex for short, see Appendix 12) with strsplit() to split the first two lines of “The Fir Tree” by any characters that are not alphanumeric. strsplit(the_fir_tree[1:2], &quot;[^a-zA-Z0-9]+&quot;) ## [[1]] ## [1] &quot;Far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; &quot;forest&quot; &quot;where&quot; &quot;the&quot; &quot;warm&quot; ## [9] &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fresh&quot; &quot;air&quot; &quot;made&quot; &quot;a&quot; &quot;sweet&quot; ## ## [[2]] ## [1] &quot;resting&quot; &quot;place&quot; &quot;grew&quot; &quot;a&quot; &quot;pretty&quot; &quot;little&quot; &quot;fir&quot; ## [8] &quot;tree&quot; &quot;and&quot; &quot;yet&quot; &quot;it&quot; &quot;was&quot; &quot;not&quot; &quot;happy&quot; ## [15] &quot;it&quot; At first sight, this result looks pretty decent. However, we have lost all punctuation, which may or may not be favorable, and the hero of this story (&quot;fir-tree&quot;) was split in half. Already it is clear that tokenization is going to be quite complicated. Luckily a lot of work has been invested in this process, and we will use these existing tools. For example, the tokenizers package (Mullen et al. 2018) contains a wealth of fast, consistent tokenizers we can use. library(tokenizers) tokenize_words(the_fir_tree[1:2]) ## [[1]] ## [1] &quot;far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; &quot;forest&quot; &quot;where&quot; &quot;the&quot; &quot;warm&quot; ## [9] &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fresh&quot; &quot;air&quot; &quot;made&quot; &quot;a&quot; &quot;sweet&quot; ## ## [[2]] ## [1] &quot;resting&quot; &quot;place&quot; &quot;grew&quot; &quot;a&quot; &quot;pretty&quot; &quot;little&quot; &quot;fir&quot; ## [8] &quot;tree&quot; &quot;and&quot; &quot;yet&quot; &quot;it&quot; &quot;was&quot; &quot;not&quot; &quot;happy&quot; ## [15] &quot;it&quot; We see sensible single-word results here; the tokenize_words() function uses the stringi package (Gagolewski 2019) under the hood, making it very fast. Word-level tokenization is done by finding word boundaries according to the specification from the International Components for Unicode (ICU). How does the word boundary algorithm (“Unicode Text Segmentation,” n.d.) work? Break at the start and end of text, unless the text is empty. Do not break within CRLF (new line characters). Otherwise, break before and after new lines (including CR and LF). Do not break within emoji zwj sequences. Keep horizontal whitespace together. Ignore Format and Extend characters, except after sot, CR, LF, and new line. Do not break between most letters. Do not break letters across certain punctuation. Do not break within sequences of digits, or digits adjacent to letters (“3a”, or “A3”). Do not break within sequences, such as “3.2” or “3,456.789”. Do not break between Katakana. Do not break from extenders. Do not break within emoji flag sequences. Otherwise, break everywhere (including around ideographs). While we might not understand what each and every step in this algorithm is doing, we can appreciate that it is many times more sophisticated than our initial approach of splitting on non-alphanumeric characters. In the rest of this book, we will use the tokenizers package as a baseline tokenizer for reference. Your choice of tokenizer will have an influence on your results, so don’t be afraid to experiment with different tokenizers or to write your own to fit your problem. 2.2 Types of tokens Thinking of a token as a word is a useful way to start understanding tokenization, even if it is hard to implement concretely in software. We can generalize the idea of a token beyond only a single word to other units of text. We can tokenize text at a variety of units including: characters, words, sentences, lines, paragraphs, and n-grams. In the following sections, we will explore how to tokenize text using the tokenizers package. These functions take a character vector as the input and return lists of character vectors as output. This same tokenization can also be done using the tidytext (Silge and Robinson 2016) package, for workflows using tidy data principles. sample_vector &lt;- c( &quot;This is the first of two strings&quot;, &quot;And here is the second string.&quot; ) sample_tibble &lt;- tibble(text = sample_vector) The tokenization achieved by using tokenize_words() on sample_vector: tokenize_words(sample_vector) ## [[1]] ## [1] &quot;this&quot; &quot;is&quot; &quot;the&quot; &quot;first&quot; &quot;of&quot; &quot;two&quot; &quot;strings&quot; ## ## [[2]] ## [1] &quot;and&quot; &quot;here&quot; &quot;is&quot; &quot;the&quot; &quot;second&quot; &quot;string&quot; will yield the same results as using unnest_tokens() on sample_tibble; the only difference is the data structure, and thus how we might use the result moving forward in our analysis. sample_tibble %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) ## # A tibble: 13 x 1 ## word ## &lt;chr&gt; ## 1 this ## 2 is ## 3 the ## 4 first ## 5 of ## 6 two ## 7 strings ## 8 and ## 9 here ## 10 is ## 11 the ## 12 second ## 13 string Arguments used in tokenize_words() can be passed through unnest_tokens() using the “the dots”, .... sample_tibble %&gt;% unnest_tokens(word, text, token = &quot;words&quot;, strip_punct = FALSE) ## # A tibble: 14 x 1 ## word ## &lt;chr&gt; ## 1 this ## 2 is ## 3 the ## 4 first ## 5 of ## 6 two ## 7 strings ## 8 and ## 9 here ## 10 is ## 11 the ## 12 second ## 13 string ## 14 . 2.2.1 Character tokens Perhaps the simplest tokenization is character tokenization, which splits texts into characters. Let’s use tokenize_characters() with its default parameters; this function has arguments to convert to lowercase and to strip all non-alphanumeric characters. These defaults will reduce the number of different tokens that are returned. The tokenize_*() functions by default return a list of character vectors, one character vector for each string in the input. tft_token_characters &lt;- tokenize_characters( x = the_fir_tree, lowercase = TRUE, strip_non_alphanum = TRUE, simplify = FALSE ) What do we see if we take a look? head(tft_token_characters) %&gt;% glimpse() ## List of 6 ## $ : chr [1:57] &quot;f&quot; &quot;a&quot; &quot;r&quot; &quot;d&quot; ... ## $ : chr [1:57] &quot;r&quot; &quot;e&quot; &quot;s&quot; &quot;t&quot; ... ## $ : chr [1:61] &quot;w&quot; &quot;i&quot; &quot;s&quot; &quot;h&quot; ... ## $ : chr [1:56] &quot;a&quot; &quot;r&quot; &quot;o&quot; &quot;u&quot; ... ## $ : chr [1:64] &quot;l&quot; &quot;i&quot; &quot;t&quot; &quot;t&quot; ... ## $ : chr [1:64] &quot;t&quot; &quot;h&quot; &quot;e&quot; &quot;m&quot; ... We don’t have to stick with the defaults. We can keep the punctuation and spaces by setting strip_non_alphanum = FALSE and now we see that spaces and punctuation are included in the results too. tokenize_characters( x = the_fir_tree, strip_non_alphanum = FALSE ) %&gt;% head() %&gt;% glimpse() ## List of 6 ## $ : chr [1:73] &quot;f&quot; &quot;a&quot; &quot;r&quot; &quot; &quot; ... ## $ : chr [1:74] &quot;r&quot; &quot;e&quot; &quot;s&quot; &quot;t&quot; ... ## $ : chr [1:76] &quot;w&quot; &quot;i&quot; &quot;s&quot; &quot;h&quot; ... ## $ : chr [1:72] &quot;a&quot; &quot;r&quot; &quot;o&quot; &quot;u&quot; ... ## $ : chr [1:77] &quot;l&quot; &quot;i&quot; &quot;t&quot; &quot;t&quot; ... ## $ : chr [1:77] &quot;t&quot; &quot;h&quot; &quot;e&quot; &quot;m&quot; ... TODO Find examples of when a character is hard to define. Look at Arabic, German (double s) and danish (double a). 2.2.2 Word tokens Tokenizing at the word level is perhaps the most common and widely used tokenization. We started our discussion of this topic with this kind of tokenization, and like we described before, this is the procedure of splitting text into words. To do this, let’s use the tokenize_words() function. tft_token_words &lt;- tokenize_words( x = the_fir_tree, lowercase = TRUE, stopwords = NULL, strip_punct = TRUE, strip_numeric = FALSE ) The results show us the input text split into individual words. head(tft_token_words) %&gt;% glimpse() ## List of 6 ## $ : chr [1:16] &quot;far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; ... ## $ : chr [1:15] &quot;resting&quot; &quot;place&quot; &quot;grew&quot; &quot;a&quot; ... ## $ : chr [1:15] &quot;wished&quot; &quot;so&quot; &quot;much&quot; &quot;to&quot; ... ## $ : chr [1:14] &quot;around&quot; &quot;it&quot; &quot;the&quot; &quot;sun&quot; ... ## $ : chr [1:12] &quot;little&quot; &quot;peasant&quot; &quot;children&quot; &quot;passed&quot; ... ## $ : chr [1:13] &quot;them&quot; &quot;not&quot; &quot;sometimes&quot; &quot;the&quot; ... We have already seen lowercase = TRUE, and strip_punct = TRUE and strip_numeric = FALSE control whether we remove punctuation and numeric characters respectively. We also have stopwords = NULL, which we will talk about in more depth in Chapter 3. Let’s create a tibble with two fairy tales, “The Fir Tree” and “The Little Mermaid”. Then we can use use unnest_tokens() together with some dplyr verbs to find the most commonly used words in each. hcandersen_en %&gt;% filter(book %in% c(&quot;The fir tree&quot;, &quot;The little mermaid&quot;)) %&gt;% unnest_tokens(word, text) %&gt;% count(book, word) %&gt;% group_by(book) %&gt;% arrange(desc(n)) %&gt;% slice(1:5) ## # A tibble: 10 x 3 ## # Groups: book [2] ## book word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 The fir tree the 278 ## 2 The fir tree and 161 ## 3 The fir tree tree 76 ## 4 The fir tree it 66 ## 5 The fir tree a 56 ## 6 The little mermaid the 817 ## 7 The little mermaid and 398 ## 8 The little mermaid of 252 ## 9 The little mermaid she 240 ## 10 The little mermaid to 199 The five most common words in each fairy tale are fairly uninformative, with the exception being &quot;tree&quot; in the “The Fir Tree”. These uninformative words are called stop words and will be explored in depth in Chapter 3. 2.2.3 Lines, sentence, and paragraph tokens Tokenizers to split text into larger units of text like lines, sentences, and paragraphs are rarely used directly for modeling purposes, as the tokens produced tend to be fairly unique. It is very uncommon for multiple sentences in a text to be identical! However, these tokenizers are useful for preprocessing and labeling. For example, Jane Austen’s novel Emma (as available in the janeaustenr package) is already preprocessed with each line being at most 80 characters long. However, it might be useful to split the data into chapters and paragraphs instead. Let’s create a function that takes a dataframe containing a variable called text and turns it into a dataframe where the the text is transformed to paragraphs. First, we can collapse the text into one long string using collapse = &quot;\\n&quot; to denote line breaks, and then next we can use tokenize_paragraphs() to identify the paragraphs and put them back into a dataframe. We can add a paragraph count with row_number(). add_paragraphs &lt;- function(data) { pull(data, text) %&gt;% paste(collapse = &quot;\\n&quot;) %&gt;% tokenize_paragraphs() %&gt;% unlist() %&gt;% tibble(text = .) %&gt;% mutate(paragraph = row_number()) } Now we take the raw text data and add the chapter count by detecting when the characters &quot;CHAPTER&quot; appear at the beginning of a line. Then we nest() the text column, apply our add_paragraphs() function, and then unnest() again. library(janeaustenr) emma_paragraphed &lt;- tibble(text = emma) %&gt;% mutate(chapter = cumsum(str_detect(text, &quot;^CHAPTER &quot;))) %&gt;% filter( chapter &gt; 0, !str_detect(text, &quot;^CHAPTER &quot;) ) %&gt;% nest(data = text) %&gt;% mutate(data = map(data, add_paragraphs)) %&gt;% unnest(cols = c(data)) glimpse(emma_paragraphed) ## Observations: 2,372 ## Variables: 3 ## $ chapter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ text &lt;chr&gt; &quot;Emma Woodhouse, handsome, clever, and rich, with a comfort… ## $ paragraph &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, … Now we have 2372 separate paragraphs we can analyse. Similarly, we could go a step further to split these chapters into sentences, lines, or words. 2.2.4 Tokenizing by n-grams An n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of \\(n\\) items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of \\(n\\) words. In this book, we will use n-gram to denote word n-grams unless otherwise stated. We use Latin prefixes, such that a 1-gram is called a unigram, 2-gram is called a bigram, 3-gram called a trigram and so on. We use Latin prefixes, so that a 1-gram is called a unigram, a 2-gram is called a bigram, a 3-gram called a trigram, and so on. Some example n-grams are: unigram: “Hello”, “day”, “my”, “little” bigram: “White House”, “happy dog”, “to be”, “Robin Hood” trigram: “You and I”, “please let go”, “no time like”, “great strong soldier” The benefit of using n-grams compared to words is that we can capture word order which would otherwise be lost. Similarly, when we use character n-grams, we can model the beginning and end of words, because a space will be located at the end of an n-gram for the end of a word and at the beginning of an n-gram of the beginning of a word. To split text into word n-grams, we can use the the function tokenize_ngrams(). It has a few more arguments, so let’s go over them one by one. tft_token_ngram &lt;- tokenize_ngrams( x = the_fir_tree, lowercase = TRUE, n = 3L, n_min = 3L, stopwords = character(), ngram_delim = &quot; &quot;, simplify = FALSE ) We have seen the arguments lowercase, stopwords, and simplify before; they work the same as for the other tokenizers. We also have n, the argument to determine which degree of n-gram to return. Using n = 1 returns unigrams, n = 2 bigrams, n = 3 gives trigrams, and so on. Related to n is the n_min argument, which specifies the minimum number of n-grams to include. By default both n and n_min are set to 3 making tokenize_ngrams() return only trigrams. By setting n = 3 and n_min = 1, we will get all unigrams, bigrams, and trigrams of a text. Lastly, we have the ngram_delim argument, which specifies the separator between words in the n-grams; notice that this defaults to a space. Let’s look at the result of n-gram tokenization for the first line of “The Fir Tree”. tft_token_ngram[[1]] ## [1] &quot;far down in&quot; &quot;down in the&quot; &quot;in the forest&quot; &quot;the forest where&quot; ## [5] &quot;forest where the&quot; &quot;where the warm&quot; &quot;the warm sun&quot; &quot;warm sun and&quot; ## [9] &quot;sun and the&quot; &quot;and the fresh&quot; &quot;the fresh air&quot; &quot;fresh air made&quot; ## [13] &quot;air made a&quot; &quot;made a sweet&quot; Notice how the words in the trigrams overlap so that the word “down” appears in the middle of the first trigram and beginning of the second trigram. N-gram tokenization slides along the text to create overlapping sets of tokens. It is important to choose the right value for n when using n-grams for the question we want to answer. Using unigrams is faster and more efficient, but we don’t capture information about word order. Using a higher value for n keeps more information, but the vector space of tokens increases dramatically, corresponding to a reduction in token counts. A sensible starting point in most cases is three. However, if you don’t have a large vocabulary in your dataset, consider starting at two instead three and experimenting from there. Figure 2.1 demonstrates how token frequency starts to decrease dramatically for trigrams and higher order n-grams. length_and_max &lt;- function(x) { tab &lt;- table(x) paste(length(tab), max(tab), sep = &quot;-&quot;) } plotting_data &lt;- hcandersen_en %&gt;% nest(data = c(text)) %&gt;% mutate(data = map_chr(data, ~ paste(.x$text, collapse = &quot; &quot;))) %&gt;% mutate( unigram = tokenize_ngrams(data, n = 1, n_min = 1) %&gt;% map_chr(length_and_max), bigram = tokenize_ngrams(data, n = 2, n_min = 2) %&gt;% map_chr(length_and_max), trigram = tokenize_ngrams(data, n = 3, n_min = 3) %&gt;% map_chr(length_and_max), quadrugram = tokenize_ngrams(data, n = 4, n_min = 4) %&gt;% map_chr(length_and_max) ) %&gt;% select(unigram, bigram, trigram, quadrugram) %&gt;% pivot_longer(cols = unigram:quadrugram, names_to = &quot;ngrams&quot;) %&gt;% separate(value, c(&quot;length&quot;, &quot;max&quot;), convert = TRUE) %&gt;% mutate(ngrams = factor(ngrams, levels = c(&quot;quadrugram&quot;, &quot;trigram&quot;, &quot;bigram&quot;, &quot;unigram&quot;))) plotting_data %&gt;% ggplot(aes(length, ngrams, color = max)) + geom_jitter(width = 0, alpha = 0.8, height = 0.35) + scale_color_viridis_c(trans = &quot;log&quot;, labels = scales::comma) + labs( x = &quot;Number of unique n-grams&quot;, y = NULL, color = &quot;Count of\\nmost frequent\\nngram&quot;, title = &quot;Unique n-grams by n-gram order&quot;, subtitle = &quot;Each point represents a H.C. Andersen Fairy tale&quot; ) Figure 2.1: Using longer n-grams results in a higher number of unique tokens with fewer counts 2.3 Where does tokenization break down? Tokenization will generally be one of first steps we take with the text data when building a model, so it is important to consider carefully what happens in this step of data preprocessing. As with most software there is a trade-off between speed and customizability, as demonstrated in section 2.5. The fastest tokenization methods give us little control over how it is done. While the defaults work well in many cases, we encounter situations where we want to impose stricter rules to get better tokenized results. Consider the following sentence. “Don’t forget you owe the bank $1 million for the house.” This sentence has several interesting parts which we need to decide whether to keep or to ignore when tokenizing. The first issue is the contraction in &quot;Don't&quot; which presents us with several possible options. The fastest option is to keep this as one word, but it could also be split up into &quot;do&quot; and &quot;n't&quot;. By performing such a split, we could learn whether contractions such as &quot;n't&quot; will be different then &quot;not&quot;, but we will also have a broader reduction as the words “wouldn’t” and “shouldn’t” will be split according to the same pattern. TODO FOR EMIL: THE LAST SENTENCE OF THE ABOVE PARAGRAPH IS UNCLEAR AND I CAN’T TELL WHAT IT IS INTENDING TO SAY The next issue at hand is how to deal with &quot;\\$1&quot;; the dollar sign is highly important part of this sentence as it denotes a kind of currency. We could either remove or keep this punctuation symbol, and if we keep the dollar sign, we can choose between keeping one or two tokens, &quot;\\$1&quot; or &quot;\\$&quot; and &quot;1&quot;. If we look at the default for tokenize_words(), we notice that it defaults to removing most punctuation including $. tokenize_words(&quot;$1&quot;) ## [[1]] ## [1] &quot;1&quot; We can keep the dollar sign if we don’t strip punctuation. tokenize_words(&quot;$1&quot;, strip_punct = FALSE) ## [[1]] ## [1] &quot;$&quot; &quot;1&quot; When dealing with this sentence, we also need to decide whether to keep the final period as a token or not. If we remove it, we will not be able to locate the last word in a sentence using n-grams. Information we can lose when we tokenize occurs more frequently in online and more casual text. Multiple spaces, extreme usage of exclamation characters, and deliberate use of capitalization can be completely lost depending on our choice of tokenizer and tokenization parameters. At the same time, it is not always worth keeping that kind of information about how text is being used. If we are studying trends in disease epidemics using Twitter data, the style the tweets are written with is likely not nearly as important as what words are used. However, if we are trying to model social groupings, language style and how individuals use language toward each other becomes much more important. TODO Do comparing of compression of data with different types of tokenizations hcandersen_en %&gt;% nest(text) %&gt;% mutate(data = map_chr(data, ~ paste(.x$text, collapse = &quot; &quot;))) %&gt;% mutate( chars = tokenize_characters(data) %&gt;% map_int(~ table(.x) %&gt;% length()), chars_non_alphanum = tokenize_characters(data, strip_non_alphanum = FALSE) %&gt;% map_int(~ table(.x) %&gt;% length()), words = tokenize_words(data) %&gt;% map_int(~ table(.x) %&gt;% length()), words_no_lowercase = tokenize_words(data, lowercase = FALSE) %&gt;% map_int(~ table(.x) %&gt;% length()), words_stems = tokenize_word_stems(data) %&gt;% map_int(~ table(.x) %&gt;% length()) ) %&gt;% select(-data) %&gt;% pivot_longer(-book) %&gt;% ggplot(aes(name, value)) + geom_boxplot() + geom_jitter(alpha = 0.1) + scale_y_log10() + theme_minimal() + coord_flip() + labs( title = &quot;Number of distinct tokens varies greatly with choice of tokenizer&quot;, x = NULL, y = &quot;Number of distinct tokens&quot; ) 2.4 Building your own tokenizer Sometimes the out-of-the-box tokenizers won’t be able to do what we need them to do. In this case, we will have to wield stringi/stringr and regular expressions (see Appendix 12). There are two main approaches to tokenization. Split the string up according to some rule. Extract tokens based on some rule. The number and complexity of our rules is determined by our desired outcome. We can reach complex outcomes by chaining together many smaller rules. In this section, we will implement a couple of specialty tokenizers to showcase these techniques. 2.4.1 Tokenize to characters, only keeping letters Here we want to make a modification to what tokenize_characters() does such that we only keep keep letters. Upon first thought, there are 2 main options. We can use tokenize_characters() and remove anything that is not a letter, or we can extract the letters one by one. Let’s try the latter option. This is an extract task and we will be using str_extract_all() as each string has the possibility of including more then 1 token. Since we want to extract letters we can use the letters character class [:alpha:] to match letters and the quantifier {1} to only extract the first one. In this example, leaving out the quantifier yields the same result as including it. However, for more complex regular expressions, specifying the quantifier allows the string handling to run faster. letter_tokens &lt;- str_extract_all( &quot;This sentence include 2 numbers and 1 period.&quot;, &quot;[:alpha:]{1}&quot; ) letter_tokens ## [[1]] ## [1] &quot;T&quot; &quot;h&quot; &quot;i&quot; &quot;s&quot; &quot;s&quot; &quot;e&quot; &quot;n&quot; &quot;t&quot; &quot;e&quot; &quot;n&quot; &quot;c&quot; &quot;e&quot; &quot;i&quot; &quot;n&quot; &quot;c&quot; &quot;l&quot; &quot;u&quot; &quot;d&quot; &quot;e&quot; ## [20] &quot;n&quot; &quot;u&quot; &quot;m&quot; &quot;b&quot; &quot;e&quot; &quot;r&quot; &quot;s&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;p&quot; &quot;e&quot; &quot;r&quot; &quot;i&quot; &quot;o&quot; &quot;d&quot; We may be tempted to specify the character class as something like [a-zA-Z]{1}. This option would in fact run faster, but we would lose non-English letter characters. This is a design choice we have to make depending on the goals of our specific problem. danish_sentence &lt;- &quot;Så mødte han en gammel heks på landevejen; hun var så ækel, hendes underlæbe hang hende lige ned på brystet.&quot; str_extract_all(danish_sentence, &quot;[:alpha:]&quot;) ## [[1]] ## [1] &quot;S&quot; &quot;å&quot; &quot;m&quot; &quot;ø&quot; &quot;d&quot; &quot;t&quot; &quot;e&quot; &quot;h&quot; &quot;a&quot; &quot;n&quot; &quot;e&quot; &quot;n&quot; &quot;g&quot; &quot;a&quot; &quot;m&quot; &quot;m&quot; &quot;e&quot; &quot;l&quot; &quot;h&quot; ## [20] &quot;e&quot; &quot;k&quot; &quot;s&quot; &quot;p&quot; &quot;å&quot; &quot;l&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;v&quot; &quot;e&quot; &quot;j&quot; &quot;e&quot; &quot;n&quot; &quot;h&quot; &quot;u&quot; &quot;n&quot; &quot;v&quot; ## [39] &quot;a&quot; &quot;r&quot; &quot;s&quot; &quot;å&quot; &quot;æ&quot; &quot;k&quot; &quot;e&quot; &quot;l&quot; &quot;h&quot; &quot;e&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;s&quot; &quot;u&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;r&quot; ## [58] &quot;l&quot; &quot;æ&quot; &quot;b&quot; &quot;e&quot; &quot;h&quot; &quot;a&quot; &quot;n&quot; &quot;g&quot; &quot;h&quot; &quot;e&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;l&quot; &quot;i&quot; &quot;g&quot; &quot;e&quot; &quot;n&quot; &quot;e&quot; ## [77] &quot;d&quot; &quot;p&quot; &quot;å&quot; &quot;b&quot; &quot;r&quot; &quot;y&quot; &quot;s&quot; &quot;t&quot; &quot;e&quot; &quot;t&quot; str_extract_all(danish_sentence, &quot;[a-zA-Z]&quot;) ## [[1]] ## [1] &quot;S&quot; &quot;m&quot; &quot;d&quot; &quot;t&quot; &quot;e&quot; &quot;h&quot; &quot;a&quot; &quot;n&quot; &quot;e&quot; &quot;n&quot; &quot;g&quot; &quot;a&quot; &quot;m&quot; &quot;m&quot; &quot;e&quot; &quot;l&quot; &quot;h&quot; &quot;e&quot; &quot;k&quot; ## [20] &quot;s&quot; &quot;p&quot; &quot;l&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;v&quot; &quot;e&quot; &quot;j&quot; &quot;e&quot; &quot;n&quot; &quot;h&quot; &quot;u&quot; &quot;n&quot; &quot;v&quot; &quot;a&quot; &quot;r&quot; &quot;s&quot; ## [39] &quot;k&quot; &quot;e&quot; &quot;l&quot; &quot;h&quot; &quot;e&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;s&quot; &quot;u&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;r&quot; &quot;l&quot; &quot;b&quot; &quot;e&quot; &quot;h&quot; &quot;a&quot; ## [58] &quot;n&quot; &quot;g&quot; &quot;h&quot; &quot;e&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;l&quot; &quot;i&quot; &quot;g&quot; &quot;e&quot; &quot;n&quot; &quot;e&quot; &quot;d&quot; &quot;p&quot; &quot;b&quot; &quot;r&quot; &quot;y&quot; &quot;s&quot; ## [77] &quot;t&quot; &quot;e&quot; &quot;t&quot; Choosing between [:alpha:] and [a-zA-Z] may seem quite similar, but the resulting differences can have a big impact on your analysis. 2.4.2 Allow for hyphenated words In our examples so far, we have noticed that the string “fir-tree” is typically split into two tokens. Let’s explore two different approaches for how to handle this hyphenated word as one token. First, let’s split on white space; this is a decent way to identify words in English and some other languages, and it does not split hyphenated words as the hyphen character isn’t considered a white-space. Second, let’s find a regex to match words with a hyphen and extract those. Splitting by white-space is not too difficult because we can use character classes, as show in Table 12.2. We will use the white space character class [:space:] to split our sentence. str_split(&quot;This isn&#39;t a sentence with hyphenated-words.&quot;, &quot;[:space:]&quot;) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;a&quot; ## [4] &quot;sentence&quot; &quot;with&quot; &quot;hyphenated-words.&quot; This worked pretty well. This version doesn’t drop punctuation, but we can achieve this by removing punctuation characters at the beginning and end of words. str_split(&quot;This isn&#39;t a sentence with hyphenated-words.&quot;, &quot;[:space:]&quot;) %&gt;% map(~ str_remove_all(.x, &quot;^[:punct:]+|[:punct:]+$&quot;)) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;a&quot; &quot;sentence&quot; ## [5] &quot;with&quot; &quot;hyphenated-words&quot; This regex used to remove the punctuation is a little complicated so let’s discuss it, piece by piece. The regex ^[:punct:]* will look at the beginning of the string (^) to match any punctuation characters ([:punct:]) where it will select one or more (+). The other regex [:punct:]+$ will look for punctuation characters ([:punct:]) that appear one or more times (+) at the end of the string ($). These will alternate (|) so that we get matches from both sides of the words. The reason we use the quantifier + is because there are cases where a word is followed by multiple characters we don’t want, such as &quot;okay...&quot; and &quot;Really?!!!&quot;. We are using map() since str_split() returns a list, and we want str_remove_all() to be applied to each element in the list. (The example here only has one element.) If you are in a situation where you want to avoid the dependencies that come with purrr, you can use lapply() instead. lapply(str_remove_all, pattern = “^[:punct:]+|[:punct:]+$”) Now let’s see if we can get the same result using extraction. We will start by constructing a regular expression that will capture hyphenated words; our definition here is a word with one hyphen located inside it. Since we want the hyphen to be inside the word, we will need to have a non-zero number of characters on either side of the hyphen. str_extract_all(&quot;This isn&#39;t a sentence with hyphenated-words.&quot;, &quot;[:alpha:]+-[:alpha:]+&quot;) ## [[1]] ## [1] &quot;hyphenated-words&quot; Wait, this only matched the hyphenated word! This happened because we are only matching words with hyphens. If we add the quantifier ? then we can match 0 or 1 occurrences. str_extract_all(&quot;This isn&#39;t a sentence with hyphenated-words.&quot;, &quot;[:alpha:]+-?[:alpha:]+&quot;) ## [[1]] ## [1] &quot;This&quot; &quot;isn&quot; &quot;sentence&quot; &quot;with&quot; ## [5] &quot;hyphenated-words&quot; Now we are getting more words, but the ending of &quot;isn't&quot; isn’t there anymore and we lost the word &quot;a&quot;. We can get matches for the whole contraction by expanding the character class [:alpha:] to include the character '. We do that by using [[:alpha:]']. str_extract_all(&quot;This isn&#39;t a sentence with hyphenated-words.&quot;, &quot;[[:alpha:]&#39;]+-?[[:alpha:]&#39;]+&quot;) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;sentence&quot; &quot;with&quot; ## [5] &quot;hyphenated-words&quot; Next we need to find out why &quot;a&quot; wasn’t matched. If we look at the regular expression, we remember that we imposed the restriction that a non-zero number of characters needed to surround the hyphen to avoid matching words that start or end with a hyphen. This means that the smallest possible pattern matched is 2 characters long. We can fix this by using an alternation with |. We will keep our previous match on the left-hand side, and include [:alpha:]{1} on the right-hand side to match the single length words that won’t be picked up by the left-hand side. Notice how we aren’t using [[:alpha:]'] since we are not interested in matching single ' characters. str_extract_all(&quot;This isn&#39;t a sentence with hyphenated-words.&quot;, &quot;[[:alpha:]&#39;]+-?[[:alpha:]&#39;]+|[:alpha:]{1}&quot;) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;a&quot; &quot;sentence&quot; ## [5] &quot;with&quot; &quot;hyphenated-words&quot; That is getting to be quite a complex regex, but we are now getting the same answer as before. 2.4.3 Character n-grams TODO change to toktok tokenizer Next let’s explore character n-grams. For the purpose of this example, a character n-gram is defined as a consecutive group of \\(n\\) characters. The matching will not extend over spaces, but will include overlapping matches within words. With this definiton, the 3-grams of &quot;nice dog&quot; would be &quot;nic&quot;, &quot;ice&quot;, &quot;dog&quot;. Since the regex engines in R normally don’t support overlapping matches, we have to get creative. First we will use a “lookahead” to find the location of all the matches, then we will use those locations to match the n-grams. sentence &lt;- c( &quot;This isn&#39;t a sentence with hyphenated-words.&quot;, &quot;Same with this one&quot; ) ngram_loc &lt;- str_locate_all(sentence, &quot;(?=(\\\\w{3}))&quot;) map2(ngram_loc, sentence, ~ str_sub(.y, .x[, 1], .x[, 1] + 2)) ## [[1]] ## [1] &quot;Thi&quot; &quot;his&quot; &quot;isn&quot; &quot;sen&quot; &quot;ent&quot; &quot;nte&quot; &quot;ten&quot; &quot;enc&quot; &quot;nce&quot; &quot;wit&quot; &quot;ith&quot; &quot;hyp&quot; ## [13] &quot;yph&quot; &quot;phe&quot; &quot;hen&quot; &quot;ena&quot; &quot;nat&quot; &quot;ate&quot; &quot;ted&quot; &quot;wor&quot; &quot;ord&quot; &quot;rds&quot; ## ## [[2]] ## [1] &quot;Sam&quot; &quot;ame&quot; &quot;wit&quot; &quot;ith&quot; &quot;thi&quot; &quot;his&quot; &quot;one&quot; 2.4.4 Wrapping it into a function We have shown how we can use regular expressions to extract the tokens we want, perhaps to use in modeling. So far, the code has been rather unstructured. We would ideally wrap these tasks into functions that can be used the same way tokenize_words() is used. Let’s start with the example with hyphenated words. To make the function a little more flexible, let’s add an option to transform all the output to lowercase. tokenize_hyphenated_words &lt;- function(x, lowercase = TRUE) { if (lowercase) { x &lt;- str_to_lower(x) } str_split(x, &quot;[:space:]&quot;) %&gt;% map(~ str_remove_all(.x, &quot;^[:punct:]+|[:punct:]+$&quot;)) } tokenize_hyphenated_words(the_fir_tree[1:3]) ## [[1]] ## [1] &quot;far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; &quot;forest&quot; &quot;where&quot; &quot;the&quot; &quot;warm&quot; ## [9] &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fresh&quot; &quot;air&quot; &quot;made&quot; &quot;a&quot; &quot;sweet&quot; ## ## [[2]] ## [1] &quot;resting-place&quot; &quot;grew&quot; &quot;a&quot; &quot;pretty&quot; ## [5] &quot;little&quot; &quot;fir-tree&quot; &quot;and&quot; &quot;yet&quot; ## [9] &quot;it&quot; &quot;was&quot; &quot;not&quot; &quot;happy&quot; ## [13] &quot;it&quot; ## ## [[3]] ## [1] &quot;wished&quot; &quot;so&quot; &quot;much&quot; &quot;to&quot; &quot;be&quot; ## [6] &quot;tall&quot; &quot;like&quot; &quot;its&quot; &quot;companions&quot; &quot;the&quot; ## [11] &quot;pines&quot; &quot;and&quot; &quot;firs&quot; &quot;which&quot; &quot;grew&quot; Notice how we transformed to lowercase first because the rest of the operations are case insensitive. Next let’s turn our character n-gram tokenizer into a function, with a variable n argument. tokenize_character_ngram &lt;- function(x, n) { ngram_loc &lt;- str_locate_all(x, paste0(&quot;(?=(\\\\w{&quot;, n, &quot;}))&quot;)) map2(ngram_loc, x, ~ str_sub(.y, .x[, 1], .x[, 1] + n - 1)) } tokenize_character_ngram(the_fir_tree[1:3], n = 3) ## [[1]] ## [1] &quot;Far&quot; &quot;dow&quot; &quot;own&quot; &quot;the&quot; &quot;for&quot; &quot;ore&quot; &quot;res&quot; &quot;est&quot; &quot;whe&quot; &quot;her&quot; &quot;ere&quot; &quot;the&quot; ## [13] &quot;war&quot; &quot;arm&quot; &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fre&quot; &quot;res&quot; &quot;esh&quot; &quot;air&quot; &quot;mad&quot; &quot;ade&quot; &quot;swe&quot; ## [25] &quot;wee&quot; &quot;eet&quot; ## ## [[2]] ## [1] &quot;res&quot; &quot;est&quot; &quot;sti&quot; &quot;tin&quot; &quot;ing&quot; &quot;pla&quot; &quot;lac&quot; &quot;ace&quot; &quot;gre&quot; &quot;rew&quot; &quot;pre&quot; &quot;ret&quot; ## [13] &quot;ett&quot; &quot;tty&quot; &quot;lit&quot; &quot;itt&quot; &quot;ttl&quot; &quot;tle&quot; &quot;fir&quot; &quot;tre&quot; &quot;ree&quot; &quot;and&quot; &quot;yet&quot; &quot;was&quot; ## [25] &quot;not&quot; &quot;hap&quot; &quot;app&quot; &quot;ppy&quot; ## ## [[3]] ## [1] &quot;wis&quot; &quot;ish&quot; &quot;she&quot; &quot;hed&quot; &quot;muc&quot; &quot;uch&quot; &quot;tal&quot; &quot;all&quot; &quot;lik&quot; &quot;ike&quot; &quot;its&quot; &quot;com&quot; ## [13] &quot;omp&quot; &quot;mpa&quot; &quot;pan&quot; &quot;ani&quot; &quot;nio&quot; &quot;ion&quot; &quot;ons&quot; &quot;the&quot; &quot;pin&quot; &quot;ine&quot; &quot;nes&quot; &quot;and&quot; ## [25] &quot;fir&quot; &quot;irs&quot; &quot;whi&quot; &quot;hic&quot; &quot;ich&quot; &quot;gre&quot; &quot;rew&quot; We can use paste0() in this function to construct an actual regex. 2.5 Tokenization benchmark TODO showcase other libraries for tokenization the_fir_tree1 &lt;- c(&quot;1&quot;, the_fir_tree) bench::mark( `[:alpha:]` = str_extract_all(the_fir_tree1, &quot;[:alpha:]&quot;), `[a-zA-Z]` = str_extract_all(the_fir_tree1, &quot;[a-zA-Z]&quot;), `[a-zA-Z]{1}` = str_extract_all(the_fir_tree1, &quot;[a-zA-Z]{1}&quot;), `[:Letter:]` = str_extract_all(the_fir_tree1, &quot;[:Letter:]&quot;) ) ## # A tibble: 4 x 6 ## expression min median `itr/sec` mem_alloc `gc/sec` ## &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt; &lt;dbl&gt; &lt;bch:byt&gt; &lt;dbl&gt; ## 1 [:alpha:] 2.26ms 2.44ms 411. 117KB 0 ## 2 [a-zA-Z] 1.35ms 1.53ms 653. 117KB 2.15 ## 3 [a-zA-Z]{1} 1.56ms 1.58ms 627. 117KB 0 ## 4 [:Letter:] 2.11ms 2.26ms 447. 117KB 2.16 2.6 Summary To build a predictive model, text data needs to be broken down into meaningful units, called tokens. These tokens range from individual characters to words to n-grams and even more complex structures, and the particular procedures of identifying tokens from text can be important. Fast and consistent tokenizers are available, but understanding how they behave and in what cirstumstances they work best will set you up for success. It’s also possible to build custom tokenizers when necessary. Once text data is tokenized, a common next preprocessing step is to consider how to handle very common words that are not very informative, stop words. Chapter 3 examines this in detail. References "],
["stopwords.html", "3 Stop words 3.1 Using premade stop word lists 3.2 Creating your own stop words list 3.3 All stop word lists are context specific 3.4 What happens when you remove stop words 3.5 Stop words in languages other than English 3.6 Summary", " 3 Stop words Once we have tokenized text into words, it often becomes clear that not all of these words carry the same amount of information with them, if any information at all. Words that don’t carry meaningful information are called stop words. It is common advice and practice to remove stop words for various NLP tasks, but the task of stop word removal is more nuanced than many resources may lead you to believe. In this chapter, we will investigate what a stop word list is, the differences between them, and the effects of using them in your preprocessing workflow. The concept of stop words has a long history with Hans Peter Luhn credited with coining the term back in 1960 (Luhn 1960). Examples of these words in English are “a”, “the”, “of”, and “didn’t”; these words are very common and don’t seem to add much to the meaning of a text other than ensuring the structure of the sentence is sound. Thinking of words as being either informative or non-informative is quite limiting, and we prefer to consider words as having a more fluid or continuous amount of information associated with them, where this information is context-specific as well. Historically, one of the main reasons for removing stop words was to decrease computational time for text mining; it can be regarded as a dimensionality reduction of text data and was commonly used in search engines to give better results (Huston and Croft 2010). 3.1 Using premade stop word lists A quick solution to getting a list of stop words is to use one that is already created for you. This is appealing because it requires a low level of effort, but beware that not all lists are created equal. Nothman, Qin, and Yurchak (2018) found some alarming results in a study of 52 stop word lists available in open-source software packages. Their unexpected findings included how different stop word lists have a varying number of words depending on the specificity of the list. Among some of the more grave issues were misspellings (“fify” instead of “fifty”), the inclusion of clearly informative words such as “computer” and “cry”, and various internal inconsistencies such as including the word “has” but not the word “does”. This is not to say that you should never use a stop word list that has been included in an open-source software project. However, you should always inspect and verify the list you are using, both to make sure it hasn’t changed since you used it last, and also to check that it is appropriate for your use case. There is a broad selection of stop word lists available today. For the purpose of this chapter we will focus on three lists of English stop words provided by the stopwords package (Benoit, Muhr, and Watanabe 2019). The first is from the SMART (System for the Mechanical Analysis and Retrieval of Text) Information Retrieval System, an information retrieval system developed at Cornell University in the 1960s (Lewis et al. 2004). The second is the English Snowball stop word list (Porter 2001), and the last is the English list from the Stopwords ISO collection. These stop word lists are all considered general purpose and not domain specific. Before we start delving into the content inside the lists, let’s take a look at how many words are included in each. library(stopwords) length(stopwords(source = &quot;smart&quot;)) length(stopwords(source = &quot;snowball&quot;)) length(stopwords(source = &quot;stopwords-iso&quot;)) ## [1] 571 ## [1] 175 ## [1] 1298 The length of these lists are quite varied, with the longest list being over seven times longer than the shortest! Let’s examine the overlap of the words that appear in the three lists in Figure 3.1. Figure 3.1: Set intersections for three common stop word lists These three lists are almost true subsets of each other. The only excepetion is a set of ten words that appear in Snowball and ISO but not in the SMART list. What are those words? setdiff( stopwords(source = &quot;snowball&quot;), stopwords(source = &quot;smart&quot;) ) ## [1] &quot;she&#39;s&quot; &quot;he&#39;d&quot; &quot;she&#39;d&quot; &quot;he&#39;ll&quot; &quot;she&#39;ll&quot; &quot;shan&#39;t&quot; &quot;mustn&#39;t&quot; ## [8] &quot;when&#39;s&quot; &quot;why&#39;s&quot; &quot;how&#39;s&quot; All these words are contractions. This is not because the SMART lexicon doesn’t include contractions, because if we look there are almost fifty of them. str_subset(stopwords(source = &quot;smart&quot;), &quot;&#39;&quot;) ## [1] &quot;a&#39;s&quot; &quot;ain&#39;t&quot; &quot;aren&#39;t&quot; &quot;c&#39;mon&quot; &quot;c&#39;s&quot; &quot;can&#39;t&quot; ## [7] &quot;couldn&#39;t&quot; &quot;didn&#39;t&quot; &quot;doesn&#39;t&quot; &quot;don&#39;t&quot; &quot;hadn&#39;t&quot; &quot;hasn&#39;t&quot; ## [13] &quot;haven&#39;t&quot; &quot;he&#39;s&quot; &quot;here&#39;s&quot; &quot;i&#39;d&quot; &quot;i&#39;ll&quot; &quot;i&#39;m&quot; ## [19] &quot;i&#39;ve&quot; &quot;isn&#39;t&quot; &quot;it&#39;d&quot; &quot;it&#39;ll&quot; &quot;it&#39;s&quot; &quot;let&#39;s&quot; ## [25] &quot;shouldn&#39;t&quot; &quot;t&#39;s&quot; &quot;that&#39;s&quot; &quot;there&#39;s&quot; &quot;they&#39;d&quot; &quot;they&#39;ll&quot; ## [31] &quot;they&#39;re&quot; &quot;they&#39;ve&quot; &quot;wasn&#39;t&quot; &quot;we&#39;d&quot; &quot;we&#39;ll&quot; &quot;we&#39;re&quot; ## [37] &quot;we&#39;ve&quot; &quot;weren&#39;t&quot; &quot;what&#39;s&quot; &quot;where&#39;s&quot; &quot;who&#39;s&quot; &quot;won&#39;t&quot; ## [43] &quot;wouldn&#39;t&quot; &quot;you&#39;d&quot; &quot;you&#39;ll&quot; &quot;you&#39;re&quot; &quot;you&#39;ve&quot; We seem to have stumbled upon an inconsistency; why does SMART include &quot;he's&quot; but not &quot;she's&quot;? It is hard to say, but this would be worth rectifying before applying these stop word lists to an analysis or model preprocessing. It is likely that this stop word list was generated by selecting the most frequent words across a large corpus of text that had more representation for text about men than women. This is once again a reminder that we should always look carefully at the premade word lists and other artifacts we use to make sure it works well with our needs. It is perfectly acceptable to start with a premade word list and remove or append additional words according to your particular use case. When you select a stop word list, it is important that you consider its size and breadth. Having a small and concise list of words can moderately reduce your token count while not having too great of an influence on your models, assuming that you picked appropriate words. As the size of your stop word list grows, each word added will have a diminishing positive effect with the increasing risk that a meaningful word has been placed on the list by mistake. In a later chapter on model building, we will show an example where we analyze the effects of different stop word lists. 3.1.1 Stop word removal in R Now that we have some stop word lists, we can move forward with removing these words. The particular way we remove stop words depends on the shape of our data. If you have your text in a tidy format with one word per row, you can use filter() from dplyr with a negated %in% if you have the stop words as a vector, or you can use anti_join() from dplyr if the stop words are in a tibble(). Like in our previous chapter, let’s examine the text of “The Fir-Tree” by Hans Christian Andersen, and use tidytext to tokenize the text into words. library(hcandersenr) library(tidyverse) library(tidytext) fir_tree &lt;- hca_fairytales() %&gt;% filter( book == &quot;The fir tree&quot;, language == &quot;English&quot; ) tidy_fir_tree &lt;- fir_tree %&gt;% unnest_tokens(word, text) And we can use the Snowball stop word list as an example. Since the stop words return from this function as a vector, we will use filter(). tidy_fir_tree %&gt;% filter(!(tidy_fir_tree$word %in% stopwords(source = &quot;snowball&quot;))) ## # A tibble: 1,547 x 3 ## book language word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The fir tree English far ## 2 The fir tree English forest ## 3 The fir tree English warm ## 4 The fir tree English sun ## 5 The fir tree English fresh ## 6 The fir tree English air ## 7 The fir tree English made ## 8 The fir tree English sweet ## 9 The fir tree English resting ## 10 The fir tree English place ## # … with 1,537 more rows If we use the get_stopwords() function from tidytext instead, then we can use the anti_join() function. tidy_fir_tree %&gt;% anti_join(get_stopwords(source = &quot;snowball&quot;)) ## # A tibble: 1,547 x 3 ## book language word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The fir tree English far ## 2 The fir tree English forest ## 3 The fir tree English warm ## 4 The fir tree English sun ## 5 The fir tree English fresh ## 6 The fir tree English air ## 7 The fir tree English made ## 8 The fir tree English sweet ## 9 The fir tree English resting ## 10 The fir tree English place ## # … with 1,537 more rows The result of these two stop word removals is the same since we used the same stop word list in both cases. 3.2 Creating your own stop words list Another way to get a stop word list is to create one yourself. Let’s explore a few different ways to find appropriate words to use. We will use the tokenized data from “The Fir-Tree” as a first example. Let’s take the words and rank them by their count or frequency. Figure 3.2: We counted words in “The Fir Tree” and ordered them by count or frequency. We recognize many of what we would consider stop words in the first column here, with three big exceptions. We see &quot;tree&quot; at 3, &quot;fir&quot; at 12 and &quot;little&quot; at 22. These words appear high on our list but do provide valuable information as they all reference the main character. What went wrong with this approach? Creating a stop word list using high-frequency words works best when it is created on a corpus of documents, not individual documents. This is because the words found in a single document will be document specific and the overall pattern of words will not generalize that well. In NLP, a corpus is a set of texts or documents. The set of Hans Christian Andersen’s fairy tales can be considered a corpus, with each fairy tale a document within that corpus. The set of United States Supreme Court opinions can be considered a different corpus, with each written opinion being a document within that corpus. The word &quot;tree&quot; does seem important as it is about the main character, but it could also be appearing so often that it stops providing any information. Let’s try a different approach, extracting high-frequency words from the corpus of all English fairy tales by H.C. Andersen. Figure 3.3: We counted words in all English fairy tales by Hans Christian Andersen and ordered them by count or frequency. This list is more appropriate for our concept of stop words, and now it is time for us to make some choices. How many do we want to include in our stop word list? Which words should we add and/or remove based on prior information? Selecting the number of words to remove is best done by a case-by-case basis as it can be difficult to determine apriori how many different “meaningless” words appear in a corpus. Our suggestion is to start with a low number like twenty and increase by ten words until you get to words that are not appropriate as stop words for your analytical purpose. It is worth keeping in mind that this list is not perfect. It is based on the corpus of documents we had available, which is potentially biased since all the fairy tales were written by the same European white male from the early 1800s. This bias can be minimized by removing words we would expect to be over-represented or to add words we expect to be under-represented. Easy examples are to include the compliments to the words in the lists if they are not present. Include &quot;big&quot; if &quot;small&quot; is present, &quot;old&quot; if &quot;young&quot; is present. This example list has words associated with women often listed lower in rank than words associated with men. With &quot;man&quot; being at rank 79, but &quot;woman&quot; at rank 179, choosing a threshold of 100 would lead to only one of these words be included. Depending on how important you think such nouns are going to be in your texts, either add &quot;woman&quot; or delete &quot;man&quot;. Figure 3.4 shows how the words associated with men have higher rank than the words associated with women. By using a single threshold to create a stop word list, you would likely only include one form of such words. Figure 3.4: We counted tokens and ranked according to total. Rank 1 has most occurrences. Imagine now we would like to create a stop word list that spans multiple different genres, in such a way that the subject-specific stop words don’t overlap. For this case, we would like words to be denoted as a stop word only if it is a stop word in all the genres. You could find the words individually in each genre and using the right intersections. However, that approach might take a substantial amount of time. Below is a bad example where we try to create a multi-language list of stop words. To accomplish this we calculate the inverse document frequency (IDF) of each word, and create the stop word list based on the words with the lowest IDF. The following function takes a tokenized dataframe and returns a dataframe with a column for each word and a column for the IDF. library(rlang) calc_idf &lt;- function(df, word, document) { words &lt;- df %&gt;% pull({{ word }}) %&gt;% unique() n_docs &lt;- length(unique(pull(df, {{ document }}))) n_words &lt;- df %&gt;% nest(data = c({{ word }})) %&gt;% pull(data) %&gt;% map_dfc(~ words %in% unique(pull(.x, {{ word }}))) %&gt;% rowSums() tibble( word = words, idf = log(n_docs / n_words) ) } Here is the result where we try to create a cross-language list of stop words, by taking each fairy tale as a document. It is not very good! The overlap between what words appear in each language is very small, and that is what we mostly see in this list. Figure 3.5: We counted words from all of H.C. Andersen’s fairy tales in Danish, English, French, German, and Spanish and ordered by count or frequency. TODO do same example with English only. do MP, VP and SAT https://pdfs.semanticscholar.org/c543/8e216071f6180c228cc557fb1d3c77edb3a3.pdf 3.3 All stop word lists are context specific Since all work related to the text is specific to context, it is important to make sure that the stop word list you use reflects the word space that you are planning on using it on. One common concern to consider is how pronouns bring information to your text. Pronouns are included in many different stop word lists (although inconsistently) and they will often not be noise in text data. On the other hand, sometimes you will have to add in words yourself, depending on the domain. If you are working with texts for dessert recipes, certain ingredients (sugar, eggs, water) and actions (whisking, baking, stirring) may be frequent enough to pass your stop word threshold, but it’s possible you will want to keep them as they may be informative. Throwing away “eggs” as a common word would make it harder or downright impossible to determine if certain recipes are vegan or not, while whisking and stirring may be fine to remove as distinguishing between recipes that do and don’t require a whisk might not be that big of a deal. 3.4 What happens when you remove stop words We have discussed different ways of finding and removing stop words; now let’s see what happens once you do remove them. First, let’s explore the impact of the number of words that are included in the list. Figure 3.6 shows what percentage of words are removed as a function of the number of words in a text. The different colors represent the 3 different stop word lists we have considered in this chapter. Figure 3.6: Proportion of words removed for different stop word lists and different document lengths We notice, as we would predict, that larger stop word lists remove more words then shorter stop word lists. In this example with fairy tales, over half of the words have been removed, with the largest list removing over 80% of the words. We observe that shorter texts have a lower percentage of stop words. Since we are looking at fairy tales, this could be explained by the fact that a story has to be told regardless of the length of the fairy tale, so shorter texts are going to be more dense with more informative words. Another problem you might have is dealing with misspellings. Most premade stop word lists assume that all the words are spelled correctly. Handling misspellings when using premade lists can be done by manually adding common misspellings. You could imagine creating all words that are a certain string distance away from the stop words, but we do not recommend this as you would quickly include informative words this way. One of the downsides of creating your own stop word lists using frequencies is that you are limited to using words that you have already observed. It could happen that &quot;she'd&quot; is included in your training corpus but the word &quot;he'd&quot; did not reach the threshold This is a case where you need to look at your words and adjust accordingly. Here the large premade stop word lists can serve as inspiration for missing words. In a later chapter (TODO add link) will we investigate the influence of removing stop words in the context of modeling. Given the right list of words, you see no harm to the model performance, and may even see improvement in result due to noise reduction (R. Feldman and Sanger 2007). 3.5 Stop words in languages other than English So far in this chapter, we have been spent the majority of the time on the English language, but English is not representative of every language. The stop word lists we examined in this chapter have been English and the notion of “short” and “long” lists we have used here are specific to English as a language. You should expect different languages to have a varying number of “uninformative” words, and for this number to depend on the morphological richness of a language; lists that contain all possible morphological variants of each stop word could become quite large. Different languages have different numbers of words in each class of words. An example is how the grammatical case influences the articles used in German. Below are a couple of diagrams showing the use of definite and indefinite articles in German. Notice how German nouns have three genders (masculine, feminine, and neuter), which are not uncommon in languages around the world. Articles are almost always considered as stop words in English as they carry very little information. However, German articles give some indication of the case which can be used when selecting a list of stop words in German or any other language where the grammatical case is reflected in the text. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rvuososmls .gt_table { display: table; border-collapse: collapse; margin-left: auto; /* table.margin.left */ margin-right: auto; /* table.margin.right */ color: #333333; font-size: 16px; /* table.font.size */ background-color: #FFFFFF; /* table.background.color */ width: auto; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #rvuososmls .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; /* table.background.color */ border-left-style: hidden; /* heading.border.lr.style */ border-left-width: 1px; /* heading.border.lr.width */ border-left-color: #D3D3D3; /* heading.border.lr.color */ border-right-style: hidden; /* heading.border.lr.style */ border-right-width: 1px; /* heading.border.lr.width */ border-right-color: #D3D3D3; /* heading.border.lr.color */ } #rvuososmls .gt_title { color: #333333; font-size: 125%; /* heading.title.font.size */ font-weight: initial; /* heading.title.font.weight */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; /* table.background.color */ border-bottom-width: 0; } #rvuososmls .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ font-weight: initial; /* heading.subtitle.font.weight */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; /* table.background.color */ border-top-width: 0; } #rvuososmls .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #rvuososmls .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #rvuososmls .gt_col_headings { border-top-style: solid; /* column_labels.border.top.style */ border-top-width: 2px; /* column_labels.border.top.width */ border-top-color: #D3D3D3; /* column_labels.border.top.color */ border-bottom-style: solid; /* column_labels.border.bottom.style */ border-bottom-width: 2px; /* column_labels.border.bottom.width */ border-bottom-color: #D3D3D3; /* column_labels.border.bottom.color */ border-left-style: none; /* column_labels.border.lr.style */ border-left-width: 1px; /* column_labels.border.lr.width */ border-left-color: #D3D3D3; /* column_labels.border.lr.color */ border-right-style: none; /* column_labels.border.lr.style */ border-right-width: 1px; /* column_labels.border.lr.width */ border-right-color: #D3D3D3; /* column_labels.border.lr.color */ } #rvuososmls .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 100%; /* column_labels.font.size */ font-weight: normal; /* column_labels.font.weight */ text-transform: inherit; /* column_labels.text_transform */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #rvuososmls .gt_sep_right { border-right: 5px solid #FFFFFF; } #rvuososmls .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ text-transform: inherit; /* row_group.text_transform */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ border-left-style: none; /* row_group.border.left.style */ border-left-width: 1px; /* row_group.border.left.width */ border-left-color: #D3D3D3; /* row_group.border.left.color */ border-right-style: none; /* row_group.border.right.style */ border-right-width: 1px; /* row_group.border.right.width */ border-right-color: #D3D3D3; /* row_group.border.right.color */ vertical-align: middle; } #rvuososmls .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #rvuososmls .gt_striped { background-color: rgba(128, 128, 128, 0.05); /* row.striping.background_color */ } #rvuososmls .gt_from_md > :first-child { margin-top: 0; } #rvuososmls .gt_from_md > :last-child { margin-bottom: 0; } #rvuososmls .gt_row { padding-top: 8px; /* data_row.padding */ padding-bottom: 8px; /* data_row.padding */ padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; /* table_body.hlines.style */ border-top-width: 1px; /* table_body.hlines.width */ border-top-color: #D3D3D3; /* table_body.hlines.color */ border-left-style: none; /* table_body.vlines.style */ border-left-width: 1px; /* table_body.vlines.width */ border-left-color: #D3D3D3; /* table_body.vlines.color */ border-right-style: none; /* table_body.vlines.style */ border-right-width: 1px; /* table_body.vlines.width */ border-right-color: #D3D3D3; /* table_body.vlines.color */ vertical-align: middle; overflow-x: hidden; } #rvuososmls .gt_stub { color: #333333; background-color: #FFFFFF; /* stub.background.color */ font-weight: initial; /* stub.font.weight */ text-transform: inherit; /* stub.text_transform */ border-right-style: solid; /* stub.border.style */ border-right-width: 2px; /* stub.border.width */ border-right-color: #D3D3D3; /* stub.border.color */ padding-left: 12px; } #rvuososmls .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ text-transform: inherit; /* summary_row.text_transform */ padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; } #rvuososmls .gt_first_summary_row { padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: solid; /* summary_row.border.style */ border-top-width: 2px; /* summary_row.border.width */ border-top-color: #D3D3D3; /* summary_row.border.color */ } #rvuososmls .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ text-transform: inherit; /* grand_summary_row.text_transform */ padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; } #rvuososmls .gt_first_grand_summary_row { padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: double; /* grand_summary_row.border.style */ border-top-width: 6px; /* grand_summary_row.border.width */ border-top-color: #D3D3D3; /* grand_summary_row.border.color */ } #rvuososmls .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #rvuososmls .gt_footnotes { color: #333333; background-color: #FFFFFF; /* footnotes.background.color */ border-bottom-style: none; /* footnotes.border.bottom.style */ border-bottom-width: 2px; /* footnotes.border.bottom.width */ border-bottom-color: #D3D3D3; /* footnotes.border.bottom.color */ border-left-style: none; /* footnotes.border.lr.color */ border-left-width: 2px; /* footnotes.border.lr.color */ border-left-color: #D3D3D3; /* footnotes.border.lr.color */ border-right-style: none; /* footnotes.border.lr.color */ border-right-width: 2px; /* footnotes.border.lr.color */ border-right-color: #D3D3D3; /* footnotes.border.lr.color */ } #rvuososmls .gt_footnote { margin: 0px; font-size: 90%; /* footnotes.font.size */ padding: 4px; /* footnotes.padding */ } #rvuososmls .gt_sourcenotes { color: #333333; background-color: #FFFFFF; /* source_notes.background.color */ border-bottom-style: none; /* source_notes.border.bottom.style */ border-bottom-width: 2px; /* source_notes.border.bottom.width */ border-bottom-color: #D3D3D3; /* source_notes.border.bottom.color */ border-left-style: none; /* source_notes.border.lr.style */ border-left-width: 2px; /* source_notes.border.lr.style */ border-left-color: #D3D3D3; /* source_notes.border.lr.style */ border-right-style: none; /* source_notes.border.lr.style */ border-right-width: 2px; /* source_notes.border.lr.style */ border-right-color: #D3D3D3; /* source_notes.border.lr.style */ } #rvuososmls .gt_sourcenote { font-size: 90%; /* source_notes.font.size */ padding: 4px; /* source_notes.padding */ } #rvuososmls .gt_left { text-align: left; } #rvuososmls .gt_center { text-align: center; } #rvuososmls .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rvuososmls .gt_font_normal { font-weight: normal; } #rvuososmls .gt_font_bold { font-weight: bold; } #rvuososmls .gt_font_italic { font-style: italic; } #rvuososmls .gt_super { font-size: 65%; } #rvuososmls .gt_footnote_marks { font-style: italic; font-size: 65%; } German Definite Articles (the) Masculine Feminine Neuter Plural Nominative der die das die Accusative den die das die Dative dem der dem den Genitive des der des der html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #tqwnglndza .gt_table { display: table; border-collapse: collapse; margin-left: auto; /* table.margin.left */ margin-right: auto; /* table.margin.right */ color: #333333; font-size: 16px; /* table.font.size */ background-color: #FFFFFF; /* table.background.color */ width: auto; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #tqwnglndza .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; /* table.background.color */ border-left-style: hidden; /* heading.border.lr.style */ border-left-width: 1px; /* heading.border.lr.width */ border-left-color: #D3D3D3; /* heading.border.lr.color */ border-right-style: hidden; /* heading.border.lr.style */ border-right-width: 1px; /* heading.border.lr.width */ border-right-color: #D3D3D3; /* heading.border.lr.color */ } #tqwnglndza .gt_title { color: #333333; font-size: 125%; /* heading.title.font.size */ font-weight: initial; /* heading.title.font.weight */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; /* table.background.color */ border-bottom-width: 0; } #tqwnglndza .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ font-weight: initial; /* heading.subtitle.font.weight */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; /* table.background.color */ border-top-width: 0; } #tqwnglndza .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #tqwnglndza .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #tqwnglndza .gt_col_headings { border-top-style: solid; /* column_labels.border.top.style */ border-top-width: 2px; /* column_labels.border.top.width */ border-top-color: #D3D3D3; /* column_labels.border.top.color */ border-bottom-style: solid; /* column_labels.border.bottom.style */ border-bottom-width: 2px; /* column_labels.border.bottom.width */ border-bottom-color: #D3D3D3; /* column_labels.border.bottom.color */ border-left-style: none; /* column_labels.border.lr.style */ border-left-width: 1px; /* column_labels.border.lr.width */ border-left-color: #D3D3D3; /* column_labels.border.lr.color */ border-right-style: none; /* column_labels.border.lr.style */ border-right-width: 1px; /* column_labels.border.lr.width */ border-right-color: #D3D3D3; /* column_labels.border.lr.color */ } #tqwnglndza .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 100%; /* column_labels.font.size */ font-weight: normal; /* column_labels.font.weight */ text-transform: inherit; /* column_labels.text_transform */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #tqwnglndza .gt_sep_right { border-right: 5px solid #FFFFFF; } #tqwnglndza .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ text-transform: inherit; /* row_group.text_transform */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ border-left-style: none; /* row_group.border.left.style */ border-left-width: 1px; /* row_group.border.left.width */ border-left-color: #D3D3D3; /* row_group.border.left.color */ border-right-style: none; /* row_group.border.right.style */ border-right-width: 1px; /* row_group.border.right.width */ border-right-color: #D3D3D3; /* row_group.border.right.color */ vertical-align: middle; } #tqwnglndza .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #tqwnglndza .gt_striped { background-color: rgba(128, 128, 128, 0.05); /* row.striping.background_color */ } #tqwnglndza .gt_from_md > :first-child { margin-top: 0; } #tqwnglndza .gt_from_md > :last-child { margin-bottom: 0; } #tqwnglndza .gt_row { padding-top: 8px; /* data_row.padding */ padding-bottom: 8px; /* data_row.padding */ padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; /* table_body.hlines.style */ border-top-width: 1px; /* table_body.hlines.width */ border-top-color: #D3D3D3; /* table_body.hlines.color */ border-left-style: none; /* table_body.vlines.style */ border-left-width: 1px; /* table_body.vlines.width */ border-left-color: #D3D3D3; /* table_body.vlines.color */ border-right-style: none; /* table_body.vlines.style */ border-right-width: 1px; /* table_body.vlines.width */ border-right-color: #D3D3D3; /* table_body.vlines.color */ vertical-align: middle; overflow-x: hidden; } #tqwnglndza .gt_stub { color: #333333; background-color: #FFFFFF; /* stub.background.color */ font-weight: initial; /* stub.font.weight */ text-transform: inherit; /* stub.text_transform */ border-right-style: solid; /* stub.border.style */ border-right-width: 2px; /* stub.border.width */ border-right-color: #D3D3D3; /* stub.border.color */ padding-left: 12px; } #tqwnglndza .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ text-transform: inherit; /* summary_row.text_transform */ padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; } #tqwnglndza .gt_first_summary_row { padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: solid; /* summary_row.border.style */ border-top-width: 2px; /* summary_row.border.width */ border-top-color: #D3D3D3; /* summary_row.border.color */ } #tqwnglndza .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ text-transform: inherit; /* grand_summary_row.text_transform */ padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; } #tqwnglndza .gt_first_grand_summary_row { padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: double; /* grand_summary_row.border.style */ border-top-width: 6px; /* grand_summary_row.border.width */ border-top-color: #D3D3D3; /* grand_summary_row.border.color */ } #tqwnglndza .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #tqwnglndza .gt_footnotes { color: #333333; background-color: #FFFFFF; /* footnotes.background.color */ border-bottom-style: none; /* footnotes.border.bottom.style */ border-bottom-width: 2px; /* footnotes.border.bottom.width */ border-bottom-color: #D3D3D3; /* footnotes.border.bottom.color */ border-left-style: none; /* footnotes.border.lr.color */ border-left-width: 2px; /* footnotes.border.lr.color */ border-left-color: #D3D3D3; /* footnotes.border.lr.color */ border-right-style: none; /* footnotes.border.lr.color */ border-right-width: 2px; /* footnotes.border.lr.color */ border-right-color: #D3D3D3; /* footnotes.border.lr.color */ } #tqwnglndza .gt_footnote { margin: 0px; font-size: 90%; /* footnotes.font.size */ padding: 4px; /* footnotes.padding */ } #tqwnglndza .gt_sourcenotes { color: #333333; background-color: #FFFFFF; /* source_notes.background.color */ border-bottom-style: none; /* source_notes.border.bottom.style */ border-bottom-width: 2px; /* source_notes.border.bottom.width */ border-bottom-color: #D3D3D3; /* source_notes.border.bottom.color */ border-left-style: none; /* source_notes.border.lr.style */ border-left-width: 2px; /* source_notes.border.lr.style */ border-left-color: #D3D3D3; /* source_notes.border.lr.style */ border-right-style: none; /* source_notes.border.lr.style */ border-right-width: 2px; /* source_notes.border.lr.style */ border-right-color: #D3D3D3; /* source_notes.border.lr.style */ } #tqwnglndza .gt_sourcenote { font-size: 90%; /* source_notes.font.size */ padding: 4px; /* source_notes.padding */ } #tqwnglndza .gt_left { text-align: left; } #tqwnglndza .gt_center { text-align: center; } #tqwnglndza .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #tqwnglndza .gt_font_normal { font-weight: normal; } #tqwnglndza .gt_font_bold { font-weight: bold; } #tqwnglndza .gt_font_italic { font-style: italic; } #tqwnglndza .gt_super { font-size: 65%; } #tqwnglndza .gt_footnote_marks { font-style: italic; font-size: 65%; } German Indefinite Articles (a/an) Masculine Feminine Neuter Plural Nominative ein eine ein keine Accusative einen eine ein keine Dative einem einer einem keinen Genitive eines einer eines keiner Building lists of stop words in Chinese has been done both manually and automatically (F. Zou, Wang, Deng, Han, et al. 2006) but so far none has been accepted as a standard (F. Zou, Wang, Deng, and Han 2006). A full discussion of stop word identification in Chinese text would be out of scope for this book, so we will just highlight some of the challenges that differentiate it from English. Chinese text is much more complex than portrayed here. With different systems and billions of users, there is much we won’t be able to touch on here. The main difference from English is the use of logograms instead of letters to convey information. However, Chinese characters should not be confused with Chinese words. The majority of words in modern Chinese are composed of multiple characters. This means that inferring the presence of words is more complicated and the notion of stop words will affect how this segmentation of characters is done. 3.6 Summary TODO References "],
["stemming.html", "4 Stemming 4.1 How to stem text in R 4.2 Should you use stemming at all? 4.3 Understand a stemming algorithm 4.4 Handling punctuation when stemming 4.5 Compare some stemming options 4.6 Lemmatization and stemming 4.7 Stemming and stop words 4.8 Summary", " 4 Stemming When we deal with text, often documents contain different versions of one base word, often called a stem. “The Fir-Tree”, for example, contains more than one version (i.e., inflected form) of the word &quot;tree&quot;. library(hcandersenr) library(tidyverse) library(tidytext) fir_tree &lt;- hca_fairytales() %&gt;% filter( book == &quot;The fir tree&quot;, language == &quot;English&quot; ) tidy_fir_tree &lt;- fir_tree %&gt;% unnest_tokens(word, text) %&gt;% anti_join(get_stopwords()) tidy_fir_tree %&gt;% count(word, sort = TRUE) %&gt;% filter(str_detect(word, &quot;^tree&quot;)) ## # A tibble: 3 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 tree 76 ## 2 trees 12 ## 3 tree&#39;s 1 Trees, we see once again, are important in this story; the singular form appears 76 times and the plural form appears twelve times. (We’ll come back to how we might handle the apostrophe in &quot;tree's&quot; later in this chapter.) What if we aren’t interested in the difference between &quot;trees&quot; and &quot;tree&quot; and we want to treat both together? That idea is at the heart of stemming, the process of identifying the base word (or stem) for a dataset of words. Stemming is concerned with the linguistics subfield of morphology, how words are formed. In this example, &quot;trees&quot; would lose its letter &quot;s&quot; while &quot;tree&quot; stays the same. If we counted word frequencies again after stemming, we would find that there are 88 occurrences of the stem &quot;tree&quot; (89, if we also find the stem for &quot;tree's&quot;). 4.1 How to stem text in R There have been many algorithms built for stemming words over the past half century or so; we’ll focus on two approaches. The first is the stemming algorithm of Porter (1980), probably the most widely used stemmer for English. Porter himself released the algorithm implemented in the framework Snowball with an open-source license; you can use it from R via the SnowballC package. (It has been extended to languages other than English as well.) library(SnowballC) tidy_fir_tree %&gt;% mutate(stem = wordStem(word)) %&gt;% count(stem, sort = TRUE) ## # A tibble: 570 x 2 ## stem n ## &lt;chr&gt; &lt;int&gt; ## 1 tree 88 ## 2 fir 34 ## 3 littl 23 ## 4 said 22 ## 5 stori 16 ## 6 thought 16 ## 7 branch 15 ## 8 on 15 ## 9 came 14 ## 10 know 14 ## # … with 560 more rows Take a look at those stems. Notice that we do now have 88 incidences of “tree”. Also notice that some words don’t look like they are spelled as real words; this is normal and expected with this stemming algorithm. The Porter algorithm identifies the stem of both “story” and “stories” as “stori”, not a regular English word but instead a special stem object. If you want to tokenize and stem your text data, you can try out the function tokenize_word_stems() from the tokenizers package, which implements Porter stemming just like what we demonstrated here. For more on tokenization, see Chapter 2. The Porter stemmer is an algorithm that starts with a word and ends up with a single stem, but that’s not the only kind of stemmer out there. Another class of stemmer are dictionary-based stemmers. One such stemmer is the stemming algorithm of the Hunspell library. The “Hun” in Hunspell stands for Hungarian; this set of NLP algorithms was originally written to handle Hungarian but has since been extended to handle many languages with compound words and complicated morphology. The Hunspell library is used mostly as a spell checker, but as part of identifying correct spellings, this library identifies word stems as well. You can use the Hunspell library from R via the hunspell package. library(hunspell) tidy_fir_tree %&gt;% mutate(stem = hunspell_stem(word)) %&gt;% unnest(stem) %&gt;% count(stem, sort = TRUE) ## # A tibble: 595 x 2 ## stem n ## &lt;chr&gt; &lt;int&gt; ## 1 tree 89 ## 2 fir 34 ## 3 little 23 ## 4 said 22 ## 5 story 16 ## 6 branch 15 ## 7 one 15 ## 8 came 14 ## 9 know 14 ## 10 now 14 ## # … with 585 more rows Notice that the code here is a little different (we had to use unnest()) and that the results are a little different. We have only real English words, and we have more total rows in the result. What happened? hunspell_stem(&quot;discontented&quot;) ## [[1]] ## [1] &quot;contented&quot; &quot;content&quot; We have two stems! This stemmer works differently; it uses both morphological analysis of a word and existing dictionaries to find possible stems. It’s possible to end up with more than one, and it’s possible for a stem to be a word that is not related by meaning to the original word. For example, one of the stems of “number” is “numb” with this library. The Hunspell library was built to be a spell checker, so depending on your analytical purposes, it may not be an appropriate choice. 4.2 Should you use stemming at all? You will often see stemming as part of NLP pipelines, sometimes without much comment about when it is helpful or not. We encourage you to think of stemming as a preprocessing step in text modeling, one that must be thought through and chosen (or not) with good judgment. Why does stemming often help, if you are training a machine learning model for text? Stemming reduces the sparsity of text data. Let’s see this in action, with a dataset of United States Supreme Court opinions available in the scotus package. How many words are there, after removing a standard dataset of stopwords? library(scotus) tidy_scotus &lt;- scotus_sample %&gt;% unnest_tokens(word, text) %&gt;% anti_join(get_stopwords()) tidy_scotus %&gt;% count(word, sort = TRUE) ## # A tibble: 89,617 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 court 83428 ## 2 v 59193 ## 3 state 45415 ## 4 states 39119 ## 5 case 35319 ## 6 act 32506 ## 7 s.ct 32003 ## 8 u.s 31376 ## 9 united 30803 ## 10 upon 30533 ## # … with 89,607 more rows There are 89,617 distinct words in this dataset we have created (after removing stopwords) but notice that even in the most common words we see a pair like &quot;state&quot; and &quot;states&quot;. A common data structure for modeling, and a helpful mental model for thinking about the sparsity of text data, is a matrix. Let’s cast() this tidy data to a sparse matrix (technically, a document-feature matrix object from the quanteda package). tidy_scotus %&gt;% count(case_name, word) %&gt;% cast_dfm(case_name, word, n) ## Document-feature matrix of: 6,060 documents, 89,617 features (99.6% sparse). Look at the sparsity of this matrix. It’s high! Think of this sparsity as the sparsity of data that we will want to use to build a supervised machine learning model. What if instead we use stemming as a preprocessing step here? tidy_scotus %&gt;% mutate(stem = wordStem(word)) %&gt;% count(case_name, stem) %&gt;% cast_dfm(case_name, stem, n) ## Document-feature matrix of: 6,060 documents, 69,619 features (99.5% sparse). We reduced the sparsity of our data by about 1%, and the number of word features by many thousands. Why is it possibly helpful to make our data more dense? Common sense says that reducing the number of word features in our dataset so dramatically will improve the performance of any machine learning model we train with it, assuming that we haven’t lost any important information by stemming. There is a growing body of academic research demonstrating that stemming can be counterproductive for text modeling. For example, Schofield and Mimno (2016) and related work explore how choices around stemming and other preprocessing steps don’t help and can actually hurt performance when training topic models for text. From Schofield and Mimno (2016) specifically, Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability. Topic modeling is an example of unsupervised machine learning for text and is not the same as the predictive modeling approaches we’ll be focusing on in this book, but the lesson remains that stemming may or may not be beneficial for any specific context. As we work through the rest of this chapter and learn more about stemming, consider what information we lose when we stem text in exchange for reducing the number of word features. Stemming can be helpful in some contexts, but typical stemming algorithms are somewhat aggressive and have been built to favor sensitivity (or recall, or the true positive rate) at the expense of specificity (or precision, or the true negative rate). Most common stemming algorithms you are likely to encounter will successfully reduce words to stems (i.e., not leave extraneous word endings on the words) but at the expense of collapsing some words with dramatic differences in meaning, semantics, use, etc. to the same stems. Examples of the latter are numerous, but some include: meaning and mean likely, like, liking university and universe 4.3 Understand a stemming algorithm If stemming is going to be in our NLP toolbox, it’s worth sitting down with one approach in detail to understand how it works under the hood. The Porter stemming algorithm is so approachable that we can walk through its outline in less than a page or so. It involves five steps, and the idea of a word measure. Think of any word as made up alternating groups of vowels \\(V\\) and consonants \\(C\\). One or more vowels together are one instance of \\(V\\), and one or more consonants togther are one instance of \\(C\\). We can write any word as \\[[C](VC)^m[V]\\] where \\(m\\) is called the “measure” of the word. The first \\(C\\) and the last \\(V\\) in brackets are optional. In this framework, we could write out the word &quot;tree&quot; as \\[CV\\] with \\(C\\) being “tr” and \\(V\\) being “ee”; it’s an m = 0 word. We would write out the word &quot;algorithms&quot; as \\[VCVCVC\\] and it is an m = 3 word. The first step of the Porter stemmer is (perhaps this seems like cheating) actually made of three substeps working with plural and past participle word endings. In the first substep (1a), “sses” is replaced with “ss”, “ies” is replaced with “i”, and final single “s” letters are removed. The second substep (1b) depends on the measure of the word m but works with endings like “eed”, “ed”, “ing”, adding “e” back on to make endings like “ate”, “ble”, and “ize” when appropriate. The third substep (1c) replaces “y” with “i” for words of a certain m. The second step of the Porter stemmer takes the output of the first step and regularizes a set of 20 endings. In this step, “ization” goes to “ize”, “alism” goes to “al”, “aliti” goes to “al” (notice that the ending “i” there came from the first step), and so on for the other 17 endings. The third step again processes the output, using a list of seven endings. Here, “ical” and “iciti” both go to “ic”, “ful” and “ness” are both removed, and so forth for the three other endings in this step. The fourth step involves a longer list of endings to deal with again (19), and they are all removed. Endings like “ent”, “ism”, “ment”, and more are removed in this step. The fifth and final step has two substeps, both which depend on the measure m of the word. In this step, depending on m, final “e” letters are sometimes removed and final double letters are sometimes removed. How would this work for a few example words? The word “supervised” loses its “ed” in step 1b and is not touched by the rest of the algorithm, ending at “supervis”. The word “relational” changes “ational” to “ate” in step 2 and loses its final “e” in step 5, ending at “relat”. Notice that neither of these results are regular English words, but instead special stem objects. This is expected. This algorithm was first published in Porter (1980) and is still broadly used; read Willett (2006) for background on how and why it has become a stemming standard. We can reach even further back and examine what is considered the first ever published stemming algorithm in Lovins (1968). The domain Lovins worked in was engineering, so her approach was particularly suited to technical terms. This algorithm uses much larger lists of word endings, conditions, and rules than the Porter algorithm and, although considered old-fashioned, is actually faster! Does stemming only work for English? Far from it! Check out the steps of a Snowball stemming algorithm for German. 4.4 Handling punctuation when stemming Punctuation contains information that can be used in text analysis. Punctuation is typically less information-dense than the words themselves and thus it is often removed early in a text mining analysis project, but it’s worth thinking through the impact of punctuation specifically on stemming. Think about words like &quot;they're&quot; and &quot;child's&quot;. We’ve already seen how punctuation and stemming can interact with our small example of “The Fir-Tree”; none of the stemming strategies we’ve discussed so far have recognized &quot;tree's&quot; as belonging to the same stem as &quot;trees&quot; and &quot;tree&quot;. tidy_fir_tree %&gt;% count(word, sort = TRUE) %&gt;% filter(str_detect(word, &quot;^tree&quot;)) ## # A tibble: 3 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 tree 76 ## 2 trees 12 ## 3 tree&#39;s 1 It is possible to split tokens not only on white space but also on punctuation, using a regular expression (see Appendix 12). fir_tree_counts &lt;- fir_tree %&gt;% unnest_tokens(word, text, token = &quot;regex&quot;, pattern = &quot;\\\\s+|[[:punct:]]+&quot;) %&gt;% anti_join(get_stopwords()) %&gt;% mutate(stem = wordStem(word)) %&gt;% count(stem, sort = TRUE) fir_tree_counts ## # A tibble: 572 x 2 ## stem n ## &lt;chr&gt; &lt;int&gt; ## 1 tree 89 ## 2 fir 34 ## 3 littl 23 ## 4 said 22 ## 5 stori 16 ## 6 thought 16 ## 7 branch 15 ## 8 on 15 ## 9 came 14 ## 10 know 14 ## # … with 562 more rows Now we are able to put all these related words together, having identified them with the same stem. fir_tree_counts %&gt;% filter(str_detect(stem, &quot;^tree&quot;)) ## # A tibble: 1 x 2 ## stem n ## &lt;chr&gt; &lt;int&gt; ## 1 tree 89 Handling punctuation in this way further reduces sparsity in word features. Whether this kind of tokenization and stemming strategy is a good choice in any particular data analysis situation depends on the particulars of the text characteristics. 4.5 Compare some stemming options Let’s compare a few simple stemming algorithms and see what results we end with. Let’s look at “The Fir-Tree”, specifically the tidied dataset from which we have removed stop words. Let’s compare three very straightforward stemming approaches. Only remove final instances of the letter “s”. This probably strikes you as not a great idea after our discussion in this chapter, but it is something that people try in real life, so let’s see what the impact is. Handle plural endings with slightly more complex rules. These rules are the same as step 1a of Porter stemming. Implement actual Porter stemming. We can now compare to the most commonly used stemming algorithm in English. stemming &lt;- tidy_fir_tree %&gt;% select(-book, -language) %&gt;% mutate( `Remove S` = str_remove(word, &quot;s$&quot;), `Plural endings` = case_when( str_detect(word, &quot;sses$&quot;) ~ str_replace(word, &quot;sses$&quot;, &quot;ss&quot;), str_detect(word, &quot;ies$&quot;) ~ str_replace(word, &quot;ies$&quot;, &quot;y&quot;), str_detect(word, &quot;ss$&quot;) ~ word, str_detect(word, &quot;s$&quot;) ~ str_remove(word, &quot;s$&quot;), TRUE ~ word ), `Porter stemming` = wordStem(word) ) %&gt;% rename(`Original word` = word) stemming ## # A tibble: 1,547 x 4 ## `Original word` `Remove S` `Plural endings` `Porter stemming` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 far far far far ## 2 forest forest forest forest ## 3 warm warm warm warm ## 4 sun sun sun sun ## 5 fresh fresh fresh fresh ## 6 air air air air ## 7 made made made made ## 8 sweet sweet sweet sweet ## 9 resting resting resting rest ## 10 place place place place ## # … with 1,537 more rows Figure 4.1 shows the results of these stemming strategies. All successfully handled the transition from &quot;trees&quot; to &quot;tree&quot; in the same way, but we have different results for &quot;stories&quot; to &quot;story&quot; or &quot;stori&quot; or &quot;storie&quot;, different handling of &quot;branches&quot;, and more. There are subtle differences in the output of even these straightforward stemming approaches that can effect the transformation of text features for modeling. stemming %&gt;% gather(Type, Result, `Remove S`:`Porter stemming`) %&gt;% mutate(Type = fct_inorder(Type)) %&gt;% count(Type, Result) %&gt;% group_by(Type) %&gt;% top_n(20, n) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Result, n), n, fill = Type )) + geom_col(show.legend = FALSE) + facet_wrap(~Type, scales = &quot;free_y&quot;) + coord_flip() + labs(x = NULL, y = &quot;Frequency&quot;) Figure 4.1: Results for three different stemming strategies Porter stemming is the most different from the other two approaches. In the top twenty words here, we don’t see much difference between removing only the letter “s” and taking a slightly more sophisticated approach to plural endings. In what situations do we see a difference? stemming %&gt;% filter(`Remove S` != `Plural endings`) %&gt;% distinct(`Remove S`, `Plural endings`, .keep_all = TRUE) ## # A tibble: 10 x 4 ## `Original word` `Remove S` `Plural endings` `Porter stemming` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 raspberries raspberrie raspberry raspberri ## 2 strawberries strawberrie strawberry strawberri ## 3 less les less less ## 4 brightness brightnes brightness bright ## 5 faintness faintnes faintness faint ## 6 happiness happines happiness happi ## 7 ladies ladie lady ladi ## 8 babies babie baby babi ## 9 princess princes princess princess ## 10 stories storie story stori We also see situations where the same sets of original words are bucketed differently (not just with different stem labels) under different stemming strategies. In the following very small example, two of the strategies bucket these words into two stems while one strategy buckets them into one stem. stemming %&gt;% gather(Type, Result, `Remove S`:`Porter stemming`) %&gt;% filter(Result %in% c(&quot;come&quot;, &quot;coming&quot;)) %&gt;% distinct(`Original word`, Type, Result) ## # A tibble: 9 x 3 ## `Original word` Type Result ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 come Remove S come ## 2 comes Remove S come ## 3 coming Remove S coming ## 4 come Plural endings come ## 5 comes Plural endings come ## 6 coming Plural endings coming ## 7 come Porter stemming come ## 8 comes Porter stemming come ## 9 coming Porter stemming come These different characteristics can either be positive or negative, depending on the nature of the text being modeled and the analytical question being pursued. Language use is connected to culture and identity. How might the results of stemming strategies be different for text created with the same language (like English) but in different social or cultural contexts, or by people with different identities? With what kind of text do you think stemming algorithms behave most consistently, or most as expected? What impact might that have on text modeling? 4.6 Lemmatization and stemming When people use the word “stemming” in natural language processing, they typically mean a system like the one we’ve been describing in this chapter, with rules, conditions, heuristics, and lists of word endings. Think of stemming as typically implemented in NLP as rule-based, operating on the word by itself. There is another option for normalizing words to a root that takes a different approach. Instead of using rules to cut words down to their stems, lemmatization uses knowledge about a language’s structure to reduce words down to their lemmas, the canonical or dictionary forms of words. Think of lemmatization as typically implemented in NLP as linguistics-based, operating on the word in its context. Lemmatization requires more information than the rule-based stemmers we’ve discussed so far. We need to know what part of speech a word is to correctly identify its lemma,1 and we also need more information about what words mean in their contexts. Often lemmatizers use a rich lexical database like WordNet as a way to look up word meanings for a given part-of-speech use (Miller 1995). Notice that lemmatization involves more linguistic knowledge of a language than stemming. How does lemmatization work in languages other than English? Lookup dictionaries connecting words, lemmas, and parts of speech for languages other than English have been developed as well. A modern, efficient implementation for lemmatization is available in the excellent spaCy library, which is written in Python. NLP practitioners who work with R can use this library via the spacyr package (Benoit and Matsuo 2019) or the cleanNLP package (Arnold 2017). You might also consider using spaCy directly in R Markdown via the Python engine. Implementing lemmatization is slower and more complex than stemming. Just like with stemming, lemmatization often improves the true positive rate (or recall) but at the expense of the true negative rate (or precision) compared to not using lemmatization, but typically less so than stemming. 4.7 Stemming and stop words Our deep dive into stemming came after our chapters on tokenization (Chapter 2) and stop words (Chapter 3) because this is typically when you will want to implement stemming, if appropriate to your analytical question. Stop word lists are usually unstemmed, so you usually need to remove stop words before stemming text data. For example, the Porter stemming algorithm transforms words like &quot;themselves&quot; to &quot;themselv&quot;, so stemming first would leave you without the ability to match up to the commonly used stop word lexicons. A handy trick is to use the following function on your stop word list to return the words that don’t have a stemmed version in the list. If the function returns a length 0 vector then you can stem and remove stop words in any order. library(stopwords) not_stemmed_in &lt;- function(x) { x[!SnowballC::wordStem(x) %in% x] } not_stemmed_in(stopwords(source = &quot;snowball&quot;)) ## [1] &quot;ourselves&quot; &quot;yourselves&quot; &quot;his&quot; &quot;they&quot; &quot;themselves&quot; ## [6] &quot;this&quot; &quot;are&quot; &quot;was&quot; &quot;has&quot; &quot;does&quot; ## [11] &quot;you&#39;re&quot; &quot;he&#39;s&quot; &quot;she&#39;s&quot; &quot;it&#39;s&quot; &quot;we&#39;re&quot; ## [16] &quot;they&#39;re&quot; &quot;i&#39;ve&quot; &quot;you&#39;ve&quot; &quot;we&#39;ve&quot; &quot;they&#39;ve&quot; ## [21] &quot;let&#39;s&quot; &quot;that&#39;s&quot; &quot;who&#39;s&quot; &quot;what&#39;s&quot; &quot;here&#39;s&quot; ## [26] &quot;there&#39;s&quot; &quot;when&#39;s&quot; &quot;where&#39;s&quot; &quot;why&#39;s&quot; &quot;how&#39;s&quot; ## [31] &quot;because&quot; &quot;during&quot; &quot;before&quot; &quot;above&quot; &quot;once&quot; ## [36] &quot;any&quot; &quot;only&quot; &quot;very&quot; Here we see that many of the words that are lost are the contractions. 4.8 Summary In this chapter, we explored stemming, the practice of identifying and extracting the base or stem for a word. Stemming reduces the sparsity of text data which can be helpful when training models, but at the cost of throwing information away. References "],
["embeddings.html", "5 Word Embeddings 5.1 Understand word embeddings by finding them yourself 5.2 Exploring CFPB word embeddings 5.3 Use pre-trained word embeddings 5.4 Fairness and word embeddings 5.5 Using word embeddings in the real world 5.6 Summary", " 5 Word Embeddings You shall know a word by the company it keeps. — John Rupert Firth So far in our discussion of natural language features, we have discussed preprocessing steps such as tokenization, removing stop words, and stemming in detail. We implement these types of preprocessing steps to be able to represent our text data in some data structure that is a good fit for modeling. An example of such a data structure is a sparse matrix. Perhaps, if we wanted to analyse or build a model for consumer complaints to the United States Consumer Financial Protection Bureau (CFPB), we would start with straightforward word counts. library(tidyverse) library(tidytext) library(SnowballC) complaints &lt;- read_csv(&quot;data/complaints.csv.gz&quot;) complaints %&gt;% unnest_tokens(word, consumer_complaint_narrative) %&gt;% anti_join(get_stopwords()) %&gt;% mutate(stem = wordStem(word)) %&gt;% count(complaint_id, stem) %&gt;% cast_dfm(complaint_id, stem, n) ## Document-feature matrix of: 117,214 documents, 46,099 features (99.9% sparse). The dataset of consumer complaints used in this book has been filtered to those submitted to the CFPB since 1 January 2019 that include a consumer complaint narrative (i.e., some submitted text). Another way to represent our text data is to use tf-idf instead of word counts. This weighting for text features can often work better in predictive modeling. complaints %&gt;% unnest_tokens(word, consumer_complaint_narrative) %&gt;% anti_join(get_stopwords()) %&gt;% mutate(stem = wordStem(word)) %&gt;% count(complaint_id, stem) %&gt;% bind_tf_idf(stem, complaint_id, n) %&gt;% cast_dfm(complaint_id, stem, tf_idf) ## Document-feature matrix of: 117,214 documents, 46,099 features (99.9% sparse). Notice that in either case, our final data structure is incredibly sparse and of high dimensionality with a huge number of features. Some modeling algorithms and the libraries which implement them can take advantage of the memory characteristics of sparse matrices for better performance; an example of this is regularized regression implemented in glmnet. Some modeling algorithms, including tree-based algorithms, do not perform better with sparse input, and then some libraries are not built to take advantage of sparse data structures, even if it would improve performance for those algorithms. 5.0.0.1 SPARSE VS. NON SPARSE MATRIX DIAGRAM GOES HERE Linguists have long worked on vector models for language that can reduce the number of dimensions representing text data based on how people use language; the quote that opened this chapter dates to 1957. These kinds of dense word vectors are often called word embeddings. 5.1 Understand word embeddings by finding them yourself Word embeddings are a way to represent text data as numbers based on a huge corpus of text, capturing semantic meaning from words’ context. Modern word embeddings are based on a statistical approach to modeling language, rather than a linguistics or rules-based approach. We can determine these vectors for a corpus of text using word counts and matrix factorization, as outlined by Moody (2017). This approach is valuable because it allows practitioners to find word vectors for their own collections of text (with no need to rely on pre-trained vectors) using familiar techniques that are not difficult to understand. Let’s walk through how to do this using tidy data principles and sparse matrices, on the dataset of CFPB complaints. First, let’s filter out words that are used only rarely in this dataset and create a nested dataframe, with one row per complaint. nested_words &lt;- complaints %&gt;% select(complaint_id, consumer_complaint_narrative) %&gt;% unnest_tokens(word, consumer_complaint_narrative) %&gt;% add_count(word) %&gt;% filter(n &gt;= 50) %&gt;% select(-n) %&gt;% nest(words = c(word)) nested_words ## # A tibble: 117,170 x 2 ## complaint_id words ## &lt;dbl&gt; &lt;list&lt;df[,1]&gt;&gt; ## 1 3384392 [18 × 1] ## 2 3417821 [71 × 1] ## 3 3433198 [77 × 1] ## 4 3366475 [69 × 1] ## 5 3385399 [213 × 1] ## 6 3444592 [19 × 1] ## 7 3379924 [121 × 1] ## 8 3446975 [22 × 1] ## 9 3214857 [64 × 1] ## 10 3417374 [44 × 1] ## # … with 117,160 more rows Next, let’s create a slide_windows() function, using the slide() function from the slide package (Vaughan 2020) which implements fast sliding window computations written in C. Our new function identifies skipgram windows in order to calculate the skipgram probabilities, how often we find each word near each other word. We do this by defining a fixed-size moving window that centers around each word. Do we see word1 and word2 together within this window? We can calculate probabilities based on when we do or do not. One of the arguments to this function is the window_size, which determines the size of the sliding window that moves through the text, counting up words that we find within the window. The best choice for this window size depends on your analytical question because it determines what kind of semantic meaning the embeddings capture. A smaller window size, like three or four, focuses on how the word is used and learns what other words are functionally similar. A larger window size, like ten, captures more information about the domain or topic of each word, not constrained by how functionally similar the words are (Levy and Goldberg 2014). A smaller window size is also faster to compute. slide_windows &lt;- function(tbl, window_size) { skipgrams &lt;- slide::slide( tbl, ~.x, .after = window_size - 1, .step = 1, .complete = TRUE ) safe_mutate &lt;- safely(mutate) out &lt;- map2( skipgrams, 1:length(skipgrams), ~ safe_mutate(.x, window_id = .y) ) out %&gt;% transpose() %&gt;% pluck(&quot;result&quot;) %&gt;% compact() %&gt;% bind_rows() } Now that we can find all the skipgram windows, we can calculate how often words occur on their own, and how often words occur together with other words. We do this using the point-wise mutual information (PMI), a measure of association that measures exactly what we described in the previous sentence; it’s the logarithm of the probability of finding two words together, normalized for the probability of finding each of the words alone. We use PMI to measure which words occur together more often than expected based on how often they occurred on their own. For this example, let’s use a window size of four. This next step is the computationally expensive part of finding word embeddings with this method, and can take a while to run. Fortunately, we can use the furrr package (Vaughan and Dancho 2018) to take advantage of parallel processing because identifying skipgram windows in one document is independent from all the other documents. library(widyr) library(furrr) plan(multiprocess) ## for parallel processing tidy_pmi &lt;- nested_words %&gt;% mutate(words = future_map(words, slide_windows, 4, .progress = TRUE )) %&gt;% unnest(words) %&gt;% unite(window_id, complaint_id, window_id) %&gt;% pairwise_pmi(word, window_id) tidy_pmi ## # A tibble: 4,818,402 x 3 ## item1 item2 pmi ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 systems transworld 7.09 ## 2 inc transworld 5.96 ## 3 is transworld -0.135 ## 4 trying transworld -0.107 ## 5 to transworld -0.00206 ## 6 collect transworld 1.07 ## 7 a transworld -0.516 ## 8 debt transworld 0.919 ## 9 that transworld -0.542 ## 10 not transworld -1.17 ## # … with 4,818,392 more rows When PMI is high, the two words are associated with each other, likely to occur together. When PMI is low, the two words are not associated with each other, unlikely to occur together. The step above used unite(), a function from tidyr that pastes multiple columns into one, to make a new column for window_id from the old window_id plus the complaint_id. This new column tells us which combination of window and complaint each word belongs to. We can next determine the word vectors from the PMI values using singular value decomposition. Let’s use the widely_svd() function in widyr, creating 100-dimensional word embeddings. This matrix factorization is much faster than the previous step of identifying the skipgram windows and calculating PMI. tidy_word_vectors &lt;- tidy_pmi %&gt;% widely_svd( item1, item2, pmi, nv = 100, maxit = 1000 ) tidy_word_vectors ## # A tibble: 747,500 x 3 ## item1 dimension value ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 systems 1 0.0165 ## 2 inc 1 0.0191 ## 3 is 1 0.0202 ## 4 trying 1 0.0423 ## 5 to 1 0.00904 ## 6 collect 1 0.0370 ## 7 a 1 0.0126 ## 8 debt 1 0.0430 ## 9 that 1 0.0136 ## 10 not 1 0.0213 ## # … with 747,490 more rows We have now successfully found word embeddings, with clear and understandable code. This is a real benefit of this approach; this approach is based on counting, dividing, and matrix decomposition and is thus easier to understand and implement than options based on deep learning. Training word vectors or embeddings, even with this straightforward method, still requires a large dataset (ideally, hundreds of thousands of documents or more) and a not insignificant investment of time and computational power. 5.2 Exploring CFPB word embeddings Now that we have determined word embeddings for the dataset of CFPB complaints, let’s explore them and talk about they are used in modeling. We have projected the sparse, high-dimensional set of word features into a more dense, 100-dimensional set of features. Each word can be represented as a numeric vector in this new feature space. Which words are close to each other in this new feature space of word embeddings? Let’s create a simple function that will find the nearest synonyms using our newly created word embeddings. nearest_synonyms &lt;- function(df, token) { df %&gt;% widely(~ . %*% (.[token, ]), sort = TRUE)(item1, dimension, value) %&gt;% select(-item2) } This function takes the tidy word embeddings as input, along with a word (or token) as a string. It uses matrix multiplication to find which words are closer or farther to the input word, and returns a dataframe sorted by similarity. What words are closest to &quot;error&quot; in the dataset of CFPB complaints, as determined by our word embeddings? tidy_word_vectors %&gt;% nearest_synonyms(&quot;error&quot;) ## # A tibble: 7,475 x 2 ## item1 value ## &lt;chr&gt; &lt;dbl&gt; ## 1 error 0.0373 ## 2 issue 0.0237 ## 3 problem 0.0235 ## 4 issues 0.0194 ## 5 errors 0.0187 ## 6 mistake 0.0185 ## 7 system 0.0170 ## 8 problems 0.0151 ## 9 late 0.0141 ## 10 situation 0.0138 ## # … with 7,465 more rows Errors, problems, issues, mistakes – sounds bad! What is closest to the word &quot;month&quot;? tidy_word_vectors %&gt;% nearest_synonyms(&quot;month&quot;) ## # A tibble: 7,475 x 2 ## item1 value ## &lt;chr&gt; &lt;dbl&gt; ## 1 month 0.0597 ## 2 payment 0.0407 ## 3 months 0.0355 ## 4 payments 0.0325 ## 5 year 0.0314 ## 6 days 0.0274 ## 7 balance 0.0267 ## 8 xx 0.0265 ## 9 years 0.0262 ## 10 monthly 0.0260 ## # … with 7,465 more rows We see words about payments, along with other time periods such as days and years. Notice that we did not stem this text data (see Chapter 4) but the word embeddings learned that singular and plural forms of words belong together. What words are closest in this embedding space to &quot;fee&quot;? tidy_word_vectors %&gt;% nearest_synonyms(&quot;fee&quot;) ## # A tibble: 7,475 x 2 ## item1 value ## &lt;chr&gt; &lt;dbl&gt; ## 1 fee 0.0762 ## 2 fees 0.0605 ## 3 charge 0.0421 ## 4 interest 0.0410 ## 5 charged 0.0387 ## 6 late 0.0377 ## 7 charges 0.0366 ## 8 overdraft 0.0327 ## 9 charging 0.0246 ## 10 month 0.0220 ## # … with 7,465 more rows We find words about interest, charges, and overdrafts. Since we have found word embeddings via singular value decomposition, we can use these vectors to understand what principal components explain the most variation in the CFPB complaints. tidy_word_vectors %&gt;% filter(dimension &lt;= 24) %&gt;% group_by(dimension) %&gt;% top_n(12, abs(value)) %&gt;% ungroup() %&gt;% mutate(item1 = reorder_within(item1, value, dimension)) %&gt;% ggplot(aes(item1, value, fill = as.factor(dimension))) + geom_col(show.legend = FALSE) + facet_wrap(~dimension, scales = &quot;free_y&quot;, ncol = 4) + scale_x_reordered() + coord_flip() + labs( x = NULL, y = &quot;Value&quot;, title = &quot;First 24 principal components for text of CFPB complaints&quot;, subtitle = &quot;Top words contributing to the components that explain the most variation&quot; ) Figure 5.1: Word embeddings for Consumer Finance Protection Bureau complaints It becomes very clear in Figure 5.1 that stop words have not been removed, but notice that we can learn meaningful relationships in how very common words are used. Component 12 shows us how common prepositions are often used with words like &quot;regarding&quot;, &quot;contacted&quot;, and &quot;called&quot;, while component 9 highlights the use of different common words when submitting a complaint about unethical, predatory, and/or deceptive practices. Stop words do carry information, and methods like determining word embeddings can make that information usable. We created word embeddings and can explore them to understand our text dataset, but how do we use this vector representation in modeling? 5.3 Use pre-trained word embeddings library(textdata) ## get GloVe embeddings https://github.com/mkearney/wactor Pre-trained word embeddings are trained based on very large, general purpose English language datasets. Commonly used word2vec embeddings are based on the Google News dataset, and commonly used GloVe embeddings and FastText embeddings are learned from the text of Wikipedia. 5.4 Fairness and word embeddings Perhaps more than any of the other preprocessing steps this book has covered so far, using word embeddings opens an analysis or model up to the possibility of being influenced by systemic unfairness and bias. Embeddings are trained or learned from a large corpus of text data, and whatever human prejudice or bias exists in the corpus becomes imprinted into the vector data of the embeddings. This is true of all machine learning to some extent (models learn, reproduce, and often amplify whatever biases exist in training data) but this is literally, concretely true of word embeddings. Islam, Bryson, and Narayanan (2016) show how the GloVe word embeddings (the same embeddings we used in Section 5.3) replicate human-like semantic biases. African American first names are associated with more unpleasant feelings than European American first names. Women’s first names are more associated with family and men’s first names are more associated with career. Terms associated with women are more associated with the arts and terms associated with men are more associated with science. Results like these have been confirmed over and over again, such as when Bolukbasi et al. (2016) demonstrated gender stereotypes in how word embeddings encode professions or when Google Translate exhibited apparently sexist behavior when translating text from languages with no gendered pronouns.2 Garg et al. (2018) even used how bias and stereotypes can be found in word embeddings to quantify how social attitudes towards women and minorities have changed over time. EMPHASIZE AGAIN THE TRAINING DATASETS – REFERENCE FOR GENDER BALANCE ON WIKIPEDIA It’s safe to assume that any large corpus of language will contain latent structure reflecting the biases of the people who generated that language. When embeddings with these kinds of stereotypes are used as a preprocessing step in training a predictive model, the final model can exhibit racist, sexist, or otherwise biased characteristics. Speer (2017) demonstrated how using pre-trained word embeddings to train a straightforward sentiment analysis model can result in text such as “Let’s go get Italian food” being scored much more positively than text such as “Let’s go get Mexican food” because of characteristics of the text the word embeddings were trained on. 5.5 Using word embeddings in the real world Given these profound and fundamental challenges with word embeddings, what options are out there? First, consider not using word embeddings when building a text model. Depending on the particular analytical question you are trying to answer, another numerical representation of text data (such as word frequencies or tf-idf of single words or n-grams) may be more appropriate. Consider this option even more seriously if the model you want to train is already entangled with issues of bias, such as the sentiment analysis example in Section 5.4. Consider whether finding your own word embeddings, instead of relying on pre-trained embeddings such as GloVe or word2vec, may help you. Building your own vectors is likely to be a good option when the text domain you are working in is specific rather than general purpose; some examples of such domains could include customer feedback for a clothing e-commerce site, comments posted on a coding Q&amp;A site, or legal documents. Learning good quality word embeddings is only realistic when you have a large corpus of text data (say, hundreds of thousands or more documents) but if you have that much data, it is possible that embeddings learned from scratch based on your own data may not exhibit the same kind of semantic biases that exist in pre-trained word embeddings. Almost certainly there will be some kind of bias latent in any large text corpus, but when you use your own training data for learning word embeddings, you avoid the problem of adding historic, systemic prejudice from general purpose language datasets. You can use the same approaches discussed here to check any new embeddings for dangerous biases such as racism or sexism. You can use the same approaches discussed in this chapter to check any new embeddings for dangerous biases such as racism or sexism. NLP researchers have also proposed methods for debiasing embeddings. Bolukbasi et al. (2016) aim to remove stereotypes by postprocessing pre-trained word vectors, choosing specific sets of words that are reprojected in the vector space so that some specific bias, such as gender bias, is mitigated. This is the most established method for reducing bias in embeddings to date, although other methods have been proposed as well, such as augmenting data with counterfactuals (Lu et al. 2018). Recent work (Ethayarajh, Duvenaud, and Hirst 2019) has explored whether the association tests used to measure bias are even useful, and under what conditions debiasing can be effective. Other researchers, such as Islam, Bryson, and Narayanan (2016), suggest that corrections for fairness should happen at the point of decision or action rather than earlier in the process of modeling, such as preprocessing steps like building word embeddings. The concern is that methods for debiasing word embeddings may allow the stereotypes to seep back in, and more recent work shows that this is exactly what can happen. Gonen and Goldberg (2019) highlight how pervasive and consistent gender bias is across different word embedding models, even after applying current debiasing methods. 5.6 Summary TKTKTK It’s important to keep in mind that even more advanced natural language algorithms, such as language models with transformers, also exhibit such systemic biases (Sheng et al. 2019). References "],
["forewords.html", "Forewords", " Forewords This section will act as the first section where we will use what we have learned about text and put it to use in a modeling context. This chapter will mostly be focused on bag-of-words models using methods like; Naive Bayes, Support Vector Machines (SVM) and regularized regressions models like glmnet. Everything before you start to fit a model it is essential to consider how algorithmic bias is represented. Rachel Thomas proposed a checklist in ODSC West 2019 that one can use to get an idea of how the bias can be included. Should we even be doing this? What bias is already in the data? Can the code and data be audited? What are the error rates for sub-groups? What is the accuracy of a simple rule-based alternative? What processes are in place to handle appeals or mistakes? How diverse is the team that built it? And lets quickly talk about each point. TODO For the following chapters, we will use tidymodels framework to do preprocessing, modeling and evaluation. "],
["classification.html", "6 Classification 6.1 First attempt 6.2 Different types of models 6.3 Two class or multiclass 6.4 Case study: relationship between performace 6.5 Case Study: feature hashing 6.6 What evaluation metrics are appropiate 6.7 Full game", " 6 Classification What is classification. 6.1 First attempt first attempt and full game will use same data. First attempt might use a subset of the data to make the example easier to understand. and properly to give balanced dataset, which then later can be explored. 6.1.1 Look at the data 6.1.2 Modeling 6.1.3 Evaluation 6.2 Different types of models (Not all of these models are good, but are used to show strenghs and weaknessed) - SVM - Naive Bayes - glmnet - Random forrest - knn - NULL model 6.3 Two class or multiclass 6.4 Case study: relationship between performace 6.5 Case Study: feature hashing 6.6 What evaluation metrics are appropiate Data will most likely be sparse when using BoW 6.7 Full game 6.7.1 Feature selection 6.7.2 Splitting the data 6.7.3 Specifying models 6.7.4 Cross-validation 6.7.5 Evaluation Inteprebility. “Can we get comparable performance with a simpler model?” Compare with simple rule based model "],
["regression.html", "7 Regression 7.1 First attempt 7.2 Different types of models 7.3 Case study: varying n-grams &amp; stop words 7.4 Case study: Adding custom feature 7.5 What evaluation metrics are appropiate 7.6 Full game", " 7 Regression What is regression. How is it different then classification. Defining what to regress on. Does it generalize 7.1 First attempt first attempt and full game will use same data. First attempt might use a subset of the data to make the example easier to understand. and properly to give balanced dataset, which then later can be explored. 7.1.1 Look at the data 7.1.2 Modeling 7.1.3 Evaluation Closely examine high performaing samples and low performance sampling. 7.2 Different types of models (Not all of these models are good, but are used to show strenghs and weaknessed) - SVM - Naive Bayes - glmnet - Random forrest - knn - NULL model 7.3 Case study: varying n-grams &amp; stop words Use different n of grams, combined with different ways of removing stopwords. 7.4 Case study: Adding custom feature a custom feature is something that we can easily count and add to our model that isn’t part of simple BoW. https://github.com/mkearney/textfeatures examples include: number of urls 7.5 What evaluation metrics are appropiate Data will most likely be sparse when using BoW 7.6 Full game 7.6.1 Feature selection 7.6.2 Splitting the data 7.6.3 Specifying models 7.6.4 Cross-validation 7.6.5 Evaluation "],
["clustering.html", "8 Clustering", " 8 Clustering "],
["forewords-1.html", "Forewords", " Forewords In this last section of the book we will follow the same style as the last section as we are still trying to create accurate predictions, but we are switching our attention to deep neural networks. This section will act as the first section where we will use what we have learned about text and put it to use in a modeling context. This section will focus on bag-of-words and word embeddings using CNN, RNN and LSTMs. Everything before you start to fit a model it is essential to consider how algorithmic bias is represented. Rachel Thomas proposed a checklist in ODSC West 2019 that one can use to get an idea of how the bias can be included. Should we even be doing this? What bias is already in the data? Can the code and data be audited? What are the error rates for sub-groups? What is the accuracy of a simple rule-based alternative? What processes are in place to handle appeals or mistakes? How diverse is the team that built it? And lets quickly talk about each point. TODO For the following chapters, we will use tensorflow with keras framework to do preprocessing, modeling and evaluation. TODO add pro and cons table for differences between deep learning and non-deep learning "],
["classification-1.html", "9 Classification 9.1 Case Study: Applying the wrong model", " 9 Classification 9.1 Case Study: Applying the wrong model Here will we demonstrate what happens when the wrong model is used. Model from ML-classification will be used on this dataset. "],
["regression-1.html", "10 Regression", " 10 Regression "],
["clustering-1.html", "11 Clustering", " 11 Clustering "],
["regexp.html", "12 Regular expressions 12.1 Literal characters 12.2 Full stop, the wildcard 12.3 Character classes 12.4 Quantifiers 12.5 Anchors 12.6 Additional resources", " 12 Regular expressions Some people, when confronted with a problem, think: “I know, I’ll use regular expressions.” Now they have two problems. — Jamie Zawinski This section will give a brief overview on how to write and use a regular expression, often abbreviated regex. Regular expressions are a way to specify or search for patterns of strings using a sequence of characters. By combining a selection of simple patterns, we can capture quite complicated strings. Many functions in R take advantage of regular expressions. Some examples from base R include grep, grepl, regexpr, gregexpr, sub, gsub, and strsplit, as well as ls and list.files. The stringr package (Wickham 2019) uses regular expressions extensively; the regular expressions are passed as the pattern = argument. Regular expressions can be used to detect, locate, or extract parts of a string. 12.1 Literal characters The most basic regular expression consists of only a single character. Here let’s detect if each of the following strings in the character vector animals contains the letter “j”. library(stringr) animals &lt;- c(&quot;jaguar&quot;, &quot;jay&quot;, &quot;bat&quot;) str_detect(animals, &quot;j&quot;) ## [1] TRUE TRUE FALSE We are also able to extract the match with str_extract. This may not seem too useful right now, but it becomes very helpful once we use more advanced regular expressions. str_extract(animals, &quot;j&quot;) ## [1] &quot;j&quot; &quot;j&quot; NA Lastly we are able to locate the position of a match using str_locate. str_locate(animals, &quot;j&quot;) ## start end ## [1,] 1 1 ## [2,] 1 1 ## [3,] NA NA The functions str_detect, str_extract, and str_locate are some of the most simple and powerful main functions in stringr, but the stringr package includes many more functions. To see the remaining functions, run help(package = “stringr”) to open the documentation. We can also match multiple characters in a row. animals &lt;- c(&quot;jaguar&quot;, &quot;jay&quot;, &quot;bat&quot;) str_detect(animals, &quot;jag&quot;) ## [1] TRUE FALSE FALSE Notice how these characters are case sensitive. wows &lt;- c(&quot;wow&quot;, &quot;WoW&quot;, &quot;WOW&quot;) str_detect(wows, &quot;wow&quot;) ## [1] TRUE FALSE FALSE 12.1.1 Meta characters There are 14 meta characters that carry special meaning inside regular expressions. We need to “escape” them with a backslash if we want to match the literal character (and backslashes need to be doubled in R). Think of “escaping” as stripping the character of its special meaning. The plus symbol + is one of the special meta characters for regular expressions. math &lt;- c(&quot;1 + 2&quot;, &quot;14 + 5&quot;, &quot;3 - 5&quot;) str_detect(math, &quot;\\\\+&quot;) ## [1] TRUE TRUE FALSE If we tried to use the plus sign without escaping it, like &quot;+&quot;, we would get an error and this line of code would not run. The complete list of meta characters is displayed in Table 12.1 (n.d.). TODO: where is the reference in the line above? Table 12.1: All meta characters Description Character opening square bracket [ closing square bracket ] backslash \\ caret ^ dollar sign $ period/dot . vertical bar | question mark ? asterisk * plus sign + opening curly brackets { closing curly brackets } opening parentheses ( closing parentheses ) 12.2 Full stop, the wildcard Let’s start with the full stop/period/dot, which acts as a “wildcard.” This means that this character will match anything in place other then a newline character. strings &lt;- c(&quot;cat&quot;, &quot;cut&quot;, &quot;cue&quot;) str_extract(strings, &quot;c.&quot;) ## [1] &quot;ca&quot; &quot;cu&quot; &quot;cu&quot; str_extract(strings, &quot;c.t&quot;) ## [1] &quot;cat&quot; &quot;cut&quot; NA 12.3 Character classes So far we have only been able to match either exact characters or wildcards. Character classes (also called character sets) let us do more than that. A character class allows us to match a character specified inside the class. A character class is constructed with square brackets. The character class [ac] will match either an “a” or a “c”. strings &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) str_detect(strings, &quot;[ac]&quot;) ## [1] TRUE FALSE TRUE Spaces inside character classes are meaningful as they are interpreted as literal characters. Thus the character class “[ac]” will match the letter “a” and “c”, while the character class “[a c]” will match the letters “a” and “c” but also a space. We can use a hyphen character to define a range of characters. Thus [1-5] is the same as [12345]. numbers &lt;- c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;) str_detect(numbers, &quot;[2-7]&quot;) ## [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE sentence &lt;- &quot;This is a long sentence with 2 numbers with 1 digits.&quot; str_locate_all(sentence, &quot;[1-2a-b]&quot;) ## [[1]] ## start end ## [1,] 9 9 ## [2,] 30 30 ## [3,] 35 35 ## [4,] 45 45 We can also negate characters in a class with a caret ^. Placing a caret immediately inside the opening square bracket will make the regular expression match anything not inside the class. Thus the regular expression [^ac] will match anything that isn’t the letter “a” or “c”. strings &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) str_detect(strings, &quot;[^ac]&quot;) ## [1] FALSE TRUE FALSE 12.3.1 Shorthand character classes Certain character classes are so commonly used that they have been predefined with names. A couple of these character classes have even shorter shorthands. The class [:digit:] denotes all the digits 0, 1, 2, 3, 4, 5, 6, 7, 8 and 9 but it can also be described by \\\\d. Table 12.2 presents these useful predefined character classes. Table 12.2: All character classes Class Description [:digit:] or \\\\d Digits; [0-9] [:alpha:] Alphabetic characters, uppercase and lowercase [A-z] [:alnum:] Alphanumeric characters, letters, and digits [A-z0-9] [:graph:] Graphical characters [[:alnum:][:punct:]] [:print:] Printable characters [[:alnum:][:punct:][:space:]] [:lower:] Lowercase letters [a-z] [:upper:] Uppercase letters [A-Z] [:cntrl:] Control characters such as newline, carriage return, etc. [:punct:] Punctuation characters: !“#$%&amp;’()*+,-./:;&lt;=&gt;?@[]^_`{|}~ [:blank:] Space and tab [:space:] or \\\\s Space, tab, vertical tab, newline, form feed, carriage return [:xdigit:] Hexadecimal digits [0-9A-Fa-f] \\\\S Not space [^[:space:]] \\\\w Word characters: letters, digits, and underscores [A-z0-9_] \\\\W Non-word characters [^A-z0-9_] \\\\D Non-digits [^0-9] Notice that these short-hands are locale specific. This means that the danish character ø will be picked up in class [:lower:] but not in the class [a-z] as the character isn’t located between a and z. TODO find a easy way to showcase regex in different locales. (Note from Julia: I think probably just print these non-reproducibly.) 12.4 Quantifiers We can specify how many times we expect something to occur using quantifiers. If we want to find a digit with four numerals, we don’t have to write [:digit:][:digit:][:digit:][:digit:]. Table 12.3 shows how to specify repetitions. Notice that ? is shorthand for {0,1}, * is shorthand for {0,} and + is shorthand for {1,} (“Quantifiers , *, ? And n” 2019). Table 12.3: Regular expression quantifiers Regex Matches ? zero or one times * zero or more times + one or more times {n} exactly n times {n,} at least n times {n,m} between n and m times We can detect both color and colour by placing a quantifier after the “u” that detects 0 or 1 times used. col &lt;- c(&quot;colour&quot;, &quot;color&quot;, &quot;farver&quot;) str_detect(col, &quot;colou?r&quot;) ## [1] TRUE TRUE FALSE And we can extract four-digit numbers using {4}. sentences &lt;- c(&quot;The year was 1776.&quot;, &quot;Alexander Hamilton died at 47.&quot;) str_extract(sentences, &quot;\\\\d{4}&quot;) ## [1] &quot;1776&quot; NA Sometimes we want the repetition to happen over multiple characters. This can be achieved by wrapping what we want repeated in parentheses. In the following example, we want to match all the instances of “NA” in the string. We put &quot;NA &quot; inside a set of parentheses and putting + after it to make sure we match at least once. batman &lt;- &quot;NA NA NA NA NA NA NA NA NA NA NA NA NA NA BATMAN!!!&quot; str_extract(batman, &quot;(NA )+&quot;) ## [1] &quot;NA NA NA NA NA NA NA NA NA NA NA NA NA NA &quot; However, notice that this also matches the last space, which we don’t want. We can fix this by matching zero or more “NA” followed by exactly 1 “NA”. batman &lt;- &quot;NA NA NA NA NA NA NA NA NA NA NA NA NA NA BATMAN!!!&quot; str_extract(batman, &quot;(NA )*(NA){1}&quot;) ## [1] &quot;NA NA NA NA NA NA NA NA NA NA NA NA NA NA&quot; By default these matches are “greedy”, meaning that they will try to match the longest string possible. We can instead make them “lazy” by placing a ? after, as shown in Table 12.4. This will make the regular expressions try to match the shortest string possible instead of the longest. Table 12.4: Lazy quantifiers regex matches ?? zero or one times, prefers 0 *? zero or more times, match as few times as possible +? one or more times, match as few times as possible {n}? exactly n times, match as few times as possible {n,}? at least n times, match as few times as possible {n,m}? between n and m times, match as few times as possible but at least n Comparing greedy and lazy matches gives us 3 and 7 “NA”’s respectively. batman &lt;- &quot;NA NA NA NA NA NA NA NA NA NA NA NA NA NA BATMAN!!!&quot; str_extract(batman, &quot;(NA ){3,7}&quot;) ## [1] &quot;NA NA NA NA NA NA NA &quot; str_extract(batman, &quot;(NA ){3,7}?&quot;) ## [1] &quot;NA NA NA &quot; 12.5 Anchors The meta characters ^ and $ have special meaning in regular expressions. They force the engine to check the beginning and end of the string respectively, hence the name anchor. A mnemonic device to remember this is “First you get the power(^) and the you get the money(\\$)”. seasons &lt;- c(&quot;The summer is hot this year&quot;, &quot;The spring is a lovely time&quot;, &quot;Winter is my favorite time of the year&quot;, &quot;Fall is a time of peace&quot;) str_detect(seasons, &quot;^The&quot;) ## [1] TRUE TRUE FALSE FALSE str_detect(seasons, &quot;year$&quot;) ## [1] TRUE FALSE TRUE FALSE We can also combine the two to match a string completely. folder_names &lt;- c(&quot;analysis&quot;, &quot;data-raw&quot;, &quot;data&quot;, &quot;R&quot;) str_detect(folder_names, &quot;^data$&quot;) ## [1] FALSE FALSE TRUE FALSE 12.6 Additional resources This appendix covered some of the basics of getting started with (or refreshed about) regular expressions. If you want to learn more: RStudio maintains an excellent collection of cheat sheets, some of which are related to regular expressions. www.rexegg.com has many pages of valuable information, including this “quick start” page with helpful tables. https://www.regular-expressions.info/ is another great general regular expression site. The strings chapter in R for Data Science delves into examples written in R. Lastly if you want to go down to the metal, check out Mastering Regular Expressions. References "],
["software.html", "Software", " Software We generated all plots in this book using ggplot2 and its light theme (theme_light()). tokenizers R package. https://github.com/ropensci/tokenizers Provides fast, Consistent Tokenization of Natural Language Text. stringi R package. https://github.com/gagolews/stringi THE string processing package for R with ICU. "],
["appendixdata.html", "Data hcandersenr scotus GitHub issue", " Data TODO fill in hcandersenr The hcandersenr(Hvitfeldt 2019a) package includes the text of the 157 known fairy tales by the Danish author H.C. Andersen. The text comes with 5 different languages with 156 in English, 154 in Spanish, 150 in German, 138 in Danish and 58 in French The package comes with a dataset for each language with the naming convention hcandersen_**, where ** is a country code. Each dataset comes as a data.frame with two columns; text and book where the book variable has the text divided into strings of up to 80 characters. The package also comes with a dataset called EK which includes information about the publication date, language of origin and names in the different languages. scotus The scotus(Hvitfeldt 2019b) package contains a sample of the Supreme Court of the United States’ opinions. The scotus_sample data.frame includes 1 opinion per row along with the year, case name, docket number, and a unique ID number. The text has had minimal preprocessing done on them and will include the header information in the text field. Example of the beginning of an opinion is shown below ## No. 97-1992 ## VAUGHN L. MURPHY, Petitioner v. UNITED PARCEL SERVICE, INC. ## ON WRIT OF CERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE TENTH ## CIRCUIT ## [June 22, 1999] ## Justice O&#39;Connor delivered the opinion of the Court. ## Respondent United Parcel Service, Inc. (UPS), dismissed petitioner Vaughn ## L. Murphy from his job as a UPS mechanic because of his high blood pressure. ## Petitioner filed suit under Title I of the Americans with Disabilities Act of ## 1990 (ADA or Act), 104 Stat. 328, 42 U.S.C. § 12101 et seq., in Federal District ## Court. The District Court granted summary judgment to respondent, and the Court ## of Appeals for the Tenth Circuit affirmed. We must decide whether the Court ## of Appeals correctly considered petitioner in his medicated state when it held ## that petitioner&#39;s impairment does not &quot;substantially limi[t]&quot; one or more of ## his major life activities and whether it correctly determined that petitioner ## is not &quot;regarded as disabled.&quot; See §12102(2). In light of our decision in Sutton ## v. United Air Lines, Inc., ante, p. ____, we conclude that the Court of Appeals&#39; ## resolution of both issues was correct. GitHub issue This dataset includes 1161 Github issue title and an indicator of whether the issue was about documentation or not. The dataset is split into a training data set and evaluation data set. TODO find out how to store this data library(jsonlite) library(readr) library(tidyverse) json_to_df &lt;- function(x) { json &lt;- parse_json(x) tibble(text = json$text, label = json$label, answer = json$answer) } github_issues_training &lt;- read_lines(&quot;https://raw.githubusercontent.com/explosion/projects/master/textcat-docs-issues/docs_issues_training.jsonl&quot;) %&gt;% map_dfr(json_to_df) github_issues_eval &lt;- read_lines(&quot;https://raw.githubusercontent.com/explosion/projects/master/textcat-docs-issues/docs_issues_eval.jsonl&quot;) %&gt;% map_dfr(json_to_df) References "],
["references.html", "References", " References "]
]
