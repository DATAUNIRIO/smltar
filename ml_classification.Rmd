# Classification

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE, 
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
``` 

What is classification?

## First attempt

The first attempt and full game will use the same data. 
The first attempt might use a subset of the data to make the example easier to understand.
and properly give a balanced dataset, which then later can be explored.

### Look at the data

We are going to be working with US consumer complaints on financial products and company responses.
It contains a text field containing the complaint along with information regarding what it was for,
how it was filed and the response. 
In this chapter, we will try to predict what type of product the complaints are referring to. 
This first attempt will be limited to predicting if the product is a mortgage or not.

We can read in the complaint data \@ref(us-consumer-finance-complaints) with `read_csv()`.

```{r complaints, message=FALSE}
library(textrecipes)
library(tidymodels)
library(tidytext)
library(stringr)
library(discrim)
library(readr)

complaints <- read_csv("data/complaints.csv.gz")
```

then we will start by taking a quick look at the data to see what we have to work with

```{r, dependson="complaints"}
glimpse(complaints)
```

The first thing to note is our target variable `product` which we need to trim only display "Mortgage" and "Other",
and the `consumer_complaint_narrative` variable which contains the complaints.
Here is the first 6 complaints:

```{r, dependson="complaints"}
head(complaints$consumer_complaint_narrative)
```

Throughout the narratives is a series of capital x's. This has been done to hide Personally identifiable information (PII). This is not a universal censoring mechanism and can vary from source to source, hopefully you will be able to get this information in the data dictionary but you should always look at the data yourself to verify. We also see that all monetary amounts are surrounded by curly brackets, this is another step of preprocessing that has been done for us.

We can craft a regular expression to extract all the dollar amounts.

```{r, dependson="complaints"}
complaints$consumer_complaint_narrative %>%
  str_extract_all("\\{\\$[0-9\\.]*\\}") %>%
  compact() %>%
  head()
```

## Building our first classification model

Since this data is given to us after the fact, 
we need to make sure that only the information that would be available at the time of prediction is included in the model,
otherwise we are going to be very disappointed once the model is pushed to production.
The variables we can use as predictors are

- `date_received`
- `issue`
- `sub_issue`
- `consumer_complaint_narrative`
- `company`
- `state`
- `zip_code`
- `tags`
- `submitted_via`

Many of these have quite a lot of levels.
First we will include `date_received` for further consideration, along with `consumer_complaint_narrative` and `tags`.
`submitted_via` would have been a viable candidate too, but all the entries are "web".
The other variables could be of use too, but they are categorical variables with many values so we will exclude them for now.

We start by splitting the data into a training and testing dataset.
But before we do that we will create a factor variable of `product` with the levels "Mortgage" and "Other".
Then we will use the `initial_split()` from **rsample** to create a binary split of the data. 
The `strata` argument is used to make sure that the split is created to make sure the distribution of `product` is similar in the training set and testing set. 
Since the split is done using random sampling we set a seed so we can reproduce the results.

```{r, complaintssplit}
set.seed(1234)
complaints2class <- complaints %>%
  mutate(product = factor(if_else(product == "Mortgage", "Mortgage", "Other")))

complaints_split <- initial_split(complaints2class, strata = product)

complaints_train <- training(complaints_split)
complaints_test <- testing(complaints_split)
```

Looking at the dimensions of the two split shows that it worked successfully.

```{r, dependson="complaintssplit"}
dim(complaints_train)
dim(complaints_test)
```

Next we need to do some preprocessing. We need to do this since the models we are trying to use only support all numeric data. 

```{block, type = "rmdnote"}
Some models are able to handle factor variables and missing data. But it is in our best interest to manually deal with these problems so we know how they are handled.
```

The **recipes** package allows us to create a specification of the preprocessing steps we want to perform. Furthermore it contains the transformations we have trained on the training set and apply them in the same way for the testing set.
First off we use the `recipe()` function to initialize a recipe, we use a formula expression to specify the variables we are using along with the dataset.

```{r complaintrec1, dependson="complaintssplit"}
complaint_rec <- 
  recipe(product ~ date_received + tags + consumer_complaint_narrative, 
         data = complaints_train)
```

First will we take a look at the `date_received` variable. We use the `step_date()` to extract the month and day of the week (dow). Then we remove the original variable and dummify the variables created with `step_dummy()`.

```{r}
complaint_rec <- complaint_rec %>%
  step_date(date_received, features = c("month", "dow"), role = "dates") %>%
  step_rm(date_received) %>%
  step_dummy(has_role("dates"))
```

the `tags` variable includes some missing data. We deal with this by using `step_unknown()` to that adds a new level to the factor variable for cases of missing data. Then we dummify the variable with `step_dummy()`

```{r complaintrec2, dependson="complaintrec1"}
complaint_rec <- complaint_rec %>%
  step_unknown(tags) %>%
  step_dummy(tags)
```

Lastly we use **textrecipes** to handle the `consumer_complaint_narrative` variable. First we perform tokenization to words with `step_tokenize()`, by default this is done using `tokenizers::tokenize_words()`.
Next we remove stopwords with `step_stopwords()`, the default choice is the snowball stopword list, but custom lists can be provided too. Before we calculate the tf-idf we use `step_tokenfilter()` to only keep the 50 most frequent tokens, this is to avoid creating too many variables. To end off we use `step_tfidf()` to perform tf-idf calculations.

```{r complaintrec3, dependson="complaintrec2"}
complaint_rec <- complaint_rec %>%
  step_tokenize(consumer_complaint_narrative) %>%
  step_stopwords(consumer_complaint_narrative) %>%
  step_tokenfilter(consumer_complaint_narrative, max_tokens = 50) %>%
  step_tfidf(consumer_complaint_narrative)
```

Now that we have a full specification of the recipe we run `prep()` on it to train each of the steps on the training data.

```{r complaintprep, dependson="complaintrec3"}
complaint_prep <- prep(complaint_rec)
```

We can now extract the transformed training data with `juice()`. To apply the prepped recipe to the testing set we use the `bake()` function.

```{r complaintdata, dependson=c("complaintprep", "complaintssplit")}
train_data <- juice(complaint_prep)
test_data <- bake(complaint_prep, complaints_test)
```

For the modeling we will use a simple Naive Bayes model (TODO add citation to both Naive Bayes and its use in text classification).
One of the main advantages of Naive Bayes is its ability to handle a large number of features that we tend to get when using word count methods.
Here we have only kept the 50 most frequent tokens, we could have kept more tokens and a Naive Bayes model would be able to handle it okay, but we will limit it for this first time.

```{r nbspec}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("klaR")
nb_spec
```

Now we have everything we need to fit our first classification model, we just have to run `fit()` on our model specification and our training data.

```{r nbfit, dependson="nbspec"}
nb_fit <- nb_spec %>%
  fit(product ~ ., data = train_data)
```

We have more successfully fitted out first classification model.

### Evaluation

```{r dependson="nbfit", eval=FALSE}
nb_fit %>%
  predict(test_data) %>%
  mutate(truth = test_data$product) %>%
  accuracy(truth, .pred_class)
```


## Different types of models

(Not all of these models are good, but are used to show strengths and weaknesses)

- SVM
- Naive Bayes
- glmnet
- Random forrest
- knn
- NULL model

## Two class or multiclass

## Case study: What happens if you don't censor your data

The complaints data already have sensitive information censored out with XXXX and XX.
This can be seen as a kind of annotation, we don't get to know the specific account numbers and birthday which would be mostly unique anyways and filtered out.

Below we have is the most frequent trigrams [#tokenizing-by-n-grams] from our training dataset.

```{r censoredtrigram, dependson="complaintssplit", fig.cap="Many of the most frequent trigrams feature censored words."}
complaints_train %>%
  slice(1:1000) %>%
  unnest_tokens(trigrams, consumer_complaint_narrative, token = "ngrams", 
                collapse = FALSE) %>%
  count(trigrams, sort = TRUE) %>%
  mutate(censored = str_detect(trigrams, 'xx')) %>%
  slice(1:20) %>%
  ggplot(aes(n, reorder(trigrams, n), fill = censored)) +
  geom_col() +
  scale_fill_manual(values = c("grey40", "firebrick")) +
  labs(y = "Trigrams", x = "Count")
```

As you see the vast majority includes one or more censored words.
Not only does the most used trigrams include some kind of censoring, 
but the censored words include some signal as they are not used uniformly between the products.
In the following chart, we take the top 25 most frequent trigrams that includes one of more censoring,
and plot the proportions of the usage in "Mortgage" and "Other".

```{r trigram25}
top_censored_trigrams <- complaints_train %>%
    slice(1:1000) %>%
  unnest_tokens(trigrams, consumer_complaint_narrative, token = "ngrams", 
                collapse = FALSE) %>%
  count(trigrams, sort = TRUE) %>%
  filter(str_detect(trigrams, 'xx')) %>%
  slice(1:25)

plot_data <- complaints_train %>%
  unnest_tokens(trigrams, consumer_complaint_narrative, token = "ngrams", 
                collapse = FALSE) %>%
  right_join(top_censored_trigrams, by = "trigrams") %>%
  count(trigrams, product, .drop = FALSE) 

plot_data %>%
  ggplot(aes(n, trigrams, fill = product)) +
  geom_col(position = "fill")
```

There is a good spread in the proportions, tokens like "on xx xx" and "of xx xx" are used when referencing to a date, eg "we had a problem on 06/25 2012".
Remember that the current tokenization engine strips the punctuation before tokenizing. 
This means that the above examples are being turned into "we had a problem on 06 25 2012" before creating n-grams.

We can as a practical example replace all cases of XX and XXXX with random integers to crudely simulate what the data might look like before it was censored. 
This is going a bit overboard since dates will be given values between 00 and 99 which would not be right, 
and that we don't know if only numerics have been censored.
Below is a simple function `uncesor_vec()` that locates all instances of `XX` and replaces them with a number between 11 and 99.
We don't need to handle the special case of `XXXX` as it automatically being handled.

```{r uncensor_vec}
uncensor <- function(n) {
  as.character(sample(seq(10 ^ (n - 1), 10 ^ n - 1), 1))
}

uncensor_vec <- function(x) {
  locs <- str_locate_all(x, "XX")
  
  map2_chr(x, locs, ~ {
    for (i in seq_len(nrow(.y))) {
      str_sub(.x, .y[i, 1], .y[i, 2]) <- uncensor(2)
    }
    .x
  })
}
```

And we can run a quick test to see if it works.

```{r, dependson="uncesor_vec"}
uncensor_vec("In XX/XX/XXXX I leased a XXXX vehicle")
```

Now we try to produce the same chart as \@ref(fig:censoredtrigram) but with the only difference being that we apply our uncensoring function to the text before tokenizing.

```{r uncensoredtrigram, dependson=c("complaintssplit", "uncensor_vec"), fig.cap="Trigrams without numbers flout to the top as the uncensored tokens are too spread out."}
complaints_train %>%
    slice(1:1000) %>%
  mutate(text = uncensor_vec(consumer_complaint_narrative)) %>%
  unnest_tokens(trigrams, text, token = "ngrams", 
                collapse = FALSE) %>%
  count(trigrams, sort = TRUE) %>%
  mutate(censored = str_detect(trigrams, 'xx')) %>%
  slice(1:20) %>%
  ggplot(aes(n, reorder(trigrams, n), fill = censored)) +
  geom_col() +
  scale_fill_manual(values = c("grey40", "firebrick")) +
  labs(y = "Trigrams", x = "Count")
```

The same trigrams that appear in the last chart appeared in this one as well, 
but none of the uncensored words appear in the top which is what is to be expected.
This is expected because while `xx xx 2019` appears towards the top in the first as it indicates a date in the year 2019, having that uncensored would split it into 365 buckets.
Having dates being censored gives more power to pick up the signal of a date as a general construct giving it a higher chance of being important.
But it also blinds us to the possibility that certain dates and months are more prevalent.

We have talked a lot about censoring data in this section.
Another way to look at this is a form of preprocessing in your data pipeline.
It is very unlikely that you want any specific person's social security number, credit card number or any other kind of personally identifiable information ([PII](https://en.wikipedia.org/wiki/Personal_data)) imbedded into your model.
Not only is it likely to provide a useful signal as they appear so rarely and most likely highly correlated with other known variables in your database.
More importantly, that information can become embedded in your model and begin to leak if you are not careful as showcased by @carlini2018secret, @Fredrikson2014 and @Fredrikson2015.
Both of these issues are important, and one of them could land you in a lot of legal trouble if you are not careful. 

If for example, you have a lot of social security numbers you should definitely not pass them on to your model, but there is no hard in annotation the presence of a social security number. 
Since a social security number has a very specific form we can easily construct a regular expression \@ref(regexp) to locate them.

```{block, type = "rmdnote"}
A social security number comes in the form AAA-BB-CCCC where AAA is a number between 001 and 899 excluding 666, BB is a number between 01 and 99 and CCCC is a number between 0001 and 9999. This gives us the following regex

(?!000|666)[0-8][0-9]{2}-(?!00)[0-9]{2}-(?!0000)[0-9]{4}
```

We can use a replace function to replace it with something that can be picked up by later preprocessing steps. 
A good idea is to replace it with a "word" that won't be accidentally broken up by a tokenizer.

```{r}
ssn_text <- c("My social security number is 498-08-6333",
              "No way, mine is 362-60-9159", 
              "My parents numbers are 575-32-6985 and 576-36-5202")

ssn_pattern <-  "(?!000|666)[0-8][0-9]{2}-(?!00)[0-9]{2}-(?!0000)[0-9]{4}"

str_replace_all(string = ssn_text, 
                pattern = ssn_pattern,
                replacement = "ssnindicator")
```

This technique isn't just useful for personally identifiable information but can be used anytime you want to intentionally but similar words in the same bucket, hashtags, emails, and usernames can sometimes also benefit from being annotated.

## Case study: Adding custom features

percentage of X's
a custom feature is something that we can easily count and add to our model that isn't part of simple BoW.
https://github.com/mkearney/textfeatures
examples include: number of urls

## Case study: the relationship between performance

## Case Study: feature hashing

## What evaluation metrics are appropriate

Data will most likely be sparse when using BoW

## Full game

### Feature selection

### Splitting the data

### Specifying models

### Cross-validation

### Evaluation

Interpretability.

"Can we get comparable performance with a simpler model?"
Compare with simple rule-based model
