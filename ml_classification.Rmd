# Classification

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE, 
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
``` 

What is classification.

## First attempt

first attempt and full game will use same data. 
First attempt might use a subset of the data to make the example easier to understand.
and properly to give balanced dataset, which then later can be explored.

### Look at the data

```{r, message=FALSE}
library(textrecipes)
library(tidymodels)
library(tidytext)
library(discrim)
library(readr)
```

We are going to be working with US consumer complaints on financial products and company responses.
It contains a text field containining the complaint along with information regarding what it was for,
how it was filed and the response. 
In this chapter we will try to predict what type of product the complaints are referring to. 
This first attempt will be limited to predicting if the product is a mortgage or not.

We can read in the complaint data \@ref(us-consumer-finance-complaints) with `read_csv()`.

```{r, message=FALSE}
complaints <- read_csv("data/complaints.csv.gz")
```

then we will start by taking a quick look at the data to see what we have to work with

```{r}
glimpse(complaints)
```

First thing to note is our target variable `product` which we need to trim only display "Mortgage" and "Other",
and the `consumer_complaint_narrative` variable which contains the complaints.
Here is the first 6 complaints:

```{r}
head(complaints$consumer_complaint_narrative)
```

Throughout the narratives is series of capital x's. This has been done to hide Personally identifiable information (PII). This is not a universial censoring mechanish and can vary from source to source, hopefully you will be able to get this information in the data dictionary but you should always look at the data yoursef to verify. We also see that all monetary amounts are surrounded by curly brackets, this is another step of preprocessing that have been done for us.

We can craft a regular expression to extract all the dollar amounts.

```{r}
complaints$consumer_complaint_narrative %>%
  str_extract_all("\\{\\$[0-9\\.]*\\}") %>%
  compact() %>%
  head()
```

## Building our first classification model

Since this data is given to us after the fact, 
we need to make sure that only the information that would be avaliable at the time of prediction is included in the model,
otherwise we are going to be very disappointed once the model is pushed to production.
The variables we can use as predictors are

- `date_received`
- `issue`
- `sub_issue`
- `consumer_complaint_narrative`
- `company`
- `state`
- `zip_code`
- `tags`
- `submitted_via`

Many of these have quite a lot of levels.
First we will include `date_received` for further consideration, along with `consumer_complaint_narrative` and `tags`.
`submitted_via` would have been a viable candidate too, but all the entries are "web".
The other variables could be of use too, but they are catagorial variables with many values so we will exclude them for now.

We start by splitting the data into a training and testing dataset.
But before we do that we will create a factor variable of product with the levels "Mortgage" and "Other".
Then we will use the `initial_split()` from **rsample** to create a binary split of the data. 
The `strata` argument is used to make sure that the split is created to make sure the distribution of `product` is similar in the training set and testing set. 
Since the split is done using random sampling we set a seed so we can reproduce the results.

```{r}
set.seed(1234)
complaints2class <- complaints %>%
  mutate(product = factor(if_else(product == "Mortgage", "Mortgage", "Other")))

complaints_split <- initial_split(complaints2class, strata = product)

complaints_train <- training(complaints_split)
complaints_test <- testing(complaints_split)
```

Looking the dimensions of the two split shows that it worked succesfully.

```{r}
dim(complaints_train)
dim(complaints_test)
```

Next we need to do some preprocessing. We need to do this since the models we are trying to use only support all numeric data. 

```{block, type = "rmdnote"}
Some models are able to handle factor variables and missing data. But it is in our best interest to manually deal with these problems so we know how they are handled.
```

The **recipes** package allows us to create a specification of the preprocessing steps we want to perform. Futhermore it contains the transformations we have trained on the training set and apply them in the same way for the testing set.
First off we use the `recipe()` function to initialize a recipes, we use a formula expression to specify the variables we are using along with the dataset.

```{r}
complaint_rec <- 
  recipe(product ~ date_received + tags + consumer_complaint_narrative, 
         data = complaints_train)
```

First will we take a look at the `date_received` variable. We use the `step_date()` to extract the month and day of the week (dow). Then we remove the original variable and dummify the variables created with `step_dummy()`.

```{r}
complaint_rec <- complaint_rec %>%
  step_date(date_received, features = c("month", "dow"), role = "dates") %>%
  step_rm(date_received) %>%
  step_dummy(has_role("dates"))
```

the `tags` variable includes some missing data. We deal with this by using `step_unknown()` to that adds a new level to the factor variable for cases of missing data. Then we dummify the variable with `step_dummy()`

```{r}
complaint_rec <- complaint_rec %>%
  step_unknown(tags) %>%
  step_dummy(tags)
```

Lastly we use **textrecipes** to handle the `consumer_complaint_narrative` variable. First we perform tokenization to words with `step_tokenize()`, by default this is done using `tokenizers::tokenize_words()`.
Next we remove stopwords with `step_stopwords()`, the default choice is the snowball stopword list, but custom lists can be provided too. Before we calculate the tf-idf we use `step_tokenfilter()` to only keep the 50 most frequent tokens, this is to avoid creating too many variables. To end off we use `step_tfidf()` to perform tf-idf calculations.

```{r}
complaint_rec <- complaint_rec %>%
  step_tokenize(consumer_complaint_narrative) %>%
  step_stopwords(consumer_complaint_narrative) %>%
  step_tokenfilter(consumer_complaint_narrative, max_tokens = 50) %>%
  step_tfidf(consumer_complaint_narrative)
```

Now that we have a full specification of the recipe we run `prep()` on it to train each of the steps on the training data.

```{r}
complaint_prep <- prep(complaint_rec)
```

We can now extract the transformed training data with `juice()`. To apply the prepped recipe to the testing set we use the `bake()` function.

```{r}
train_data <- juice(complaint_prep)
test_data <- bake(complaint_prep, complaints_test)
```

and taking a short `glimpse()` of the prepped dataset to make sure everything went well.

```{r}
glimpse(train_data)
```

For the modeling we will use a simple Naive Bayes model (TODO add citation to both Naive bayes and its use in text classification).
One of the main advanteges of Naive Bayes is its ability to handle a large number of features which we tend to get when using word count methods.
Here we have only kept the 50 most frequent tokens, we could have keep more tokens and a Naive Bayes model would be able to handle it okay, but we will limit it for this first time.

```{r}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("klaR")
nb_spec
```

Now we have everything we need to fit our first classification model, we just have to run `fit()` on out model specification and our training data.

```{r}
nb_fit <- nb_spec %>%
  fit(product ~ ., data = train_data)
```

We have more succesfully fitted out first classification model.

### Evaluation

```{r, eval=FALSE}
nb_model_fit %>%
  predict(test_data) %>%
  mutate(truth = test_data$product) %>%
  accuracy(truth, .pred_class)
```


## Different types of models

(Not all of these models are good, but are used to show strenghs and weaknessed)

- SVM
- Naive Bayes
- glmnet
- Random forrest
- knn
- NULL model

## Two class or multiclass

## Case study: What happens if you don't censor your data

The complaints data already have sensitive information censored out with XXXX and XX.
This can be seen as a kind of annotation, we don't get to know the specific acocunt numbers and birthday which would be mostly unique anyways and filtered out.

Below we have is the most frequent trigrams [#tokenizing-by-n-grams] from our training dataset.

```{r}
complaints_train %>%
  unnest_tokens(trigrams, consumer_complaint_narrative, token = "ngrams", 
                collapse = FALSE) %>%
  count(trigrams, sort = TRUE) %>%
  mutate(censored = stringr::str_detect(trigrams, 'xx')) %>%
  slice(1:20) %>%
  ggplot(aes(n, reorder(trigrams, n), fill = censored)) +
  geom_col() +
  scale_fill_manual(values = c("grey40", "firebrick"))
```

As you see the vast majority includes one or more censored words.

```{r}
uncensor <- function(n) {
  as.character(sample(seq(10^(n-1), 10^n-1), 1))
}

uncensor_vec <- function(x) {
  locs <- str_locate_all(x, "XX")
  
  map2_chr(x, locs, ~ {
    for (i in seq_len(nrow(.y))) {
      str_sub(.x, .y[i, 1], .y[i, 2]) <- uncensor(2)
    }
    .x
  })
}

complaints_train %>%
  select(consumer_complaint_narrative) %>%
  slice(1:1000) %>%
  mutate(consumer_complaint_narrative = uncensor_vec(consumer_complaint_narrative)) %>%
  unnest_tokens(trigrams, consumer_complaint_narrative, token = "ngrams") %>%
  count(trigrams, sort = TRUE) %>%
  mutate(censored = stringr::str_detect(trigrams, 'xx')) %>%
  slice(1:20) %>%
  ggplot(aes(n, reorder(trigrams, n), fill = censored)) +
  geom_col() +
  scale_fill_manual(values = c("grey40", "firebrick"))
```

```{r}
colnames(train_data)
```

## Case study: relationship between performace

## Case Study: feature hashing

## What evaluation metrics are appropiate

Data will most likely be sparse when using BoW

## Full game

### Feature selection

### Splitting the data

### Specifying models

### Cross-validation

### Evaluation

Inteprebility.

"Can we get comparable performance with a simpler model?"
Compare with simple rule based model

