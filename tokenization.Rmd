# Tokenization {#tokenization}

```{r, message=FALSE}
library(tokenizers)
library(tidyverse)
library(tidytext)
library(hcandersenr)
the_fir_tree <- hcandersen_en %>%
  filter(book == "The fir tree") %>%
  pull(text)
```

In this chapter we will become familiar with the concepts of *tokens*, *ngrams*, *tokenization* and how to perform tokenization in R.

## What is a token?

In R you will generally have text saved as a character vector of strings. If we look at the first paragraph of The Fir Tree, then the text is split into strings, which themselves are a series of letters, spaces and punctuation.

```{r}
head(the_fir_tree, 9)
```

These strings don't contain any information regarding what are words and what isn't. This is where tokenization comes in.

In tokenization you take an input (a string) and a type (a word) and proceed to split the input in to pieces (tokens) that correspond the type. [@Manning:2008:IIR:1394399]

![](images/tokenization/tokenization.jpg) TODO

Typically will we want to allow tokenization to happen on the word level. However it is quite difficult to define what we mean by a "word". As an exercise lets start by defining a word as being any selection of alphanumeric (letter and numbers) symbols. Lets start by using some regex with `strsplit` to split the first 2 lines of The Fir Tree by anything non alphanumeric.

```{r}
strsplit(the_fir_tree[1:2], "[^a-zA-Z0-9]+")
```

At first sight it looks pretty decent. We are losing all punctuation which may or may not be favorable, and the hero of the story "fir-tree" got split in half. Already it is clear that tokenization is going to be quite complicated. Luckily a lot of work have gone into this process, and we will use what they got. Introducing the **tokenizers** package. This package contains a wealth of fast tokenizers we can use.

```{r}
library(tokenizers)
tokenize_words(the_fir_tree[1:2])
```

And we get a sensible result. `tokenize_words` is using the **stringi** using the hood making it very fast. Tokenization according to words is done by finding word boundaries according to the specification from International Components for Unicode (ICU). The word boundary algorithm [@ICUWordBoundary] goes as follows

- Break at the start and end of text, unless the text is empty.
- Do not break within CRLF.
- Otherwise break before and after Newlines (including CR and LF)
- Do not break within emoji zwj sequences.
- Keep horizontal whitespace together.
- Ignore Format and Extend characters, except after sot, CR, LF, and Newline.
- Do not break between most letters.
- Do not break letters across certain punctuation.
- Do not break within sequences of digits, or digits adjacent to letters (“3a”, or “A3”).
- Do not break within sequences, such as “3.2” or “3,456.789”.
- Do not break between Katakana.
- Do not break from extenders.
- Do not break within emoji flag sequences. 
- Otherwise, break everywhere (including around ideographs).

While you might not understand what each and every step is doing you can appreciate it is many times more sophisticated then our initial approach. In the remaining of the book we will let the **tokenizers** package determine a baseline tokenizer for reference. We want to stress that the choice of tokenizer will have an influence on your results. Don't be afraid to experiment with different tokenizers or to write your own to fit your problem.

## Types of tokens

Taking a token to mean "word" was a useful idea however hard to implement concretely in software, however we can generalize the notion of a token to mean "document unit". Under this new definition we can let a token be a variety of things including

- characters
- words
- sentences
- lines
- paragraphs and
- ngrams.

We will in the following sections showcase how to do tokenization using the **tokenizers** package. These functions will take a character vector as the input and returns lists of character vectors. However these same operations can also be done using the **tidytext** package. 

```{r}
sample_vector <- c("This is the first of two strings",
                   "And here is the second string.")
sample_tibble <- tibble(text = sample_vector)
```

therefore the tokenization done by using `tokenize_words` on `sample_vector`

```{r}
tokenize_words(sample_vector)
```

will yield the same tokenization as `unnest_tokens` done on `sample_tibble`, only difference being the format. Difference.

```{r}
sample_tibble %>%
  unnest_tokens(word, text, token = "words")
```

Furthermore arguments used in `tokenize_words` can be passed through `unnest_tokens` using the `...` like so

```{r}
sample_tibble %>%
  unnest_tokens(word, text, token = "words", strip_punct = FALSE)
```

### character tokens

The first and simplest tokenization is the character tokenization. This simply splits the texts into characters. Here we run `tokenize_characters` with default starting parameters. Notice how it have arguments to convert everything to lowercase and to strip all non alpha numeric characters. These are both done to reduce the number of different tokens we are returned. The `tokenize_*()` functions will by default return a list of character vectors, one character vector for each string in the input.

```{r}
tft_token_characters <- tokenize_characters(x = the_fir_tree,
                                            lowercase = TRUE,
                                            strip_non_alphanum = TRUE,
                                            simplify = FALSE)
```

And if we take a look this is what we get.

```{r}
head(tft_token_characters) %>%
  glimpse()
```

However we don't have to stay with the defaults and we can include the punctuation and spaces by setting `strip_non_alphanum = FALSE` and we see that spaces and punctuation are included too now.

```{r}
tokenize_characters(x = the_fir_tree,
                    strip_non_alphanum = FALSE) %>%
  head() %>%
  glimpse()
```

TODO Find examples of when a character is hard to define. Look at Arabic, German (double s) and danish (double a).

### words tokens

The word tokenization is perhaps the most common and widely studied tokenization. As we described earlier it is the procedure of splitting our text into words. To do this we will use the `tokenize_words()` functions.

```{r}
tft_token_words <- tokenize_words(x = the_fir_tree,
                                  lowercase = TRUE, 
                                  stopwords = NULL, 
                                  strip_punct = TRUE,
                                  strip_numeric = FALSE)
```

And if we take a look this is what we get.

```{r}
head(tft_token_words) %>%
  glimpse()
```

We have already seen `lowercase = TRUE`, and `strip_punct = TRUE` and `strip_numeric = FALSE` should be fairly self-explanatory. They control whether we remove punctuation and numerics respectively. Lastly we have `stopwords = NULL`, which we will talk more in depth about in chapter \@ref(stopwords).

If we create a tibble with the two fairly tales "The fir tree" and "The little mermaid". Then we can use use `unnest_tokens` together with some **dplyr** verbs to find the must commonly used words in each.

```{r}
hcandersen_en %>%
  filter(book %in% c("The fir tree", "The little mermaid")) %>%
  unnest_tokens(word, text) %>%
  count(book, word) %>%
  group_by(book) %>%
  arrange(desc(n)) %>%
  slice(1:5)
```

We see that the 5 most commonly words in each fairly tale are fairly uninformative (only exception being "tree" in the "The fir tree" which might not be too surprising). These words are called *stop words* and will be handled in chapter \@ref(stopwords).

```{block, type='rmdtip'}
this is a test
```


### lines, sentences and paragraph tokens

The following tokenizers are rarely used to create tokens for analytic uses, as the tokens produced tend to be fairly unique it is very uncommon for multiple sentences in a text to be identical. However these tokenizers will still provide value for preprocessing and labeling.

For example if we take Jane Austin's novel Emma, It is already preprocessed to have each string being a line of at most 80 characters long. However it might be useful to the data split into chapters and paragraphs instead.

First we create a function, that takes a data.frame containing a variable called `text` and turns it into a data.frame where the the text is transformed to paragraphs. This is done first collapsing the string into one string by using `collapse = "\n"` to denote line breaks, next we use `tokenize_paragraphs()` to get the paragraphs and put them back into a data.frame and add a paragraph count with `row_number()`.

```{r}
add_paragraphs <- function(data) {
  pull(data, text) %>% 
    paste(collapse = "\n") %>%
    tokenize_paragraphs() %>%
    unlist() %>%
    tibble(text = .) %>%
    mutate(paragraph = row_number())
}
```

Now we take the data and add chapter count by seeing when `CHAPTER` appears on the beginning of a line. Then we next the text column, apply our `add_paragraphs()` function and unnest again.

```{r}
library(janeaustenr)

emma_paragraphed <- tibble(text = emma) %>%
  mutate(chapter = cumsum(str_detect(text, "^CHAPTER "))) %>%
  filter(chapter > 0, !str_detect(text, "^CHAPTER ")) %>%
  nest(data = text) %>%
  mutate(data = map(data, add_paragraphs)) %>% 
  unnest(cols = c(data))

glimpse(emma_paragraphed)
```

Now we have `r nrow(emma_paragraphed)` separate paragraphs we can analyse. It is easy to see how we could go a step further to split by sentences, lines or words.

### ngrams tokens

A ngram (also referred to as a n-gram) is a term in linguistics for a contiguous sequence of n items from a given sequence of text or speech. The item can be phonemes, syllables, letters or words depending on the application, however it appears that most people take the default item to be words. In the rest of the book we will use ngram to denote word ngrams unless otherwise stated. We use Latin prefixes, such that a 1-gram is called a unigram, 2-gram is called a bigram, 3-gram called a trigram and so on. Examples of different ngrams are

- unigram: "Hello", "day", "my", "little"
- bigram: "White House", "happy dog", "to be", "Robin Hood"
- trigram: "You and I", "please let go", "no time like", "great strong soldier"

The benefit of using ngrams compared to words is that we get to capture word order which would order wise be lost. Similarly if you use character-ngrams you will be able to model the beginning and ends of words since a space will be located in the end of a ngram for the end of a word and in the beginning of the ngram of the beginning of a word.

To tokenize into ngrams we can use the the function `tokenize_ngrams()`. It has a little more arguments so lets go over them one by one.

```{r}
tft_token_ngram <- tokenize_ngrams(x = the_fir_tree,
                                   lowercase = TRUE,
                                   n = 3L,
                                   n_min = 3L,
                                   stopwords = character(),
                                   ngram_delim = " ",
                                   simplify = FALSE)
```

We have seen the arguments `lowercase`, `stopwords` and `simplify` before and they work the same as before. Next we have `n`, this is the argument to determine which degree of ngram to return. Setting `n = 1` returns unigrams, `n = 2` bigrams, `n = 3` gives trigrams and so on. Related is the `n_min` argument, which specifies the minimum number of ngrams to include. By default both `n` and `n_min` is set to 3 making `tokenize_ngrams()` return only trigrams, but by setting `n = 3` and `n_min = 1` will we get all unigrams, bigrams and trigrams of a text. Lastly we have the `ngram_delim` argument which specifies the separator between words in the ngrams. Now lets look at the result from the first line of The Fyr Tree

```{r}
tft_token_ngram[[1]]
```

Notice how the words in the trigrams overlap such that the word "down" appears in the middle of the first trigram and beginning of the second trigram. 

It is important to choose the right value for `n` when using ngrams. Using unigrams provide more stable number but you loose a lot of information since you don't include the word order, using a higher value for n give more information results, however we are drastically increasing the vector space in which the words take place thus reducing the counts drastically. A sensible starting point in most cases is 3, however if you don't have many words it might be worth to start at 2 instead of 3 and go from there. The following chart showcases the how once you start using trigrams that you get more infrequent tokens. 

```{r}
lenght_and_max <- function(x) {
  tab <- table(x)
  
  paste(length(tab), max(tab), sep = "-")
}

if (packageDescription("tidyr")$Version >= "0.8.3.9000") {
  plotting_data <- hcandersen_en %>%
    nest(data = c(text)) %>%
    mutate(data = map_chr(data, ~ paste(.x$text, collapse = " "))) %>%
    mutate(unigram = tokenize_ngrams(data, n = 1, n_min = 1) %>% map_chr(lenght_and_max),
           bigram = tokenize_ngrams(data, n = 2, n_min = 2) %>% map_chr(lenght_and_max),
           trigram = tokenize_ngrams(data, n = 3, n_min = 3) %>% map_chr(lenght_and_max),
           quadrugram = tokenize_ngrams(data, n = 4, n_min = 4) %>% map_chr(lenght_and_max)) %>%
    select(unigram, bigram, trigram, quadrugram) %>%
    pivot_longer(cols = unigram:quadrugram, names_to = "ngrams") %>%
    separate(value, c("length", "max"), convert = TRUE) %>%
    mutate(ngrams = factor(ngrams, levels = c("quadrugram", "trigram", "bigram", "unigram"))) 
  
  plotting_data  %>%
    ggplot(aes(length, ngrams, color = max)) +
    geom_jitter(width = 0, alpha = 0.8, height = 0.35) +
    scale_color_viridis_c(trans = "log", labels=scales::comma) +
    labs(x = "Number of unique ngrams", 
         y = NULL,
         color = "Count of\nmost frequent\nngram",
         title = "Using longer ngrams results in higher number of unique tokens \nwith fewer counts",
         subtitle = "Each point represents a H.C. Andersen Fairy tale")
}
```

## Where does it break down?

TODO What do you lose when you tokenize?
- double spaces
- potentially capitalization. Which could collapse patty and Patty together.

TODO compare methods and explain why they are different. 

TODO Showcase where the different methods have strengths and weaknesses

TODO Do comparing of compression of data with different types of tokenizations

```{r}
if (packageDescription("tidyr")$Version >= "0.8.3.9000") {
  hcandersen_en %>%
    nest(text) %>%
    mutate(data = map_chr(data, ~ paste(.x$text, collapse = " "))) %>%
    mutate(chars = tokenize_characters(data) %>% map_int(~table(.x) %>% length()),
           chars_non_alphanum = tokenize_characters(data, strip_non_alphanum = FALSE) %>% map_int(~table(.x) %>% length()),
           words = tokenize_words(data) %>% map_int(~table(.x) %>% length()),
           words_no_lowercase = tokenize_words(data, lowercase = FALSE) %>% map_int(~table(.x) %>% length()),
           words_stems = tokenize_word_stems(data) %>% map_int(~table(.x) %>% length())) %>%
    select(-data) %>%
    pivot_longer(-book) %>%
    ggplot(aes(name, value)) +
    geom_boxplot() +
    geom_jitter(alpha = 0.1) +
    scale_y_log10() +
    theme_minimal() +
    coord_flip() +
    labs(title = "Number of distinct tokens varies greatly with choice of tokenizer")
}
```


## Building your own tokenizer

Sometimes the out-of-the-box tokenizers wont be able to do what you want need them to do. In this case you will have to wield **stringi**/**stringr** and [regular expression](https://en.wikipedia.org/wiki/Regular_expression) (regex for short). There are two main paths to tokenization

1. *Split* the string up according to some rule
2. *Extract* tokens based on some rule

the number and complexity of your rules is determined by your desired outcome. You can reach complex outcomes by chaining together many smaller rules. In this section we will implement a couple of specialty tokenizers to showcase the techniques.  

### Tokenize to characters, only keeping letters

Here we want to make a modification to `tokenize_characters()` such that we only keep keep letters. At first thought there are 2 main options. Use `tokenize_characters()` and remove any non letters, or extract extract the letters one by one. This is a fairly simple goal so we will go with the latter option. This is an *extract* task and we will be using `str_extract_att()` as each string has the possibility of including more then 1 token. Since we want to extract letters we can use the letters character class `[:alpha:]` to match for letters and the quantifier `{1}` to only extract the first one. (In this example leaving out the quantifier yields the same result, however more specific regular expressions runs faster).

```{r}
letter_tokens <- str_extract_all("This sentence include 2 numbers and 1 period.", 
                                 "[:alpha:]{1}")
letter_tokens
```

You could be tempted to use specify the character class yourself to be something like `[a-zA-Z]{1}`, it would run faster but you would lose non-English characters. This is a design choice you would have to make depending on the goals of your specific problem. 

```{r}
danish_sentence <- "Så mødte han en gammel heks på landevejen; hun var så ækel, hendes underlæbe hang hende lige ned på brystet."

str_extract_all(danish_sentence, "[:alpha:]")  
str_extract_all(danish_sentence, "[a-zA-Z]")
```

### Allow for hyphenated words

So far we have had problems with "fir-tree" being split up, we are going to fix this problem in two different ways. First we will be splitting on white spaces this is a decent way of finding words, similarly this doesn't split up hyphenated words as the hyphen isn't considered a white-space. Next we will find a regex to match words with hyphen and extract those.

splitting by white-space is easy since we can use character classes \@ref(tab:characterclasses). We will use the white space character class `[:space:]` to split the sentence over

```{r}
str_split("This isn't a sentence with hyphenated-words.", "[:space:]")
```

And we nothing that is is doing pretty well. This version doesn't drop punctuation, but this can be achieved by removing punctuation characters in the beginning and ends of words.

```{r}
str_split("This isn't a sentence with hyphenated-words.", "[:space:]") %>%
  map(~ str_remove_all(.x, "^[:punct:]+|[:punct:]+$"))
```

This regex is a little complicated so lets split it up a little. `^[:punct:]*` will look at the beginning of the string (`^`) to look for punctuation character (`[:punct:]`) where it will select 1 or more (`+`). `[:punct:]+$` will look for punctuation character (`[:punct:]`) that appears 1 or more (`+`) and be in the end of the string (`$`). Lastly this will alternate (`|`) such that we get matches from both sides of the words. The reason we are using the quantifier `+` is because there are cases where a word is followed by multiple characters we don't want, such as `okay...` and `Really?!!!`. We are using `map()` since `str_split()` returns a list, and we want `str_remove_all()` to be applied to each element in the list, this example only have one element. 

```{block, type = "rmdnote"}
If you are in a situation where you want to avoid the dependencies that come with `purrr` you are able to use `lapply()` just as well. 
`lapply(str_remove_all, pattern = "^[:punct:]+|[:punct:]+$")`
```

Now we will see if we can get the same result by extraction. We will start of by constructing a regular expression that will capture the idea of a hyphenated word. Our definition is a word with 1 hyphen located inside. Since we want the hyphen to be inside the word will be need a non-zero number of character on either side of the hyphen. 

```{r}
str_extract_all("This isn't a sentence with hyphenated-words.", "[:alpha:]+-[:alpha:]+")
```

but wait, this only matched the hyphenated word. This is happening because we are only matching words with hyphens, if we add the quantifier `?` then we can match 0 or 1 occurrences.

```{r}
str_extract_all("This isn't a sentence with hyphenated-words.", "[:alpha:]+-?[:alpha:]+")
```

Now we are getting more words, however the ending of `"ins't"` isn't there anymore and we lost the word `"a"`. We can get matches for the whole contraction by expanding the character class `[:alpha:]` to include the character `'`. This is done with `[[:alpha:]']`.

```{r}
str_extract_all("This isn't a sentence with hyphenated-words.", "[[:alpha:]']+-?[[:alpha:]']+")
```

Next we need to find out why `"a"` wasn't matched. If we look at the regular expression, we remember that we imposed the restriction that a non-zero number of characters needed to surround the hyphen place to avoid matching words start or ending with a hyphen, this means that the smallest possible pattern matched is 2 characters long. This can be fixed by using an alternation with `|`, we will keep our previous match on the left-hand side, and include `[:alpha:]{1}` on the right-hand side to match the single length words that won't be picked up by the left-hand side. Notice how we aren't using `[[:alpha:]']` since we are not interested in matching single `'`'s.

```{r}
str_extract_all("This isn't a sentence with hyphenated-words.", "[[:alpha:]']+-?[[:alpha:]']+|[:alpha:]{1}")
```

And we are getting the same answer as before. 

### Character ngrams
TODO change to toktok tokenizer
Next we want to look at all character ngrams, for the purpose of this example a character ngram is defined as a consecutive group of n characters. This will not jump over spaces, but will include overlapping matches. So the 3-grams of `"nice dog"` would be `"nic", "ice", "dog"`. since the regex engine in R normally doesn't support overlapping matches we have to get creative. First We will use a "lookahead" to find the location of all the matches, then we will use those locations to match the ngrams.

```{r}
sentence <- c("This isn't a sentence with hyphenated-words.",
              "Same with this one")

ngram_loc <- str_locate_all(sentence, "(?=(\\w{3}))")

map2(ngram_loc, sentence, ~str_sub(.y, .x[, 1], .x[, 1]+2))
```

### Wrapping it into a function

We have shown how one can take some regular expressions to extract the tokens we want. however the code have so far been rather unstructured. We would ideally wrap these tasks into functions that can be used the same way `tokenize_words()` is used.

Lets start with the example with hyphenated words. To give it a little more spice lets add an option to turn all the words to lowercase.

```{r}
tokenize_hyphonated_words <- function(x, lowercase = TRUE) {
  if (lowercase)
    x <- tolower(x)
  
  str_split(x, "[:space:]") %>%
    map(~ str_remove_all(.x, "^[:punct:]+|[:punct:]+$"))
}


tokenize_hyphonated_words(the_fir_tree[1:3])
```

We just need to make sure that the output is a list. Notice how we turned to lowercase before we did everything else as the remaining on the operations are case insensitive.

Next lets turn character ngram into a tokenizer. And we would like to have a variable `n` argument.

```{r}
tokenize_character_ngram <- function(x, n) {
  ngram_loc <- str_locate_all(x, paste0("(?=(\\w{", n, "}))"))

  map2(ngram_loc, x, ~str_sub(.y, .x[, 1], .x[, 1] + n - 1))
}
tokenize_character_ngram(the_fir_tree[1:3], n  =3)
```

Notice how we can use `paste0()` to construct a regex. 

## Tokenization benchmark

TODO showcase other libraries for tokenization

```{r}
the_fir_tree1 <- c("1", the_fir_tree)

bench::mark(
  `[:alpha:]` = str_extract_all(the_fir_tree1, "[:alpha:]"),
  `[a-zA-Z]` = str_extract_all(the_fir_tree1, "[a-zA-Z]"),
  `[a-zA-Z]{1}` = str_extract_all(the_fir_tree1, "[a-zA-Z]{1}"),
  `[:Letter:]` = str_extract_all(the_fir_tree1, "[:Letter:]")
)
```
