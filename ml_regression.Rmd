# Regression {#mlregression}

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE, 
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
options(crayon.enabled = FALSE)

## for Julia's local environment
## spacyr::spacy_initialize(condaenv = "r-spacyr", entity = FALSE)
``` 

In the previous chapter, we focused on using modeling to predict *labels* on documents, such as what kind of financial product a CFPB complaint was discussing or MORE HERE TODO. These are examples of classification models. We can also use machine learning to predict *continuous values* that are associated with documents. For example, let's consider a sample of opinions from the United States Supreme Court, available in the **scotus** [@R-scotus] package.

```{r scotussample}
library(tidyverse)
library(scotus)

scotus_sample %>%
  as_tibble()
```

This dataset contains the entire text of each opinion in the `text` column, along with the `case_name` and `docket_number`. Notice that we also have the year that each case was decided by the Supreme Court; this is a continuous variable (rather than a group membership of discrete label). 

```{block, type = "rmdtip"}
If we want to build a model to predict which court opinions were written in which years, we would build a regression model.
```


- A **classification model** predicts a class label or group membership.
- A **regression model** predicts a numeric or continuous value.

In text modeling, we use text data (such as the text of the court opinions), sometimes combined with other structured, non-text data, to predict the continuous value of interest (such as year of the court opinion).

## A first regression model

Let's build our first regression model using this sample of Supreme Court opinions. Before we start, let's check out how many opinions we have over time in Figure \@ref(fig:scotus_hist).

```{r scotushist, dependson="scotussample", fig.cap="Supreme Court opinions per decade in sample"}
scotus_sample %>%
  mutate(year = as.numeric(year),
         year = 10 * (year %/% 10)) %>%
  count(year) %>%
  ggplot(aes(year, n)) +
  geom_col() +
  labs(x = "Year", y = "Number of opinions per decade")
```

This sample of opinions reflects the true distribution over time of available opinions for analysis; there are many more opinions per year in this dataset after about 1990 than before. We will need to account for that in our modeling.

### Building our first regression model {#firstregression}

Our first step in building a model is to split our data into training and testing sets. We use functions from **tidymodels** for this; we use `initial_split()` to set up *how* to split the data, and then we use the functions `training()` and `testing()` to create the datasets we need. Let's also convert the year to a numeric value since it was originally stored as a character.

```{r scotussplit, dependson="scotussample"}
library(tidymodels)
set.seed(1234)
scotus_split <- scotus_sample %>%
  mutate(year = as.numeric(year)) %>%
  initial_split()

scotus_train <- training(scotus_split)
scotus_test <- testing(scotus_split)
```

Next, let's preprocess our data to get it ready for modeling using a recipe. We'll use both general preprocessing functions from **tidymodels** and specialized functions just for text from **textrecipes** in this preprocessing. What are the steps in creating this recipe?

- First, we must specify in our initial `recipe()` statement the form of our model (with the formula `year ~ text`, meaning we will predict the year of each opinion from the text) and what our training data is.
- Then, we tokenize (Chapter \@ref(tokenization)) the text of the court opinions. 
- Next, we filter to only keep the top 500 tokens by term frequency, after removing stop words.
- The recipe step `step_tfidf()`, used with defaults as here, weights each token frequency by the inverse document frequency.
- As a last step, we normalize (center and scale) these tf-idf values. We need to do this centering and scaling because it's important for lasso regularization.

Finally, we `prep()` the recipe. This means we actually compute something for all these steps using our training data; we estimate the required parameters from `scotus_train` to implement these steps so this whole sequence can be applied later to another dataset, such as resampled folds or testing data.

```{r scotusrec, dependson="scotussplit"}
library(textrecipes)

scotus_rec <- recipe(year ~ text, data = scotus_train) %>%
  step_tokenize(text) %>%
  step_tokenfilter(text, max_tokens = 500) %>%
  step_tfidf(text) %>%
  step_normalize(all_predictors())

scotus_prep <- prep(scotus_rec)

scotus_prep
```

Let's create a `workflow()` to bundle together this recipe with any model specifications we may want to create later. A _model workflow_ is a convenient way to combine different modeling components (a preprocessor plus a model specification); when these are bundled explicitly, it can be easier to keep track of your modeling plan, as well as fit your model and predict on new data.

First, let's only add the data preprocessor, `scotus_rec`.

```{r scotuswf, dependson="scotusrec"}
scotus_wf <- workflow() %>%
  add_recipe(scotus_rec) 

scotus_wf
```

Notice that there is no model yet: `Model: None`. It's time to specify the model we will use! Let's build a lasso regression model with `mixture = 1`. Before fitting, we set up a model specification.

```{r lassospec}
lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

lasso_spec
```

Everything is now ready for us to fit our model. Let's add our model to the workflow with `add_model()` and fit to our training data `scotus_train`.

```{r lassofit, dependson=c("lassospec", "scotuswf")}
lasso_fit <- scotus_wf %>%
  add_model(lasso_spec) %>%
  fit(data = scotus_train)
```

We have successfully fit a regularized regression model to this dataset of Supreme Court opinions. What does the result look like? We can access the fit using `pull_workflow_fit()` and even `tidy()` the results into a convenient dataframe format.

```{r dependson="lassofit"}
lasso_fit %>%
  pull_workflow_fit() %>%
  tidy()
```

We see here, printing out for the most regularized examples, what contributes to a Supreme Court opinion being written more recently.

### Evaluating our model

One option for our evaluating our model is to predict one time on the testing set to measure performance. 

```{block, type = "rmdwarning"}
The testing set is extremely valuable data, however, and in real world situations, you can only use this precious resource one time (or at most, twice). 
```

The purpose of the testing data is to estimate how your final model will perform on new data. Often during the process of modeling, we want to compare models or different model parameters. We can't use the test set for this.

Another option for our evaluating models is to predict one time on the training set to measure performance. This is the _same data_ that was used to train the model, however, and evaluating on the training data often results in performance estimates that are too optimistic. This is especially true for powerful machine learning algorithms that can learn subtle patterns from data.

Yet another option for evaluating or comparing models is to use a separate validation set. In this situation, we split our data not into two sets (training and testing) but into three sets (testing, training, and validation). The validation set is used for computing performance metrics to compare models or model parameters. This can be a great option if you have enough data for it, but often we as practitioners are not so lucky. 

What are we to do, then, if we want to train multiple models and find the best one? Or compute a reliable estimate for how our model has performed without wasting the valuable testing set? We can use **resampling**. When we resample, we create new simulated datasets from the training set for the purpose of, for example, measuring model performance.

Let's estimate the performance of the lasso regression model we just fit. We can do this using resampled datasets built from the training set. Let's create cross 10-fold cross-validation sets, and use these resampled sets for performance estimates.

```{r scotusfolds, dependson="scotussplit"}
set.seed(123)
scotus_folds <- vfold_cv(scotus_train)

scotus_folds
```

Each of these "splits" contains information about how to create cross-validation folds from the original training data. In this example, 90% of the training data is included in each fold for analysis and the other 10% is held out for assessment.

In Section \@ref(firstregression), we fit one time to the training data as a whole. Now, to estimate how well that model performs, let's fit many times, once to each of these resampled folds, and then evaluate on the heldout part of each resampled fold.

```{r lassors, dependson=c("scotuswf", "scotusfolds", "lassospec")}
set.seed(123)
lasso_rs <- fit_resamples(
  scotus_wf %>% add_model(lasso_spec),
  scotus_folds,
  control = control_resamples(save_pred = TRUE)
)

lasso_rs
```

These results look a lot like the resamples, but they have some additional columns, like the `.metrics` that we can use to measure how well this model performed and the `.predictions` we can use to explore that performance more deeply. What results do we see, in terms of performance metrics?

```{r, dependson="lassors"}
lasso_rs %>%
  collect_metrics()
```

The default performance metrics to be computed for regression models are RMSE (root mean squared error) and $R^2$. 

```{block, type = "rmdnote"}
The lower RMSE is, the better; the closer $R^2$ is to one, the better. 
```

These values are quantitative estimates for how well our model performed, and can be compared across different kinds of models. Figure \@ref(fig:firstregpredict) shows the predicted years for these Supreme Court opinions plotted against the true years when they were published, for all the resampled datasets.

```{r firstregpredict, dependson="lassors", fig.width=8, fig.height=8, fig.cap="Most Supreme Court opinions are near the dashed line, indicating good agreement between our lasso regression predictions and the real years"}
lasso_rs %>%
  collect_predictions() %>%
  ggplot(aes(year, .pred, color = id)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Truth",
    y = "Predicted year",
    color = NULL,
    title = "Predicted and true years for Supreme Court opinions",
    subtitle = "Each cross-validation fold is shown in a different color"
  )
```

This first model we have tried did a worse job for Supreme Court opinions from before 1850, but overall this looks good!

## Compare to the null model

One way to assess a model like this one is to compare its performance to a "null model". 

```{block, type = "rmdnote"}
A null model is a simple, non-informative model that always predicts the largest class (for classification) or the mean (such as the mean year of Supreme Court opinions, in our specific regression case).
```

We can use the same function `fit_resamples()` and the same preprocessing recipe as before, switching out our lasso model specification for the `null_model()` specification.

```{r nullrs, eval = FALSE, dependson=c("scotuswf", "scotusfolds")}
## not running until this change is on CRAN: https://github.com/tidymodels/tune/pull/207
null_regression <- null_model() %>% 
  set_engine("parsnip") %>%
  set_mode("regression")

null_rs <- fit_resamples(
  scotus_wf %>% add_model(null_regression),
  scotus_folds
)

null_rs
```

What results do we obtain from the null model, in terms of performance metrics?

```{r, eval = FALSE, dependson="nullrs"}
null_rs %>%
  collect_metrics()
```

The RMSE and $R^2$ indicate that this null model is dramatically worse than our first model. Even our first very attempt at a regression model (using only unigrams and no tuning) did much better than the null model; the text of the Supreme Court opinions has enough information in it related to the year the opinions were published that we can build successful models.

## Tuning lasso hyperparameters {#tunelasso}

The value `penalty = 0.1` for regularization in Section \@ref(firstregression) was picked somewhat at random. How do we know the *right* or *best* regularization parameter penalty? Instead of learning this during model training, we can estimate the best value by training many models on resampled data sets and exploring how well all these models perform. Let's build a new model specification for **model tuning**. 

```{block, type = "rmdtip"}
Think of `tune()` here as a placeholder for the regularization penalty.
```

After the tuning process, we can select a single best numeric value.
 
```{r scotustunespec}
tune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

tune_spec
```

We can create a regular grid of values to try using a convenience function for `penalty()`.

```{r scotuslambdagrid}
lambda_grid <- grid_regular(penalty(), levels = 30)

lambda_grid
```

The function `grid_regular()` is from the dials package. It chooses sensible values to try for a parameter like the regularization penalty; here, we asked for 30 different possible values to try.

Now it is time to tune! Let's use `tune_grid()` to fit a model at each of the values for the regularization penalty in our regular grid.

```{block, type = "rmdnote"}
Tuning a model uses a similar syntax compared to fitting a model to a set of resampled datasets for the purposes of evaluation (`fit_resamples()`) because the two tasks are so similar. The difference is that when you tune, each model that you fit has _different_ parameters and you want to find the best one.
```

We add our tuneable model specification `tune_spec` to the same workflow we've been using so far that contains the preprocessing recipe.

```{r scotustuners, dependson=c("scotuswf", "scotusfolds", "scotuslambdagrid", "scotustunespec")}
set.seed(2020)
tune_rs <- tune_grid(
  scotus_wf %>% add_model(tune_spec),
  scotus_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = TRUE)
)

tune_rs
```

Now, instead of one set of metrics, we have a set of metrics for each value of the regularization penalty.

```{r dependson="scotustuners"}
tune_rs %>%
  collect_metrics
```

Let's visualize these metrics, RMSE and $R^2$, in Figure \@ref(fig:scotustunevis) to see what the best model is.

```{r scotustunevis, dependson="scotustuners", fig.width=8, fig.height=8, fig.cap="We can identify the best regularization penalty from model performance metrics, for example, at the lowest RMSE."}
tune_rs %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none") +
  labs(title = "Lasso model performance across regularization penalties",
       subtitle = "Performance metrics can be used to identity the best penalty")
```

We can view the best results with `show_best()` and a choice for the metric.

```{r dependson="scotustuners"}
tune_rs %>%
  show_best("rmse")
```

We can extract the best value for, say, RMSE from our tuning results with `select_best()`.

```{r scotuslowestrmse, dependson="scotustuners"}
lowest_rmse <- tune_rs %>%
  select_best("rmse")

lowest_rmse
```

Next, let's finalize our tuneable workflow with this regularization penalty. This is the regularization penalty that our tuning results show us result in the best model.

```{r}
final_lasso <- finalize_workflow(
  scotus_wf %>% add_model(tune_spec),
  lowest_rmse
)

final_lasso
```

Instead of `penalty = tune()` like before, now our workflow has finalized values for all arguments. The preprocessing recipe has been evaluated on the training data, and we tuned the regularization penalty so that we have a penalty value of `r round(lowest_rmse$penalty, 3)`. This workflow is ready to go!

## Compare to a random forest model

Random forest models are broadly used in predictive modeling contexts because they are low-maintenance and perform well. For example, see @Caruana2008 and @Olson2017 for comparisons of the performance of common models such as random forest, decision tree, support vector machines, etc. trained on benchmark datasets; random forest models were one of the best overall. Let's see how a random forest model performs with our dataset of Supreme Court opinions.

First, let's build a random forest model specification, using the ranger implementation. Random forest models are known for performing well without tuning, so we will just make sure we have enough `trees`.

```{r scotusrfspec}
rf_spec <- rand_forest(trees = 1000) %>%
  set_engine("ranger") %>%
  set_mode("regression")
```

Now we can fit this random forest model. Let's use `fit_resamples()` again, so we can evaluate the model performance. We will use three arguments to this function:

- Our modeling `workflow()`, with the same preprocessing recipe we have been using so far in this chapter plus our new random forest model specification
- Our cross-validation resamples of the Supreme Court opinions
- A `control` argument to specify that we want to keep the predictions, to explore after fitting

```{r, eval = FALSE, scotusrfrs, dependson=c("scotuswf", "scotusfolds", "scotusrfspec")}
rf_rs <- fit_resamples(
  scotus_wf %>% add_model(rf_spec),
  scotus_folds,
  control = control_resamples(save_pred = TRUE)
)

rf_rs
```

We can use `collect_metrics()` to obtain and format the performance metrics for this random forest model.

```{r, eval = FALSE, dependson="nullrs"}
rf_rs %>%
  collect_metrics()
```

This looks pretty promising, so let's explore the predictions for this random forest model.

```{r, eval = FALSE, rfpredict, dependson="scotusrfrs", fig.width=8, fig.height=8, fig.cap="The random forest model did not perform very sensibly across years, compared to our first attempt using a linear model with lasso regularization"}
rf_rs %>%
  collect_predictions() %>%
  ggplot(aes(year, .pred, color = id)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Truth",
    y = "Predicted year",
    color = NULL,
    title = "Predicted and true years for Supreme Court opinions using a random forest model",
    subtitle = "Each cross-validation fold is shown in a different color"
  )
```

Figure \@ref(fig:rfpredict) shows some of the strange behavior from our fitted model. This model learned to predict the large number of opinions in 1999 and 2000 very accurately, but at the expense of all other years. One of the defining characteristics of text data is that it is **sparse**, and tree-based models such as random forests may not be well-suited to sparse data. 

```{block, type = "rmdtip"}
Models that work best with text tend to be models designed for or otherwise appropriate for sparse data.
```

## Case study: removing stop words

We did not remove stop words (Chapter \@ref(stopwords)) in any of our models so far in this chapter. What impact will removing stop words have, and how do we know which stop word list is the best to use? The best way to answer these questions is with experimentation.

Removing stop words is part of data preprocessing, so we define this step as part of our preprocessing recipe. Let's build a small recipe wrapper helper function so we can pass a value `stopword_name` to `step_stopwords()`.

```{r stopwordrec, dependson="scotussplit"}
stopword_rec <- function(stopword_name) {
  recipe(year ~ text, data = scotus_train) %>%
    step_tokenize(text) %>%
    step_stopwords(text, stopword_source = stopword_name) %>%
    step_tokenfilter(text, max_tokens = 500) %>%
    step_tfidf(text) %>%
    step_normalize(all_predictors())
}
```

For example, now we can create a recipe that removes the Snowball stop words list by calling this function.

```{r dependson="stopwordrec"}
stopword_rec("snowball")
```

Next, let's set up a workflow that has a model only, using `add_model()`.

```{r tuneablewf, dependson="scotustunespec"}
tuneable_wf <- workflow() %>%
  add_model(tune_spec) 

tuneable_wf
```

Notice that for this workflow, there is no preprocessor yet: `Preprocessor: None`. This workflow uses the same tuneable lasso model specification that we used in Section \@ref(tunelasso) but we are going to combine several different preprocessing recipes with it, one for each stop word lexicon we want to try.

The last time we tuned a lasso model, we used the defaults for the penalty parameter and 30 levels. Let's restrict the values this time using the `range` argument, so we don't test out as small values for regularization, and only try 20 levels.

```{r stopwordgrid}
stopword_grid <- grid_regular(penalty(range = c(-5, 0)), levels = 20)

stopword_grid
```

Now we can put this all together and tune these models which include stop word removal. We could create a little helper function for tuning like we did for the recipe, but we have printed out all three calls to `tune_grid()` for extra clarity. Notice for each one that there are three arguments:

- A tuneable workflow, which consists of the tuneable lasso model specification and a data preprocessing recipe with stop word removal
- The same cross-validation folds we created earlier
- Our new grid of values for the regularization parameter

```{r stopwordsres, dependson=c("stopwordgrid", "tuneablewf", "stopwordrec", "scotusfolds")}
set.seed(123)
snowball_rs <- tune_grid(
  tuneable_wf %>% add_recipe(stopword_rec("snowball")),
  scotus_folds,
  grid = stopword_grid
)

set.seed(234)
smart_rs <- tune_grid(
  tuneable_wf %>% add_recipe(stopword_rec("smart")),
  scotus_folds,
  grid = stopword_grid
)

set.seed(345)
stopwords_iso_rs <- tune_grid(
  tuneable_wf %>% add_recipe(stopword_rec("stopwords-iso")),
  scotus_folds,
  grid = stopword_grid
)
```

After fitting models for each of possible regularization values to each of the cross-validation folds, these sets of results contain metrics computed for removing that set of stop words.

```{r dependson="stopwordsres"}
collect_metrics(smart_rs)
```

We can explore whether one of these sets of stop words performed better than the others by comparing the performance, for example in terms of RMSE as shown Figure \@ref(fig:snowballrmse).

```{r snowballrmse, dependson="stopwordsres", fig.cap="Comparing model performa"}
list(snowball = snowball_rs, 
     smart = smart_rs, 
     `stopwords-iso` = stopwords_iso_rs) %>%
  map_dfr(show_best, "rmse", .id = "name") %>%
  ggplot(aes(name, mean, color = name)) +
  geom_point(size = 3, alpha = 0.8, show.legend = FALSE) +
  labs(x = NULL, y = "mean RMSE",
       title = "Model performance for three stop word lexicons",
       subtitle = "For this dataset, the Snowball lexicon performed best")
```

The Snowball lexicon contains the smallest number of words (see Figure \@ref(fig:stopwordoverlap)) and, in this case, results in the best performance. Removing fewer stop words results in the best performance.

```{block, type = "rmdwarning"}
This result is not generalizable to all data sets and contexts, but the approach outlined in this section **is** generalizable. 
```

This approach can be used to compare different lexicons and find the best one for a specific data set and model. Notice also that the resulting RMSE is lower than _without_ removing stop words (compare to Figure \@ref(fig:scotustunevis) and following), indicating that removing stop words (at least the small Snowball set) is a good choice.

```{r}
knitr::knit_exit()
```


## Case study: varying n-grams 

Tune over ngrams with tokenizers: for now write with three separate recipes

Discuss again (refer back) to how different degrees measure different kinds of info

Use different n of grams, combined with different ways of removing stopwords?

```{r}
scotus_ngram_rec <- recipe(year ~ text, data = scotus_train) %>%
  step_tokenize(text, token = "ngrams", options = list()) %>%
  step_stopwords(text) %>%
  step_tokenfilter(text, max_tokens = 500) %>%
  step_tfidf(text) 

scotus_ngram_rec
```


```{r}
ngram_wf <- workflow() %>%
  add_recipe(scotus_ngram_rec) %>%
  add_model(tune_spec)

ngram_wf
```


```{r}
lambda_grid <- grid_regular(penalty(), levels = 20)

set.seed(2020)
ngram_rs <- tune_grid(
  ngram_wf,
  scotus_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = TRUE)
)
```


```{r, dependson="lassors"}
ngram_rs %>%
  collect_metrics()
```


```{r}
lowest_rmse <- ngram_rs %>%
  select_best("rmse")

final_ngram <- finalize_workflow(
  ngram_wf,
  lowest_rmse
)

final_ngram
```


```{r}
final_ngram_rs <- final_ngram %>%
  fit_resamples(scotus_folds,
                control = control_resamples(save_pred = TRUE)) 

final_ngram_rs %>%
  unnest(.predictions) %>%
  ggplot(aes(year, .pred, color = id)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Truth",
    y = "Predicted year",
    color = NULL
  )
```

## Case study: compare lemma vs. words

using spacy

In this chapter, we are tokenizing via the [spaCy](https://spacy.io/) library as made available through the spacyr R package [@Benoit19].

## Different types of models

Lasso regression works extremely well with text data, but it is not the only option.

(Not all of these models are good, but are used to show strenghs and weaknessed)

- SVM
- Naive Bayes
- glmnet
- knn


## What evaluation metrics are appropiate

Data will most likely be sparse when using BoW

## Full game

### Feature selection

### Splitting the data

### Specifying models

### Cross-validation

### Evaluation

```{r}
library(vip)

final_lasso %>%
  fit(scotus_train) %>%
  pull_workflow_fit() %>%
  vi(lambda = lowest_rmse$penalty) %>%
  mutate(
    Importance = abs(Importance),
    Variable = str_remove_all(Variable, "tfidf_text_")
  ) %>%
  group_by(Sign) %>%
  top_n(20, Importance) %>%
  ungroup %>%
  ggplot(aes(x = Importance, 
             y = fct_reorder(Variable, Importance), 
             fill = Sign)) +
  geom_col(show.legend = FALSE) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_wrap(~Sign, scales = "free") +
  labs(y = NULL)
```


Closely examine high performing samples and low performance sampling. 
