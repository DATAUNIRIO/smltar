# Stop words {#stopwords}

Once we have tokenized our text into words, it might occur to you that not all of these words carry the same amount of information with then, if any information at all. Words that don't carry any information with them are called **stop words**. It is common advice to remove stopwords in various NLP tasks, but the task of stop word removal is more nuanced than many resources would like you to think. In this chapter, we will go over what a stop word list is and the differences between them and the effects on using them in your preprocessing workflow.

The concept of stop words has a long history with Hans Peter Luhn being credited with coining the term back in 1960 [@Luhn1960]. Examples of these words are "a", "the", "of" and "didn't" since they don't appear to add more to the meaning of the text other than making sure the structure of the sentence is sound. Thinking of words as being either informative or information is quite limiting and we would rather think of words as having a more fluid amount of information associated with them, where this information is very context-specific as well. Historically one of the main reasons for removing stop words was to improve computational time as it can be seen as a dimensionality reduction of the data and was commonly done in search engines to give better results [@Huston2010].

## Using premade stop words

A quick solution to getting a list of stop words is to use one that is already created for you. This is quite tempting as it requires little to no effort. But beware not all lists are created equal. @nothman-etal-2018-stop found in a study of 52 stop words lists available in open-source software packages quite some alarming results. They found expected things like how different stop word lists have a varying number of words depending on the specificity of the list. Among some of the more grave issues was the finding of misspellings (fify instead of fifty), the inclusion of informative words such as "computer" and "cry" and various inconsistencies such as including the word "has" but not the word "does". This is not to say that you shouldn't use a stop word list that has been included in your software. But you should always inspect and verify the list you are using. Both to make sure it hasn't changed since you used it last, but also to check that it is appropriate for your use case.

There is a broad selection of stop word lists available in different packages. For the purpose of this chapter will we limit the discussion to the 3 lists provided by the **stopwords** package. The package includes word lists from a range of languages and sources. We will focus on lists of English words that give us the stopword list bases on SMART System for the Mechanical Analysis and Retrieval of Text) Information Retrieval System, an information retrieval system developed at Cornell University in the 1960s [@Lewis2014]. Next, we will consider the English Snowball stop word list [@porter2001snowball] and lastly the English list from the [Stopwords ISO](https://github.com/stopwords-iso/stopwords-iso) collection. These stopword lists are all considered to general and thus not domain-specific.

Before we start looking at the words inside the lists lets take a look at how many words are included in each

```{r, results='hold'}
library(stopwords)
length(stopwords(source = "smart"))
length(stopwords(source = "snowball"))
length(stopwords(source = "stopwords-iso"))
```

we see that the length of these lists are quite varied, with the longest list being over 7 times longer than the shortest. Let us take a look at the overlap between the words that appear in the 3 lists

```{r, echo=FALSE}
library(UpSetR)
fromList(list(smart = stopwords(source = "smart"),
              snowball = stopwords(source = "snowball"),
              iso = stopwords(source = "stopwords-iso"))) %>%
  upset(empty.intersections = "on")
```

We see that the 3 lists are almost true subsets of each other. The only deviation is 10 words that appear in Snowball and ISO but not in the SMART list. Let us take a look at those words first

```{r}
setdiff(stopwords(source = "snowball"), stopwords(source = "smart"))
```

and we see that all the words appear to be contractions. This is not because the SMART lexicon doesn't include contractions because if we look there are almost 50 of them.

```{r}
str_subset(stopwords(source = "smart"), "'")
```

And we even find an inconsistency, Why does SMART include "he's" but not "she's"? It is hard to tell, but it should be worth rectifying before applying these stop word lists. It appears that this stop word list has been computer-generated by selecting the most frequent words across a large corpus of text. This is again a reminder that one should always look carefully at the premade word list to make sure it works with your needs. It is okay to select a premade word list and remove or append additional words according to your use-case. 

When you select a stop word list it is important that you consider the size and breadth, having a small and concise list of words can moderately reduce your token count while not having too great of an influence on your models, assuming that you picked appropriate words. As the size of your list grows, each word added will have a diminishing effect with the increasing risk that a meaningful word has been placed on the list by mistake. In a later chapter, we will have a study where we will analyze the effect of different stop word lists.

### Stop word removal in R

Now that we have some stopword lists it is fairly easy to remove them, however, the way depends on the shape of your data. If you have your text in a tidy one word per row you can use `filter()` from **dplyr** with a negated `%in%` if you have the stop words as a vector, or you can use `anti_join()` from **dplyr** if the stopwords comes from a data.frame. Here we have `tidy_fir_tree` from chapter \@ref(stemming), without removing stopwords.

```{r}
tidy_fir_tree <- fir_tree %>%
  unnest_tokens(word, text)
```

And we will use the Snowball stopword list as an example. Since the stopwords come as a vector we will use `filter()`.

```{r, results='hold'}
tidy_fir_tree %>%
  filter(!(tidy_fir_tree$word %in% stopwords(source = "snowball")))
```

but if we if we use the `stop_words` from **tidytext** then we can use the `anti_join()` function.

```{r}
tidy_fir_tree %>%
  anti_join(stop_words, by = "word")
```

the result of these two stop word removals was different since we didn't use the same stop word lists.

## Creating your own stopwords list

Another way to get a stop word list is to create one yourself. We will be looking at a few different ways to find words to use. We will use the tokenized Fir-Tree as a first example. We take the words and order them by their count. The proposition is that the most frequent words happen appear the most frequently.

```{r, echo=FALSE, fig.cap="'The Fir Tree' was tokenized and tokens were counted and ordered descendingly according to count."}
tidy_fir_tree %>%
  count(word, sort = TRUE) %>%
  slice(1:120) %>%
  mutate(row = rep(1:5, each = n()/5),
         column = rep(rev(seq_len(n()/5)), length.out = n())) %>%
  mutate(word = paste0(row_number(), ": ", word)) %>%
  ggplot(aes(row, column, label = word)) +
  geom_text(hjust = 0) +
  xlim(c(1, 5.5)) +
  theme_void() +
  labs(title = "120 Most frequent tokens in 'The Fir Tree'")
```

by looking at the first column of words we recognize a lot of what we would consider stop words, with 3 big exceptions. "tree" at 3, "fir" at 12 and "little" at 22. These words appeared high on our list but do provide valuable information as it is referencing the main character. So what went wrong in this approach? Creating a stopword list using high-frequency words works best when it is created on a corpus of documents, not individual documents. This is because the words found in a single document will be document specific and not generalize that well. The word "tree" does seem important as it is about the main character, but it could also be appearing so often that it stops providing any information. Let's try extraction high-frequency words from the corpus of all English fairy tales by H.C. Andersen.

```{r, echo=FALSE, fig.cap="All of the English H.C. Andersen's fairly tales were tokenized and tokens were counted and ordered descendingly according to count."}
hcandersen_en %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE) %>%
  slice(1:120) %>%
  mutate(row = rep(1:5, each = n()/5),
         column = rep(rev(seq_len(n()/5)), length.out = n())) %>%
  mutate(word = paste0(row_number(), ": ", word)) %>%
  ggplot(aes(row, column, label = word)) +
  geom_text(hjust = 0) +
  xlim(c(1, 5.5)) +
  theme_void() +
  labs(title = "120 Most frequent tokens in H.C. Andersen's fairly tales, English")
```

these words feel more in line with our concept of stop words. Now it is time for us to make some choices. How many we want to include, and which words to add and remove based on prior information. Selection the number of words to remove is best done by a case-by-case basis as it can be hard to determine apriori how many different "meaningless" words that appear in a corpus. Our suggestion would be to start with a low number like 20 and increase by 10 words until you get to words that don't feel like stopwords. It is worth keeping in mind that this list is not perfect, and it is based on the corpus of documents we had available, a potential biased one since all the fairy tales were written by the same European white male from the early 1800s. This bias can be minimized by removing words we would expect to be over represented or to add words we expect to be underrepresented. Easy examples are to include the compliments to the words in the lists if they are not present. Include "big" if "small" is present, "old" if "young" is present. this example has female words often listed after male words. With "man" being 79, but "woman" is `r hcandersen_en %>% unnest_tokens(word, text) %>% count(word, sort = TRUE) %>% pull(word) %>% magrittr::equals("woman") %>% which()` having a cutoff at 100 would lead to only one of these words be included. Depending on how important you think pronouns are going to be in your texts to you either add "woman" or delete "man".

The following chart shows how the male-gendered words have lower rank than the female rank, showing that by simply using a cutoff you would be likely to only include the male form of the word.

```{r, echo=FALSE, fig.cap="Tokens where counted and ranked according to total count. Rank 1 has most occurrences."}
gender_words <- tribble(
  ~men, ~women,
  "he", "she",
  "his", "her",
  "man", "woman",
  "men", "women",
  "boy", "girl",
  "he's", "she's",
  "he'd", "she'd",
  "he'll", "she'll",
  "himself", "herself"
)

ordered_words <- hcandersen_en %>% 
  unnest_tokens(word, text) %>% 
  count(word, sort = TRUE) %>% 
  pull(word)

gender_words_plot <- gender_words %>%
  mutate(male_index = match(men, ordered_words),
         female_index = match(women, ordered_words)) %>%
  mutate(slope = log10(male_index) - log10(female_index)) %>%
  pivot_longer(male_index:female_index) %>%
  mutate(value = log10(value),
         label = ifelse(name == "male_index", men, women)) %>%
  mutate(name = factor(name, c("male_index", "female_index"), c("men", "women")))

limit <- max(abs(gender_words_plot$slope)) * c(-1, 1)

gender_words_plot %>%
  ggplot(aes(name, value, group = women)) +
  geom_line(aes(color = slope), size = 1) +
  scale_y_reverse(labels = function(x) 10 ^ x) +
  geom_text(aes(label = label)) +
  scale_color_distiller(type = "div", limit = limit) +
  guides(color = "none") +
  theme(panel.border = element_blank(), panel.grid.major.x = element_blank()) +
  labs(x = NULL, y = "Word Rank") +
  labs(title = "Masculine gendered words appeared more often in H.C. Andersen's fairy tales")
```

Imagine now we would like to create a stopword list that spans multiple different genres, in such a way that the subject-specific stop words don't overlap. For this case, we would like words to be denoted as a stopword only if it is a stopword in all the genres. You could find the words individually in each genre and using the right intersections. However, that approach might take a substantial amount of time.

Below is a bad example where we try to create a multi-language list of stopwords. To accomplish this will we calculate the inverse document frequency (IDF) of each word, and create the stopword-list based on the words with the lowest IDF. The following function takes a tokenized data.frame and returns a data.frame with a column for each word and a column of the IDF.

```{r}
library(rlang)
calc_idf <- function(df, word, document) {
  words <- df %>% pull({{word}}) %>% unique()

  n_docs <- length(unique(pull(df, {{document}})))
  
  n_words <- df %>%
    nest(data = c({{word}})) %>%
    pull(data) %>%
    map_dfc(~ words %in% unique(pull(.x, {{word}}))) %>%
    rowSums()
  
  tibble(word = words,
         idf = log(n_docs / n_words))
}
```

And the following is the result where we try to create a cross-language list of stopwords, by taking each fairy tale as a document. And it is not very good. Since the overlap between what words appear in each language is very small. And that is what we mostly see in this list.

```{r, echo=FALSE, fig.cap="All of the H.C. Andersen's fairly tales were tokenized and tokens were counted and ordered descendingly according to count. Laguages: Danish, English, French, German and Spanish."}
hcandersenr::hca_fairytales() %>%
  unnest_tokens(word, text) %>%
  mutate(document = paste(language, book)) %>%
  select(word, document) %>%
  calc_idf(word, document) %>%
  arrange(idf) %>%
  slice(1:120) %>%
  mutate(row = rep(1:5, each = n()/5),
         column = rep(rev(seq_len(n()/5)), length.out = n())) %>%
  mutate(word = paste0(row_number(), ": ", word)) %>%
  ggplot(aes(row, column, label = word)) +
  geom_text(hjust = 0) +
  xlim(c(1, 5.5)) +
  theme_void() +
  labs(title = "120 Most frequent tokens in H.C. Andersen's fairly tales, multi-language")
```

TODO do same example with English only.

do MP, VP and  SAT 
https://pdfs.semanticscholar.org/c543/8e216071f6180c228cc557fb1d3c77edb3a3.pdf

## All stopword lists are content specific, did you check yours?

Since all work related to the text is going to be context-specific, it is important to make sure that the stop word list you use reflects the word space that you are planning on using it on. One common consideration is if pronouns are something that brings information to your text. This is a common thing people need to think about since they are contained in a lot of different stop word lists (inconsistently) and they will a lot of times not be noise in your data.

On the other hand, you will have to fill in some of the words yourself depending on the domain. If you are working with texts for dessert recipes, certain ingredients (sugar, eggs, water) and actions (whisking, baking, stirring) will have a chance of becoming frequent enough to pass your stop word threshold, but be important enough to keep as they have high signal. Throwing away "eggs" as a common word would make it harder or downright impossible to determine if certain recipes are vegan or not,  while whisking and stirring will properly be fine to remove as differentiating between recipes that do and don't require a whisk might not be that big of a deal.

## What happens when you remove stopwords

We talked about different ways of detecting and removing stopwords now let us see what happens once you remove them. The first thing we are going to look at is the number of words that are included in the list. The following chart shows what percentage of words have been removed as a function of the number of words in a text. The different colors represent the 3 different stopword lists we have considered in this chapter.

```{r}
plotting_data <- hcandersen_en %>%
  nest(data = c(text)) %>%
  mutate(tokens = map(data, ~ unlist(tokenize_words(.x$text))),
         no_snowball = map_int(tokens, ~ length(setdiff(.x, stopwords(source = "snowball")))),
         no_smart = map_int(tokens, ~ length(setdiff(.x, stopwords(source = "smart")))),
         no_iso = map_int(tokens, ~ length(setdiff(.x, stopwords(source = "stopwords-iso")))),
         n_tokens = lengths(tokens)) %>%
  pivot_longer(no_snowball:no_iso) %>%
  mutate(value = 1 - value / n_tokens)

plotting_data %>%
  mutate(name = factor(name, c("no_snowball", "no_smart",  "no_iso"),
                       c("snowball (175)", "smart (571)", "stopwords-iso (1298)")),
         name = fct_rev(name)) %>%
  ggplot(aes(n_tokens, value, color = name)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'loess', se = FALSE) +
  scale_color_brewer(palette = "Set2") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Number of words", 
       y = "Percentage of words removed",
       color = "Removed",
       title = "Stop words take up a larger part of the text in longer fairy tales",
       subtitle = "Each group of horizontal line of points represents a H.C. Andersen Fairy tale")

```

We notice predictably that larger stopword lists remove more words then shorter stopword lists. In the examples with fairy tales then over half of the words have been removed, with the big list removing over 80% of the words. An interesting observation is that shorter texts have a lower percentage of stopwords. Since we are looking at fairy tales this could be explained by the fact that a story has to be told regardless of the length of the fairy tale so shorter texts are going to be more dense with informative words.

The stopword lists we examined in this chapter have been English and the notion of "short" and "long" lists we have used here are specific to English as a language. You should expect different languages to have a varying number of "uninformative" words. Furthermore due to the morphological richness of a language; lists that contain all possible morphological variants of each stop-word could become quite large.

Another consideration one should take it to make sure that the order in which you perform stemming and stopword removal is happening. Some lists only include words in their unstemmed form, if that is the case then you should remove stopwords before you do stemming so some words don't fall through the gaps. A handy little trick is to use the following function on your stopword list to return the words that don't have it's stemmed version in the list. If the function returns a length 0 vector then you can use stem and remove stopwords in any order.

```{r}
not_stemmed_in <- function(x) {
  x[!SnowballC::wordStem(x) %in% x]
}

not_stemmed_in(stopwords(source = "snowball"))
```

Here we see that most of the words that lost are the contradictions, since "they" is included in the list but "they'" isn't.

Another problem you might have is dealing with misspellings. Most premade stopword lists assume that all the words are spelled correctly. Handling misspelling when using premade lists can be done by manually adding common misspellings. One could imagine creating all words that are a certain edit-distance away from the stopwords, but it would not be recommended as you would quickly include informative words this way.

One of the downsides of creating your own stopword lists using frequencies is that you are limited to using words that you have already observed. It could happen that "she'd" is included in your training corpus but not "he'd" didn't reach the cutoff. This is a case where you need to look at your words and adjust accordingly. Here the large premade stopword lists can serve as inspiration for missing words.

In a later chapter (TODO add link) will we take a look at the influence of removing stopwords in the context of modeling where we see that given the right list of words you see no harm to the model performance, and may even see improvement in result due to noise reduction[@Feldman2007].

## Stopwords in non-English languages

So far in this chapter, we have been spent the majority of the time on the English language, however, English is not representative of every language. It is therefore important that you have an understanding of the language you are working with.

Different languages have different numbers of words in each class of words. An example is how the grammatical case influences the articles used in German. Below is a couple of diagrams showing the use of definite and indefinite articles in German, Notice how German nouns have 3 genders (Masculine, Feminine and neuter) which are not uncommon to have. Articles are almost always used as stopwords in English as very little information. However, German articles give some indication of the case which would need to be a consideration for you when selecting your list of stop words in German or any other language where the grammatical case is reflected in the text.

```{r, echo=FALSE}
library(magrittr)
library(gt)
tibble::tribble(
  ~Masculine, ~Feminine, ~Neuter, ~Plural, ~case,
  "der", "die", "das", "die", "Nominative",
  "den", "die", "das", "die", "Accusative",
  "dem", "der", "dem", "den", "Dative",
  "des", "der", "des", "der", "Genitive"
) %>%
  gt(rowname_col = "case") %>%
   tab_header(title = "German Definite Articles (the)")
```

```{r, echo=FALSE}
tibble::tribble(
  ~Masculine, ~Feminine, ~Neuter, ~Plural, ~case,
  "ein", "eine", "ein", "keine", "Nominative",
  "einen", "eine", "ein", "keine", "Accusative",
  "einem", "einer", "einem", "keinen", "Dative",
  "eines", "einer", "eines", "keiner", "Genitive"
) %>%
  gt(rowname_col = "case") %>%
   tab_header(title = "German Indefinite Articles (a/an)")
```


Finding lists of stop words in Chinese has been done both manually and automatically[@Zou2006ACC] but so far none of them has been accepted as a standard [@Zou2006]. A full discussion stop word identification of stop words in Chinese text would be out of scope for this book, so we will just highlight some of the challenges that differentiate it from English. 

:::note
Chinese text is must more complex than portrayed here. With different systems and billions of users, there is much we won't be able to touch on here.
:::

The main difference from English is the use of logograms instead of letters to convey information. However Chinese characters should not be confused with Chinese words. The majority of words in modern Chinese is composed of multiple characters. This means that inferring the presence of "words" is more complicated and the notion of stop words will affect how this segmentation of characters is done.
